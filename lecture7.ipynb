{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdm2iCUQLlLPyg91Kvr1WC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/lecture7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Programming"
      ],
      "metadata": {
        "id": "O5Q4m-AkYqCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Following up on our discussion of parallelism, we'll move onto GPU programming.\n",
        "* A great resource for an introduction to GPU programming can be found [here](https://enccs.github.io/gpu-programming/1-gpu-history/)\n",
        "* GPUs were originally developed for the highly parallel task of graphics processing, but have since become a workhorse in many applications including AI\n",
        "* GPUs are specialized parallel hardware for floating point operations\n",
        "* In traditional systems, the CPU controls the work flow and esignates highly parallel tasks to the GPU\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CIUYx2e-ZpOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Use GPUs\n",
        "* **Speed**: GPU computing can offer massive performance benefits for some problems\n",
        "* **Efficiency**: Compared to CPUs, GPUs can perform more calculations per watt for certain workloads\n",
        "* **Cost Effectiveness:** GPUs can also be more cost-effective than traditional CPU-based systems for some workloads\n",
        "* **Excuses:** You wanted to buy a high end GPU to play video games, but you need an excuse so you say you'll use it for work\n",
        "\n"
      ],
      "metadata": {
        "id": "tIQZXLNWbySj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Hardware\n",
        "* CPUs have a handful of complex and powerful cores\n",
        "* GPUs have a multitude of simpler cores designed for highly parallel, high-throughput problems\n",
        "* GPU cores are less general purpose, but there are lots of them. For example, an RTX 5090 has **21,760** CUDA cores\n",
        "* CPUs can efficiently handle **task level** parallelism in which multiple distinct tasks are handled simultaneously\n",
        "* GPUs are good at **data level** parallelism in which the same operation is applied in parallel to different chunks of the data\n",
        "* GPUs are like the ultimate machines for doing vectoriized operations\n",
        "* In contrast to CPUs, GPUs contain a relatively small amount of transistors dedicated to control and caching, and a much larger fraction of transistors dedicated to the mathematical operations\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://enccs.github.io/gpu-programming/_images/CPUAndGPU.png\" width=\"1000px\">\n",
        "<figcaption align=\"center\"> CPU v. GPU Architecture from ENCCS\n",
        "</figure>"
      ],
      "metadata": {
        "id": "ey0UylZDcT0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table border=\"1\" style=\"border-collapse: collapse; text-align: center; font-size:18px;\">\n",
        "  <tr>\n",
        "    <th style=\"padding:8px;\">CPU</th>\n",
        "    <th style=\"padding:8px;\">GPU</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding:8px;\">General purpose</td>\n",
        "    <td style=\"padding:8px;\">Highly specialized for parallelism</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding:8px;\">Good for serial processing</td>\n",
        "    <td style=\"padding:8px;\">Good for parallel processing</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding:8px;\">Great for task parallelism</td>\n",
        "    <td style=\"padding:8px;\">Great for data parallelism</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding:8px;\">Low latency per thread</td>\n",
        "    <td style=\"padding:8px;\">High-throughput</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "8DlPtaZKnc7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems Suited to GPUs\n",
        "* As a general rule of thumb:\n",
        "\n",
        "> **Good on GPU:** same operation on lots of data, emmbarassingly parallel, compute-heavy\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Many large-scale matrix and vector operations\n",
        "  * Matrix multiplication\n",
        "  * Elementwise vectorized operations on arrays where the same operation is computed for every element\n",
        "  * Factorization and iterative linear solvers work reasonably well\n",
        "  * These are often found in machine learning, scientific computing, and image processing applications\n",
        "* Fourier Transforms\n",
        "  * Fourier transforms can be parllelized efficienctly\n",
        "  * Also used in machine learning, scientific computing, data science, image processing\n",
        "* Monte Carlo Simulations\n",
        "  * Used across finance, physics, and other fields to simulate complex systems\n",
        "* Deep learning workloads\n",
        "\n"
      ],
      "metadata": {
        "id": "7oZscAWwn0SP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems Not Suited to GPUs\n",
        "* As a general rule of thumb:\n",
        "\n",
        "> **Bad on GPU:** lots different operations, little work per element. sequential, memory/branch dominated problems\n",
        "\n",
        "* GPUs are not especially good at **memory-bound** problems that are limited by how fast you can move data in and out of memory instead of doing computation\n",
        "* They are also not good at **branch-dominated** problems where performance is limited by control flow like lots of if/else decisions, irregular loops, or recursion\n",
        "\n",
        "**Examples:**\n",
        "* Sparse Linear Algebra\n",
        "  * Sparse matrix-vector multiply is memory-bound\n",
        "* Graph algorithms\n",
        "  * Things like breadth-first search depth-first search with irregular memory access\n",
        "* Control-heavy Code\n",
        "  * Code with lots of branching logic, dynamic programming, or irregular loops\n",
        "* Small Problems\n",
        "  * Problems need to be big enough to justify copying data to the GPU memory and launching compute kernels\n",
        "* Sequential Tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "I76l_c4QuYSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Basics\n",
        "* Although we will not be programming CUDA kernels directly in C or C++, we will look at some CUDA code to illustrate low level concepts\n",
        "* In order to perform some operation on the GPU we need to create a function called a **kernel** that will be executed simultaneously across many threads running on the GPU in parallel\n",
        "* For example, we could have a basic kernel that takes in pointers to some integer arrays and compute their sum\n",
        "\n",
        "```c\n",
        "__global__ void addKernel(int *c, const int *a, const int *b, int size) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "EKiMsBpX3DCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We might use this kernel in the context of the following program where we create some arrays of integers, copy them to GPU memory, launch the kernel, then copy the results back to the host.\n",
        "\n",
        "```c\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void addKernel(int *c, const int *a, const int *b, int size) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 5;\n",
        "    int a[size] = {1, 2, 3, 4, 5};\n",
        "    int b[size] = {10, 20, 30, 40, 50};\n",
        "    int c[size] = {0};\n",
        "\n",
        "    // Allocate GPU memory\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    cudaMalloc(&dev_a, size * sizeof(int));\n",
        "    cudaMalloc(&dev_b, size * sizeof(int));\n",
        "    cudaMalloc(&dev_c, size * sizeof(int));\n",
        "\n",
        "    // Copy inputs to GPU\n",
        "    cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    addKernel<<<1, size>>>(dev_c, dev_a, dev_b, size);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    for(int i = 0; i < size; i++) {\n",
        "        printf(\"%d + %d = %d\\n\", a[i], b[i], c[i]);\n",
        "    }\n",
        "\n",
        "    // Free GPU memory\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "E3KnkpE15tT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most of this makes sense, but there are a few weird things\n",
        "* For instance what  are `blockIdx`, `blockDim`, and `threadIdx`\n",
        "* When launching the kernel, what exactly does `addKernel<<<1, size>>>` mean?\n"
      ],
      "metadata": {
        "id": "LaU0QVxY-uoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA Threads\n",
        "* Kernels are executed on many CUDA threads simultaneously\n",
        "* Every thread is associated with a particular intrinsic index which can be used to calculate and access particular memory location in an array\n",
        "* Each thread has its context and set of private variables\n",
        "* Threads are organized into a hierarchy:\n",
        "  * Threads are grouped into groups of 32 threads called **warps**\n",
        "  * All threads in a warp execute the same instruction at the same time  e (SIMT: Single Instruction, Multiple Threads).\n",
        "  * Multiple warps are organized into **blocks**\n",
        "  * The hardware schedulaer assigns a block to an SMP or (Streaming Mulitiprocessor Unit) which has its own memory\n",
        "  * Blocks are organized into a **grid**\n",
        "* This organization also controls memory access:\n",
        "  * Global memory is accessible by all threads\n",
        "  * Shared memory is accessible between threads in a block\n",
        "  * Private memory is private to each thread\n",
        "  * Constant memory is read-only memory accessible to all threds\n"
      ],
      "metadata": {
        "id": "5UmFOl12_7aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the example kernel the block index, thread index, and block dimension are used to index into the arrays that are passed in\n",
        "* That is, these indexes identify what thread is operating and are used to decide what subset of the data to operate on\n",
        "* The image below shows an example assuming a block size of 256\n",
        "<figure>\n",
        "<img src=\"https://enccs.github.io/gpu-programming/_images/Indexing.png\" width=\"1000px\">\n",
        "<figcaption align=\"center\"> How threads in a grid can be associated with specific elements of an array from ENCCS\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "TYsu23QdVfjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When launching a kernel you specify both the number of blocks and the number of threads\n",
        "* For example this would launch 80 blocks with 256 threads each\n",
        "```c\n",
        "myKernel<<<80, 256>>>(...);\n",
        "```\n",
        "* The number of threads per block is usually limited to 1024\n",
        "* You can only synchronize threads within a block.\n",
        "* Blocks run independently and the GPU can schedule them flexibly\n"
      ],
      "metadata": {
        "id": "hIIZwbu9Xbfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shared Memory Example\n",
        "* In the above example of the add kernel, we only had one block with five threads\n",
        "* Each thread was responsible for doing a single summation\n",
        "* Values were fetched from global memory\n",
        "* This was perfectly fine, but we could also use shared memory in cases where data is frequently accessed  \n",
        "```python\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void add_shared(int *c, const int *a, const int *b, int n) {\n",
        "    extern __shared__ int s[];          // dynamic shared buffer\n",
        "    int* s_a = s;                       // first half for a\n",
        "    int* s_b = s + blockDim.x;          // second half for b\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int i  = blockIdx.x * blockDim.x + tx;\n",
        "\n",
        "    // Load GLOBAL -> SHARED (bounds check for last, partial block)\n",
        "    if (i < n) {\n",
        "        s_a[tx] = a[i];\n",
        "        s_b[tx] = b[i];\n",
        "    }\n",
        "    __syncthreads();                    // ensure all loads completed\n",
        "\n",
        "    // Compute from SHARED, write to GLOBAL\n",
        "    if (i < n) {\n",
        "        c[i] = s_a[tx] + s_b[tx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Example data (same size for a, b, c)\n",
        "    const int n = 5;\n",
        "    int h_a[n] = {1, 2, 3, 4, 5};\n",
        "    int h_b[n] = {10, 20, 30, 40, 50};\n",
        "    int h_c[n] = {0};\n",
        "\n",
        "    // Allocate device (GPU) memory\n",
        "    int *d_a = nullptr, *d_b = nullptr, *d_c = nullptr;\n",
        "    cudaMalloc(&d_a, n * sizeof(int));\n",
        "    cudaMalloc(&d_b, n * sizeof(int));\n",
        "    cudaMalloc(&d_c, n * sizeof(int));\n",
        "\n",
        "    // Copy inputs host -> device\n",
        "    cudaMemcpy(d_a, h_a, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch configuration\n",
        "    int threadsPerBlock = 256;         \n",
        "    int numBlocks = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    size_t shmemBytes = 2 * threadsPerBlock * sizeof(int);      // s_a + s_b\n",
        "\n",
        "    // Launch shared-memory kernel\n",
        "    add_shared<<<numBlocks, threadsPerBlock, shmemBytes>>>(d_c, d_a, d_b, n);\n",
        "    cudaDeviceSynchronize();  \n",
        "\n",
        "    // Copy result device -> host\n",
        "    cudaMemcpy(h_c, d_c, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        printf(\"%d + %d = %d\\n\", h_a[i], h_b[i], h_c[i]);\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n",
        "* The main difference in this example is that we specifically create a memory buffer `s` that can be accessed by all threads\n",
        "* This could be useful if we need to repeatedly access the data (although that isn't the case in this example)\n",
        "\n"
      ],
      "metadata": {
        "id": "L0FMVgRRlTE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU Programming in Python\n",
        "* Fortunately, we don't necessarily need to write C or C++ code to use CUDA\n",
        "* There are a few Python packages for writing CUDA code that can take advantage of GPUs\n"
      ],
      "metadata": {
        "id": "K6DIcFkpu6nd"
      }
    }
  ]
}