{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4ZIyCOqMAOR2CS3k7e4f1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/lecture7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Programming"
      ],
      "metadata": {
        "id": "O5Q4m-AkYqCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Following up on our discussion of parallelism, we'll move onto GPU programming\n",
        "* A great resource for an introduction to GPU programming can be found [here](https://enccs.github.io/gpu-programming/1-gpu-history/)\n",
        "* GPUs were originally developed for the highly parallel task of graphics processing, but have since become a workhorse in many applications including AI\n",
        "* GPUs are specialized parallel hardware for floating point operations\n",
        "* In traditional systems, the CPU still controls the work flow and designates highly parallel tasks to the GPU\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CIUYx2e-ZpOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Use GPUs\n",
        "* **Speed**: GPU computing can offer massive performance benefits for some problems\n",
        "* **Efficiency**: Compared to CPUs, GPUs can perform more calculations per watt for certain workloads\n",
        "* **Cost Effectiveness:** GPUs can also be more cost-effective than traditional CPU-based systems for some workloads\n",
        "* **As an Excuse:** You need a high end GPU for \"work\"\n"
      ],
      "metadata": {
        "id": "tIQZXLNWbySj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Hardware\n",
        "* CPUs have a moderate number of complex and powerful cores\n",
        "* GPUs have a multitude of simpler cores designed for highly parallel, high-throughput problems\n",
        "* GPU cores are not as powerful and they are not designed for general purpose computing\n",
        "* However, GPUs have extremely high core counts. For example, an RTX 5090 has **21,760** CUDA cores + other cores for ray tracing and tensor processing\n",
        "* CPUs can efficiently handle **task parallelism**, which involves running several diverse tasks concurrently\n",
        "* GPUs are good at **data parallelism** in which the same operation is applied in parallel to different chunks of the data\n",
        "* In contrast to CPUs, GPUs contain a relatively small amounts of transistors dedicated to control and caching, and a much larger fraction of transistors dedicated to the mathematical operations\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://enccs.github.io/gpu-programming/_images/CPUAndGPU.png\" width=\"1000px\">\n",
        "<figcaption align=\"center\"> CPU v. GPU Architecture from ENCCS\n",
        "</figure>"
      ],
      "metadata": {
        "id": "ey0UylZDcT0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table border=\"1\" style=\"border-collapse: collapse; text-align: center; font-size:18px;\">\n",
        "  <tr>\n",
        "    <th style=\"padding:8px;\">CPU</th>\n",
        "    <th style=\"padding:8px;\">GPU</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding:8px;\">General purpose</td>\n",
        "    <td style=\"padding:8px;\">Highly specialized for parallelism</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding:8px;\">Good for serial processing</td>\n",
        "    <td style=\"padding:8px;\">Good for parallel processing</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding:8px;\">Great for task parallelism</td>\n",
        "    <td style=\"padding:8px;\">Great for data parallelism</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding:8px;\">Low latency per thread</td>\n",
        "    <td style=\"padding:8px;\">High-throughput</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "8DlPtaZKnc7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems Suited to GPUs\n",
        "* As a general rule of thumb:\n",
        "\n",
        "> **Good on GPU:**\n",
        "  * same operation on lots of data\n",
        "  * emmbarassingly parallel\n",
        "  * compute-heavy\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Many large-scale matrix and vector operations\n",
        "  * Matrix multiplication\n",
        "  * Element-wise operations on arrays where the same operation is performed on every element\n",
        "  * Factorization and iterative linear solvers work reasonably well on GPUs\n",
        "  * These linear algebra operations are often found in machine learning, scientific computing, and image processing applications\n",
        "* Fourier Transforms\n",
        "  * Fourier transforms can be parllelized efficienctly\n",
        "  * They are also used in machine learning, scientific computing, data science, and image processing\n",
        "* Monte Carlo Simulations\n",
        "  * Used across finance, physics, and other fields to simulate complex systems\n",
        "* Deep learning workloads\n",
        "\n"
      ],
      "metadata": {
        "id": "7oZscAWwn0SP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems Not Suited to GPUs\n",
        "* As a general rule of thumb:\n",
        "\n",
        "> **Bad on GPU:**\n",
        " * lots different operations\n",
        " * little work per element\n",
        " * sequential problems\n",
        " *  memory/branch dominated problems\n",
        "\n",
        "\n",
        " ### Memory and Branch Dominated Problems\n",
        "\n",
        "* GPUs are not especially good at **memory-bound** problems that are limited by how fast you can move data in and out of memory instead of doing computation\n",
        "* They are also not good at **branch-dominated** problems where performance is limited by control flow like lots of if/else decisions, irregular loops, or recursion\n",
        "\n",
        "**Examples:**\n",
        "* Sparse Linear Algebra\n",
        "  * Sparse matrix-vector multiply is memory-bound\n",
        "* Graph algorithms\n",
        "  * Things like breadth-first search depth-first search with irregular memory access\n",
        "* Control-heavy Code\n",
        "  * Code with lots of branching logic, dynamic programming, or irregular loops\n",
        "* Small Problems\n",
        "  * Problems need to be big enough to justify copying data to the GPU memory and launching compute kernels\n",
        "* Sequential Tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "I76l_c4QuYSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Basics\n",
        "* Although we will not be programming CUDA kernels directly in C or C++, we will look at some CUDA code to illustrate a few lower level concepts that will be useful when doing GPU programming in Python\n",
        "* In order to perform operations on the GPU we need to create a function called a **kernel** that will be executed simultaneously across many threads running on the GPU in parallel\n",
        "* For example, we could have a basic kernel that takes in pointers to some integer arrays and computes their element-wise sum\n",
        "\n",
        "```c\n",
        "__global__ void addKernel(int *c, const int *a, const int *b, int size) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "EKiMsBpX3DCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We might use this kernel in the context of the following program where we create some arrays of integers, copy them to GPU memory, launch the kernel, then copy the results back to the host.\n",
        "\n",
        "```c\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void addKernel(int *c, const int *a, const int *b, int size) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 5;\n",
        "    int a[size] = {1, 2, 3, 4, 5};\n",
        "    int b[size] = {10, 20, 30, 40, 50};\n",
        "    int c[size] = {0};\n",
        "\n",
        "    // Allocate GPU memory\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    cudaMalloc(&dev_a, size * sizeof(int));\n",
        "    cudaMalloc(&dev_b, size * sizeof(int));\n",
        "    cudaMalloc(&dev_c, size * sizeof(int));\n",
        "\n",
        "    // Copy inputs to GPU\n",
        "    cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    addKernel<<<1, size>>>(dev_c, dev_a, dev_b, size);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    for(int i = 0; i < size; i++) {\n",
        "        printf(\"%d + %d = %d\\n\", a[i], b[i], c[i]);\n",
        "    }\n",
        "\n",
        "    // Free GPU memory\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "E3KnkpE15tT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most of this looks like pretty standard C code, but there are a few unique pieces\n",
        "* For instance what  are `blockIdx`, `blockDim`, and `threadIdx`?\n",
        "* When launching the kernel, what exactly does `addKernel<<<1, size>>>` mean?\n",
        "* To understand these lines, we need to learn about CUDA threads\n"
      ],
      "metadata": {
        "id": "LaU0QVxY-uoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA Threads\n",
        "\n",
        "* Kernels are executed on many CUDA threads simultaneously\n",
        "* Every thread is associated with a particular intrinsic index which can be used to calculate and access particular memory location in an array\n",
        "* Each thread has its context and set of private variables\n",
        "* Threads are organized into a hierarchy:\n",
        "  * Threads are grouped into groups of 32 threads called **warps**\n",
        "  * All threads in a warp execute the same instruction at the same timee (SIMT: Single Instruction, Multiple Threads).\n",
        "  * Multiple warps are organized into **blocks**\n",
        "  * The hardware schedulaer assigns a block to an SMP or (Streaming Mulitiprocessor Unit) which has its own local, memory\n",
        "  * Blocks are organized into a **grid**\n",
        "* This organizational structure is related both to the physical architecture of the GPU as well as being a software abstraction\n",
        "* This hierarchy also controls memory access:\n",
        "  * Global memory is accessible by all threads\n",
        "  * Shared memory is accessible between threads in a block\n",
        "  * Private memory is private to each thread\n",
        "  * Constant memory is read-only memory accessible to all threds\n"
      ],
      "metadata": {
        "id": "5UmFOl12_7aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the example kernel, the block index, thread index, and block dimension are used to index into the arrays `a`, `b`, and `c`\n",
        "* That is, these indexes identify what thread is working and what subset of the data to operate on\n",
        "* The image below shows an example assuming a block size of 256\n",
        "<figure>\n",
        "<img src=\"https://enccs.github.io/gpu-programming/_images/Indexing.png\" width=\"1000px\">\n",
        "<figcaption align=\"center\"> How threads in a grid can be associated with specific elements of an array from ENCCS\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "TYsu23QdVfjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When launching a kernel you specify both the number of blocks and the number of threads\n",
        "* For example this would launch 80 blocks with 256 threads each\n",
        "```c\n",
        "myKernel<<<80, 256>>>(...);\n",
        "```\n",
        "* The number of threads per block is usually limited to 1024. On older GPUs this was 512.\n",
        "* You can only synchronize threads within a block. Blocks run independently and the GPU can schedule them flexibly\n"
      ],
      "metadata": {
        "id": "hIIZwbu9Xbfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shared Memory Example\n",
        "* In the above example of the add kernel, we only had one block with five threads\n",
        "* Each thread was responsible for doing a single summation\n",
        "* Values were fetched from global memory\n",
        "* This was perfectly fine, but we could also use shared memory in cases where data is frequently accessed  \n",
        "```python\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void add_shared(int *c, const int *a, const int *b, int n) {\n",
        "    extern __shared__ int s[];          // dynamic shared buffer\n",
        "    int* s_a = s;                       // first half for a\n",
        "    int* s_b = s + blockDim.x;          // second half for b\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int i  = blockIdx.x * blockDim.x + tx;\n",
        "\n",
        "    // Load GLOBAL -> SHARED (bounds check for last, partial block)\n",
        "    if (i < n) {\n",
        "        s_a[tx] = a[i];\n",
        "        s_b[tx] = b[i];\n",
        "    }\n",
        "    __syncthreads();                    // ensure all loads completed\n",
        "\n",
        "    // Compute from SHARED, write to GLOBAL\n",
        "    if (i < n) {\n",
        "        c[i] = s_a[tx] + s_b[tx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Example data (same size for a, b, c)\n",
        "    const int n = 5;\n",
        "    int h_a[n] = {1, 2, 3, 4, 5};\n",
        "    int h_b[n] = {10, 20, 30, 40, 50};\n",
        "    int h_c[n] = {0};\n",
        "\n",
        "    // Allocate device (GPU) memory\n",
        "    int *d_a = nullptr, *d_b = nullptr, *d_c = nullptr;\n",
        "    cudaMalloc(&d_a, n * sizeof(int));\n",
        "    cudaMalloc(&d_b, n * sizeof(int));\n",
        "    cudaMalloc(&d_c, n * sizeof(int));\n",
        "\n",
        "    // Copy inputs host -> device\n",
        "    cudaMemcpy(d_a, h_a, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch configuration\n",
        "    int threadsPerBlock = 256;         \n",
        "    int numBlocks = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    size_t shmemBytes = 2 * threadsPerBlock * sizeof(int);      // s_a + s_b\n",
        "\n",
        "    // Launch shared-memory kernel\n",
        "    add_shared<<<numBlocks, threadsPerBlock, shmemBytes>>>(d_c, d_a, d_b, n);\n",
        "    cudaDeviceSynchronize();  \n",
        "\n",
        "    // Copy result device -> host\n",
        "    cudaMemcpy(h_c, d_c, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        printf(\"%d + %d = %d\\n\", h_a[i], h_b[i], h_c[i]);\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n",
        "* The main difference in this example is that we specifically create a memory buffer `s` that can be accessed by all threads\n",
        "* This could be useful if we need to repeatedly access the data (although that isn't the case in this example)\n",
        "\n"
      ],
      "metadata": {
        "id": "L0FMVgRRlTE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU Programming in Python\n",
        "* Fortunately, we don't necessarily need to write C or C++ code to use CUDA\n",
        "* There are a few Python packages for writing CUDA code that can take advantage of GPUs\n",
        "* We'll discuss two Python packages in particular\n",
        "\n",
        "#### CuPy — Essentially NumPy on the GPU\n",
        "* What it is: A NumPy-compatible array library whose ops run on the GPU.\n",
        "* How you use it: Replace NumPy as np with cupy as cp\n",
        "* Most NumPy code just works\n",
        "* Under the hood: Calls CUDA libraries (cuBLAS, cuSOLVER, cuFFT, cuSPARSE, cuRAND, etc.).\n",
        "* Capabilities\n",
        "  * Vectorized math, broadcasting, indexing, ufuncs\n",
        "  * Linear algebra (cp.linalg → cuBLAS/cuSOLVER), FFT (cp.fft → cuFFT), random (cp.random → cuRAND), sparse (cupyx.scipy.sparse → cuSPARSE).\n",
        "  * Custom kernels without leaving Python\n",
        "\n",
        "#### Numba - Writing Kernels in Python\n",
        "* What it is: A JIT compiler that turns a subset of Python into CUDA kernels\n",
        "* How you use it: Decorate functions with @cuda.jit (or create CUDA ufuncs with @vectorize(target=\"cuda\")).\n",
        "* Capabilities\n",
        "  * Full thread/block/grid control; launch with [blocks, threads].\n",
        "  * Shared memory, atomics, syncthreads(), warp intrinsics (e.g., shuffles), streams/events.\n",
        "  * Memory management: cuda.to_device, device_array, pinned and managed memory.\n",
        "* Best for writing custom kernels when no library op fits\n",
        "* Offers more fine-grained control over blocks/threads/shared memory—all in Python\n"
      ],
      "metadata": {
        "id": "K6DIcFkpu6nd"
      }
    }
  ]
}