{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f48684b2",
      "metadata": {
        "id": "f48684b2"
      },
      "source": [
        "\n",
        "# Profiling with Scalene\n",
        "\n",
        "**Goals**\n",
        "- Understand what Scalene measures (Python vs. native vs. GPU time, and memory).\n",
        "- Profile mixes of Python, NumPy/Numba, and GPU code paths.\n",
        "- Generate and read HTML reports.\n",
        "- Apply profiling to I/O-heavy workloads (optional Zarr demo).\n",
        "- Profile `mpi4py` programs rank-by-rank."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01cff35d",
      "metadata": {
        "id": "01cff35d"
      },
      "source": [
        "\n",
        "## What Scalene Measures (and why it matters)\n",
        "\n",
        "Scalene is a sampling profiler that attributes time to:\n",
        "- **Python time** (interpreter, GIL-bound)\n",
        "- **Native time** (NumPy/Numba/C-extensions)\n",
        "- **GPU time** (CuPy / Numba CUDA)\n",
        "- **Memory** (alloc/free) per line (where supported)\n",
        "\n",
        "This lets you decide whether to:\n",
        "- Vectorize / JIT (if Python time is high), or\n",
        "- Look at I/O / memory / algorithmic choices (if native/GPU time dominates).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fb2aaf0",
      "metadata": {
        "id": "6fb2aaf0"
      },
      "source": [
        "## Profiler Types: Sampling Profilers vs. Instrumenting Profilers\n",
        "\n",
        "Understanding how profilers collect their data is essential for interpreting results correctly — especially in HPC or hybrid Python + native code environments.\n",
        "\n",
        "\n",
        "### Two Main Types of Profilers\n",
        "\n",
        "| Type | How It Works | Example Tools |\n",
        "|------|---------------|----------------|\n",
        "| **Sampling profiler** | Periodically interrupts a running program and records *where* it is (stack trace). | `Scalene`, `Pyinstrument` |\n",
        "| **Instrumenting profiler** | Modifies or wraps code to record *every function entry/exit* and measure exact times. | `cProfile`, `line_profiler`, `mpiP` (for MPI calls) |\n",
        "\n",
        "\n",
        "### Sampling Profilers\n",
        "\n",
        "**How they work:**  \n",
        "A timer fires at regular intervals (e.g., every 1 ms). Each time:\n",
        "1. The profiler pauses the program briefly.\n",
        "2. Records the current function/line being executed.\n",
        "3. Resumes execution immediately.\n",
        "\n",
        "After many samples, the profiler estimates where most time is spent based on how often each line appears in the samples.\n",
        "\n",
        "**Example (Scalene):**\n",
        "| Line | Python % | Native % | GPU % |\n",
        "|------|-----------|----------|-------|\n",
        "| 12 | 45 | 10 | 0 |\n",
        "| 22 | 2 | 50 | 0 |\n",
        "| 35 | 0 | 0 | 50 |\n",
        "\n",
        "**Advantages**\n",
        "- Very **low overhead** suitable for long HPC runs.  \n",
        "- Works with **JIT** or **native** code (Numba, C/C++, CUDA).  \n",
        "- Can attribute **CPU**, **native**, and **GPU** time separately.  \n",
        "- Doesn’t distort performance (no code rewriting).\n",
        "\n",
        "**Limitations**\n",
        "- **Statistical**, not exact — tiny or rare functions may be missed.  \n",
        "- **Resolution** limited by sampling frequency.  \n",
        "- Results may vary slightly between runs.\n",
        "\n",
        "\n",
        "### Instrumenting Profilers\n",
        "\n",
        "**How they work:**  \n",
        "Profiler inserts code around every function call to record precise start and end times.\n",
        "\n",
        "```python\n",
        "start = time.perf_counter()\n",
        "foo()\n",
        "elapsed = time.perf_counter() - start\n",
        "```\n",
        "\n",
        "**Advantages**\n",
        "* Exact per-function timing and call counts.\n",
        "* Deterministic results (repeatable).\n",
        "* Great for micro-optimizing small sections of code.\n",
        "\n",
        "**Limitations**\n",
        "* High overhead — can slow code 2–10×.\n",
        "* Can’t see into native/JIT code (NumPy, Numba, C).\n",
        "* Poor fit for long or parallel (MPI/GPU) runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fe77204",
      "metadata": {
        "id": "7fe77204"
      },
      "source": [
        "## Pure Python vs. NumPy (native) hotspot demo\n",
        "\n",
        "We'll create two versions of the same computation:\n",
        "- A **pure Python** double loop (intentionally slow).\n",
        "- A **NumPy** vectorized version (native BLAS under the hood).\n",
        "\n",
        "We'll run Scalene **as a CLI** so it generates an HTML report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241a087d",
      "metadata": {
        "id": "241a087d",
        "outputId": "bceb7acf-b4d0-4251-9cea-35634fed1c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting py_vs_numpy.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile py_vs_numpy.py\n",
        "import numpy as np\n",
        "\n",
        "def slow_python(N):\n",
        "    X = np.arange(N**2).reshape(N,N)\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "            X[i,j] = X[i,j]**2\n",
        "    return X\n",
        "\n",
        "def fast_numpy(N):\n",
        "    X = np.arange(N**2).reshape(N,N)\n",
        "    X = X**2\n",
        "    return X\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    slow_python(2000)\n",
        "    fast_numpy(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38bbf67b",
      "metadata": {
        "id": "38bbf67b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Run scalene to create an HTML report (uncomment to execute).\n",
        "# The --html flag writes 'scalene_py_vs_numpy.html' next to the script.\n",
        "!scalene --html --outfile profiles/scalene_py_vs_numpy.html py_vs_numpy.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfb7fd0",
      "metadata": {
        "id": "8bfb7fd0"
      },
      "source": [
        "### Interpretation\n",
        "\n",
        "\n",
        "| **Column** | **Meaning** | **How It’s Measured** | **Interpretation / What to Look For** |\n",
        "|-------------|-------------|------------------------|---------------------------------------|\n",
        "| **% of Time** | Total fraction of wall-clock runtime spent on this line (sum of Python + Native + GPU). | Statistical sampling of stack traces every few ms. | High → major hotspot. Focus optimization here. |\n",
        "| **% Python** | Time spent executing interpreted Python bytecode. | Samples taken while the GIL is held. | High → interpreter-bound → vectorize or JIT with Numba. |\n",
        "| **% Native** | Time spent in compiled/native code (NumPy, C/C++, Fortran, Numba CPU). | Separate sampling thread attributes time outside the GIL. | High → already optimized native code; algorithmic or I/O limits dominate. |\n",
        "| **% System** | Time in OS/kernel calls (I/O, sleep, etc.). | Samples where stack shows system-level frames. | High → I/O-bound or waiting on the OS. |\n",
        "| **% GPU** | Time the GPU was busy executing kernels launched by this line. | Hooks into CUDA runtime; counts active GPU time. | High → GPU compute-bound; check data transfers & kernel efficiency. |\n",
        "| **CPU Mem Avg (MB/s)** | Average rate of host-side allocations/frees while this line was active. | Hooks `malloc`/`free` and Python allocator; computes bytes / sec. | High → heavy allocation throughput. |\n",
        "| **CPU Mem Peak (MB)** | Maximum increase in total process memory after this line executed. | Tracks deltas in process RSS / heap size. | High → transient spikes or potential memory growth/leak. |\n",
        "| **GPU Mem Avg / Peak** |  Device memory allocation rate / peak usage. | CUDA driver hooks — partial support (mainly CuPy). | ? Not sure if this works for Numba CUDA |\n",
        "\n",
        "---\n",
        "\n",
        "###  Reading Tips\n",
        "- **High Python %** → optimize in Python space (Numba/vectorize).  \n",
        "- **High Native %** → library code is doing work; tune algorithms.  \n",
        "- **High GPU %** → GPU dominates; focus on data movement.  \n",
        "- **High CPU Mem Avg/Peak** → frequent or large host allocations.  \n",
        "- **High System %** → I/O or synchronization bottleneck.  \n",
        "- Blank GPU/Memory columns → no allocations or unsupported (normal for Numba CUDA).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1076aa23",
      "metadata": {
        "id": "1076aa23"
      },
      "source": [
        "\n",
        "## Numba (CPU JIT) demo\n",
        "\n",
        "Numba JIT-compiles Python to native code, which Scalene will attribute as **native time**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05e411c",
      "metadata": {
        "id": "b05e411c",
        "outputId": "dfdb1934-2447-48ab-d7a6-0ee138be09bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting numba_cpu.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile numba_cpu.py\n",
        "import numpy as np\n",
        "from numba import njit\n",
        "\n",
        "@njit(cache=True, fastmath=True)\n",
        "def matmul_numba(a, b):\n",
        "    return a @ b\n",
        "\n",
        "def main(n):\n",
        "\n",
        "    # Warmup\n",
        "    a = np.random.rand(10,10)\n",
        "    b = np.random.rand(10,10)\n",
        "    c = matmul_numba\n",
        "\n",
        "    # Test\n",
        "    a = np.random.rand(n, n)\n",
        "    b = np.random.rand(n, n)\n",
        "    c = matmul_numba(a, b)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(1500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7fa1b7c",
      "metadata": {
        "id": "a7fa1b7c"
      },
      "outputs": [],
      "source": [
        "\n",
        "!scalene --html --outfile profiles/scalene_numba_cpu.html numba_cpu.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5675356",
      "metadata": {
        "id": "a5675356"
      },
      "source": [
        "\n",
        "## GPU with Numba CUDA\n",
        "\n",
        "A custom kernel to illustrate kernel launch vs. device compute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3058d191",
      "metadata": {
        "id": "3058d191",
        "outputId": "b4a68c6a-c3f7-441a-c4c1-b1dfdfbade09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting numba_cuda.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile numba_cuda.py\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def f(a, x, y, out):\n",
        "    i = cuda.grid(1)\n",
        "    if i < x.size:\n",
        "        out[i] = a * x[i] + y[i]\n",
        "\n",
        "def main(n):\n",
        "    x = np.random.rand(n).astype(np.float32)\n",
        "    y = np.random.rand(n).astype(np.float32)\n",
        "    out = np.empty_like(x)\n",
        "\n",
        "    d_x = cuda.to_device(x)\n",
        "    d_y = cuda.to_device(y)\n",
        "    d_out = cuda.device_array_like(x)\n",
        "\n",
        "    threads = 256\n",
        "    blocks = (n + threads - 1) // threads\n",
        "    f[blocks, threads](2.0, d_x, d_y, d_out)\n",
        "    cuda.synchronize()\n",
        "    print(\"sum:\", float(d_out.copy_to_host().sum()))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(50000000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4858a3",
      "metadata": {
        "id": "7f4858a3",
        "outputId": "bcca7593-0382-4dc7-f56b-b310dd8bd2b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sum: 75000272.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!scalene --html --outfile profiles/scalene_numba_cuda.html numba_cuda.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1ce707",
      "metadata": {
        "id": "5f1ce707"
      },
      "source": [
        "\n",
        "## Profiling `mpi4py` programs (rank-by-rank)\n",
        "\n",
        "Scalene does not aggregate across ranks automatically when using mpi4py. Hence, The common pattern is to **produce one report per rank**.\n",
        "\n",
        "One approach to deal with this is to use your MPI launcher to start each rank under Scalene and write a per-rank report. Examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5837190d",
      "metadata": {
        "id": "5837190d",
        "outputId": "7c0428fa-f1d3-4679-ba39-fbefc75070c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting mpi_demo.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile mpi_demo.py\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Example MPI program for profiling with Scalene\n",
        "This demonstrates typical MPI operations that you'd want to profile\n",
        "\"\"\"\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def expensive_computation(size):\n",
        "    \"\"\"Simulate some CPU-intensive work\"\"\"\n",
        "    data = np.random.rand(size, size)\n",
        "    result = np.linalg.inv(data @ data.T + np.eye(size))\n",
        "    return result\n",
        "\n",
        "def memory_intensive_operation(size):\n",
        "    \"\"\"Simulate memory allocation\"\"\"\n",
        "    arrays = []\n",
        "    for i in range(10):\n",
        "        arrays.append(np.random.rand(size, size))\n",
        "    return np.sum([arr.sum() for arr in arrays])\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    print(f\"Rank {rank}/{size} starting...\")\n",
        "\n",
        "    # Different work for different ranks\n",
        "    if rank == 0:\n",
        "        # Root does some preparation\n",
        "        data = expensive_computation(1000)\n",
        "        total = memory_intensive_operation(500)\n",
        "        print(f\"Rank 0: Prepared data, total = {total:.2f}\")\n",
        "    else:\n",
        "        # Workers do their own computation\n",
        "        result = expensive_computation(100)\n",
        "        local_sum = memory_intensive_operation(1000)\n",
        "        print(f\"Rank {rank}: Computed result, sum = {local_sum:.2f}\")\n",
        "\n",
        "    # Synchronize\n",
        "    comm.Barrier()\n",
        "\n",
        "    # Gather operation\n",
        "    local_value = rank * 10.0\n",
        "    all_values = comm.gather(local_value, root=0)\n",
        "\n",
        "    if rank == 0:\n",
        "        print(f\"Gathered values: {all_values}\")\n",
        "\n",
        "    # Broadcast operation\n",
        "    if rank == 0:\n",
        "        broadcast_data = np.random.rand(1000, 1000)\n",
        "    else:\n",
        "        broadcast_data = None\n",
        "\n",
        "    broadcast_data = comm.bcast(broadcast_data, root=0)\n",
        "\n",
        "    # Everyone does some work with broadcast data\n",
        "    local_result = np.sum(broadcast_data) * rank\n",
        "\n",
        "    # Reduce operation\n",
        "    total_result = comm.reduce(local_result, op=MPI.SUM, root=0)\n",
        "\n",
        "    if rank == 0:\n",
        "        print(f\"Total result from reduce: {total_result:.2f}\")\n",
        "\n",
        "    print(f\"Rank {rank} completed successfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df529f00",
      "metadata": {
        "id": "df529f00",
        "outputId": "41a60999-d978-495d-b5dd-330cddc7d82e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1/4 starting...\n",
            "Rank 0/4 starting...\n",
            "Rank 2/4 starting...\n",
            "Rank 3/4 starting...\n",
            "Rank 1: Computed result, sum = 4999967.96\n",
            "Rank 3: Computed result, sum = 5000287.63\n",
            "Rank 2: Computed result, sum = 5000328.62\n",
            "Rank 0: Prepared data, total = 1249704.16\n",
            "Gathered values: [0.0, 10.0, 20.0, 30.0]\n",
            "Rank 3 completed successfully\n",
            "Total result from reduce: 3000639.90\n",
            "Rank 0 completed successfully\n",
            "Rank 1 completed successfully\n",
            "Rank 2 completed successfully\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# To profile each rank programmatically:\n",
        "!mpiexec --allow-run-as-root -n 4 bash -c 'scalene --html --outfile profiles/profile-rank-${OMPI_COMM_WORLD_RANK}.html mpi_demo.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc076115",
      "metadata": {
        "id": "cc076115"
      },
      "source": [
        "While `Scalene` can be used to profile individual MPI ranks, it's probably better to use manual timing `MPI.Wtime()` or an MPI specific profiler like `mpip`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e50ba6",
      "metadata": {
        "id": "98e50ba6"
      },
      "source": [
        "# Profiling with `cProfile`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d8995e",
      "metadata": {
        "id": "54d8995e"
      },
      "source": [
        "`cProfile` is an instrumenting profiler. It is not as flexible for handling GPU / native code, but is still a useful tool.\n",
        "\n",
        "| Feature | **cProfile** | **Scalene** |\n",
        "|----------|---------------|--------------|\n",
        "| **Profiling type** | Instrumentation (wraps every function call) | Statistical sampling (periodically interrupts and samples stack) |\n",
        "| **Overhead** | Moderate–high (2–10× slowdown possible) | Low (suitable for long/HPC jobs) |\n",
        "| **Granularity** | Function-level (per call count & time) | Line-level (per line, with Python vs. Native vs. GPU breakdown) |\n",
        "| **Accuracy** | Exact per-call timing | Statistical estimates (approximate but representative) |\n",
        "| **Sees NumPy/Numba internals?** | No – counts as one call | Yes – marked as “Native” time |\n",
        "| **Memory profiling** | No | Yes (host allocations, MB/s + peak) |\n",
        "| **GPU profiling** | No | Yes (CUDA/CuPy kernels) |\n",
        "| **Best for** | Finding *which functions* are slow | Seeing *which lines* dominate and which subsystem (Python vs. Native vs. GPU) |\n",
        "\n",
        "**In practice:**  \n",
        "- Use **`cProfile`** for quick call-frequency/function-level insight.  \n",
        "- Use **`Scalene`** for whole-program, long-run, or mixed CPU/GPU workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1835b9e",
      "metadata": {
        "id": "e1835b9e",
        "outputId": "c88e2977-20a1-4f93-aafd-e0a0ae023593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         9 function calls in 0.080 seconds\n",
            "\n",
            "   Ordered by: internal time\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.048    0.048    0.048    0.048 2606227525.py:4(slow_function)\n",
            "        1    0.030    0.030    0.032    0.032 2606227525.py:11(fast_function)\n",
            "        1    0.002    0.002    0.002    0.002 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "        1    0.000    0.000    0.080    0.080 2606227525.py:17(main)\n",
            "        1    0.000    0.000    0.002    0.002 {method 'sum' of 'numpy.ndarray' objects}\n",
            "        2    0.000    0.000    0.000    0.000 {built-in method numpy.arange}\n",
            "        1    0.000    0.000    0.002    0.002 _methods.py:49(_sum)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import cProfile, pstats, io\n",
        "import numpy as np\n",
        "\n",
        "def slow_function(n):\n",
        "    s = 0\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            s += (i*j) % 7 / 7.0\n",
        "    return s\n",
        "\n",
        "def fast_function(n):\n",
        "    i = np.arange(n)\n",
        "    j = np.arange(n)\n",
        "    A = (i[:, None] * j[None, :]) % 7\n",
        "    return A.sum() / 7.0\n",
        "\n",
        "def main():\n",
        "    slow_function(600)\n",
        "    fast_function(2000)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pr = cProfile.Profile()\n",
        "    pr.enable()\n",
        "    main()\n",
        "    pr.disable()\n",
        "\n",
        "    # Print summary stats\n",
        "    s = io.StringIO()\n",
        "    pstats.Stats(pr, stream=s).strip_dirs().sort_stats(\"tottime\").print_stats(15)\n",
        "    print(s.getvalue())\n",
        "\n",
        "    # Optional: save for visualization\n",
        "    pr.dump_stats(\"profiles/cprofile_example.prof\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f057c138",
      "metadata": {
        "id": "f057c138"
      },
      "source": [
        "You can use the `snakeviz` package to visualize the profile created by `cProfile`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47590f0a",
      "metadata": {
        "id": "47590f0a"
      },
      "outputs": [],
      "source": [
        "!snakeviz cprofile_example.prof"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}