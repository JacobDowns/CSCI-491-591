{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6/s6728rBWcJXlWE3MlFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/lecture6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced mpi4py Features"
      ],
      "metadata": {
        "id": "slRqa4xRq8AM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In this lecture we'll cover a handful of advanced features in mpi4py\n",
        "* We'll discuss parallel I/O, persistent communication, and one sided communication"
      ],
      "metadata": {
        "id": "S-AXH3DarAOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persistent Communication\n",
        "* In some of examples, particularly the heat equation, we sent many of the same type of message repreatedly in a loop\n",
        "* In such cases, communication can be optimized by using persistent communication, a particular case of nonblocking communication allowing the reduction of the overhead\n",
        "* For point-to-point communication, persistent communication is used by setting up requests with `Send_init` and Recv_init`\n",
        "* In each loop iteration, you would then call `Start` or `Startall` and subsequently `Wait` or `Waitall`\n",
        "\n"
      ],
      "metadata": {
        "id": "3PCVrBNdr8LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage Pattern\n",
        "* Create a request one time\n",
        "```python\n",
        "req_s = comm.Send_init(buf, dest=..., tag=...)\n",
        "req_r = comm.Recv_init(buf, source=..., tage=...)\n",
        "```\n",
        "* In a loop you can repeat the message with the outlines form many times\n",
        "```python\n",
        "req_s.Start()\n",
        "req_r.Start()\n",
        "MPI.Request.Waitall([req_s, req_r])\n",
        "```\n",
        "After you're finished sending messages, clean up with\n",
        "```python\n",
        "req_s.Free()\n",
        "req_r.Free()\n",
        "```\n",
        "* It's a little funky to have to free something in a Python program, but a persistent request creates a `request` that holds onto\n",
        "  * A pointer to the buffer\n",
        "  * Datatype description\n",
        "  * The communicator and tag\n",
        "* Free will tell MPI you're done with these resources and is another reminder of how MPI is a lower level library being wrapped in Python\n"
      ],
      "metadata": {
        "id": "HeZI-879thSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Sending Data in a Ring!\n",
        "* Below, let's look at an example where we have each rank send some information to its left, wrapping around to the last rank"
      ],
      "metadata": {
        "id": "fKeXxf5HvGKY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suj15gkRAZZT"
      },
      "outputs": [],
      "source": [
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "size  = comm.Get_size()\n",
        "\n",
        "right = (rank + 1) % size\n",
        "left  = (rank - 1) % size\n",
        "\n",
        "sendbuf = np.array(rank, dtype='i')   # Will send our rank\n",
        "recvbuf = np.array(-1, dtype='i')     # Will receive from 'left'\n",
        "\n",
        "# Build persistent requests once\n",
        "send_req = comm.Send_init(sendbuf, dest=right, tag=0)\n",
        "recv_req = comm.Recv_init(recvbuf,  source=left,  tag=0)\n",
        "\n",
        "n_iters = 5\n",
        "for it in range(n_iters):\n",
        "    # Optionally update what we send each iter\n",
        "    sendbuf[...] = rank + 100*it\n",
        "\n",
        "    # Start both; then wait for both\n",
        "    send_req.Start()\n",
        "    recv_req.Start()\n",
        "    MPI.Request.Waitall([send_req, recv_req])\n",
        "\n",
        "    print(f\"[iter {it}] rank {rank} got {recvbuf} from {left}\")\n",
        "\n",
        "send_req.Free()\n",
        "recv_req.Free()"
      ]
    }
  ]
}