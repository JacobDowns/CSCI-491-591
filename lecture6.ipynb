{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/lecture6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slRqa4xRq8AM"
      },
      "source": [
        "# Advanced mpi4py Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-AXH3DarAOn"
      },
      "source": [
        "* In this lecture we'll cover a handful of advanced features in mpi4py\n",
        "* We'll discuss, persistent communication, one sided communication, and parallel I/O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PCVrBNdr8LD"
      },
      "source": [
        "### Persistent Communication\n",
        "* In some of our previous examples such as the parallel heat equation, we sent many of the same type of message repreatedly in a loop\n",
        "* In such cases, communication can be optimized by using persistent communication, a particular case of nonblocking communication that reduces the overhead of repreatedly setting up the same communication\n",
        "* For point-to-point communication, persistent communication is used by setting up requests with `Send_init` and `Recv_init`\n",
        "* In each loop iteration, you would then call `Start` or `Startall` and subsequently `Wait` or `Waitall`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeZI-879thSi"
      },
      "source": [
        "#### 1. Usage Pattern\n",
        "* Create a request one time\n",
        "```python\n",
        "req_s = comm.Send_init(buf, dest=..., tag=...)\n",
        "req_r = comm.Recv_init(buf, source=..., tag=...)\n",
        "```\n",
        "* In a loop you can repeat the message with the outlined form many times\n",
        "```python\n",
        "req_s.Start()\n",
        "req_r.Start()\n",
        "MPI.Request.Waitall([req_s, req_r])\n",
        "```\n",
        "After you're finished sending messages, clean up with\n",
        "```python\n",
        "req_s.Free()\n",
        "req_r.Free()\n",
        "```\n",
        "* It's a little funky to have to free something in a Python program, but a persistent communication creates a `request` object that holds onto\n",
        "  * A pointer to the buffer\n",
        "  * Datatype description\n",
        "  * The communicator and tag\n",
        "* Free will tell MPI you're done with these resources\n",
        "* This is another reminder of how MPI is a lower level library being wrapped in Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKeXxf5HvGKY"
      },
      "source": [
        "#### 2. Example: Sending Data in a Ring!\n",
        "* Below, let's look at an example where we have each rank send some information to its left, wrapping around to the last rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suj15gkRAZZT"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "size  = comm.Get_size()\n",
        "\n",
        "right = (rank + 1) % size\n",
        "left  = (rank - 1) % size\n",
        "\n",
        "sendbuf = np.array(rank, dtype='i')   # Will send our rank\n",
        "recvbuf = np.array(-1, dtype='i')     # Will receive from 'left'\n",
        "\n",
        "# Build persistent requests once\n",
        "send_req = comm.Send_init(sendbuf, dest=right, tag=0)\n",
        "recv_req = comm.Recv_init(recvbuf,  source=left,  tag=0)\n",
        "\n",
        "n_iters = 5\n",
        "for it in range(n_iters):\n",
        "    # Optionally update what we send each iter\n",
        "    sendbuf[...] = rank + 100*it\n",
        "\n",
        "    # Start both; then wait for both\n",
        "    send_req.Start()\n",
        "    recv_req.Start()\n",
        "    MPI.Request.Waitall([send_req, recv_req])\n",
        "\n",
        "    print(f\"[iter {it}] rank {rank} got {recvbuf} from {left}\")\n",
        "\n",
        "send_req.Free()\n",
        "recv_req.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fus_5a44XJ7S",
        "outputId": "6b228adc-0999-43ab-a231-0fc8cedcfea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[iter 0] rank 0 got 3 from 3\n",
            "[iter 1] rank 0 got 103 from 3\n",
            "[iter 2] rank 0 got 203 from 3\n",
            "[iter 3] rank 0 got 303 from 3\n",
            "[iter 4] rank 0 got 403 from 3\n",
            "[iter 0] rank 1 got 0 from 0\n",
            "[iter 1] rank 1 got 100 from 0\n",
            "[iter 2] rank 1 got 200 from 0\n",
            "[iter 3] rank 1 got 300 from 0\n",
            "[iter 4] rank 1 got 400 from 0\n",
            "[iter 0] rank 2 got 1 from 1\n",
            "[iter 1] rank 2 got 101 from 1\n",
            "[iter 2] rank 2 got 201 from 1\n",
            "[iter 3] rank 2 got 301 from 1\n",
            "[iter 4] rank 2 got 401 from 1\n",
            "[iter 0] rank 3 got 2 from 2\n",
            "[iter 1] rank 3 got 102 from 2\n",
            "[iter 2] rank 3 got 202 from 2\n",
            "[iter 3] rank 3 got 302 from 2\n",
            "[iter 4] rank 3 got 402 from 2\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U0xCNSSXJ7S"
      },
      "source": [
        "* What fun, we sent some data in a ring!\n",
        "* There is potentially a slight performance benefit from persistent communication and it can help clean up code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwGstNTCXJ7T"
      },
      "source": [
        "# One-Sided Communication\n",
        "* We have stated that MPI doesn't use a shared memory paradigm, hence the necessity of sending messages\n",
        "* However, MPI supports a on-sided communication model using **Remote Memmory Access** (RMA), which behaves similarly to a shared memory model\n",
        "    * In RMA one process (the origin) directly reads from or writes into memory exposed by another procces (the target)\n",
        "    * The target process doesn't need to call a send / receive on the other end\n",
        "* In mpi4py, one-sided operations are available using windows via the `Win`\n",
        "* The main operations for windows are:\n",
        "    * `Put`: write data into a target's windows\n",
        "    * `Get` : Read data from a target's window\n",
        "    * `Accumulate` : atomic fetch and combine into target (sum, max, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why is it Useful\n",
        "* Why would we use one-sided communication as opposed to normal two way communication?\n",
        "\n",
        "* Decoupling synchronization\n",
        "\n",
        "  * In two-way communication, both sender and receiver must call matching operations (e.g., `Send`, `Recv`).\n",
        "  * With one-sided, only the origin process issues the operation; the target doesn't need to explicitly participate\n",
        "  * This leads to simpler control flow for some applications\n",
        "  * Great when receivers don't know in advance who will send them data.\n",
        "* Potential for lower latency and higher throughput\n",
        "  * RMA can take advantage of specialized network hardware for direct memory access (RDMA) when communicating across nodes\n",
        "  * Avoids extra protocol overhead from matching send/recv calls.\n",
        "\n",
        "* With this said two-sided communication is often simpler and preferred over one-sided communication as it works well for many problems and is easier to debug\n",
        "* Moreover two-sided communication supports messages with Python objects, whereas one-sided communication is limited to NumPy arrays"
      ],
      "metadata": {
        "id": "AOkUwbScZCgn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnatWEN7XJ7T"
      },
      "source": [
        "## 1. Synchronization\n",
        "* You'll probably notice that this paradigm is much like a shared memory model for a multithreaded application\n",
        "* This comes with many of the same challenges as threading including race conditions. To contend with this we need some way of handling synchronization\n",
        "* RMA has a couple primary synchronization mechanisms:\n",
        "    * **Fence**: collective barrier-like synchronization\n",
        "    * **Lock / Unlock**: finer control for accessing one target at a time\n",
        "* RMA is not a true shared memory model, but it behaves much like one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6mD9f3SXJ7T"
      },
      "source": [
        "## 2. Example\n",
        "\n",
        "* This first example is very basic\n",
        "* Each process exposes a single integer which can be read or written to\n",
        "* Rank 0 writes into rank 1's window\n",
        "* `Fence` is used for synchronization much like `comm.Barrier()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LliJi7OFXJ7T"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "buf = np.array(rank, dtype='i')\n",
        "win = MPI.Win.Create(buf, comm=comm)\n",
        "\n",
        "if rank == 0:\n",
        "    data = np.array(42, dtype='i')\n",
        "    win.Fence()\n",
        "    win.Put([data, MPI.INT], 1)\n",
        "    win.Fence()\n",
        "elif rank == 1:\n",
        "    win.Fence()\n",
        "    win.Fence()\n",
        "    print(f\"Rank 1 sees {buf.item()}\")\n",
        "\n",
        "win.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O-ubtAqXJ7T",
        "outputId": "3903ee52-3b94-4411-9170-989091259228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1 sees 42\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 2 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl-xT2LnXJ7T"
      },
      "source": [
        "## 3. Synchronization in RMA\n",
        "\n",
        "Synchronization in RMA is a bit complex and requires knowing some terminology.\n",
        "\n",
        "1. **Origin and Target**\n",
        "\n",
        "* When we use the term **origin** or origin process, we mean the rank that initiates the RMA operation\n",
        "* This process calls functions like `Put`, `Get`, and `Accumulate`\n",
        "* In contrast, the **target** or target process is the rank that exposes some memory in a window\n",
        "* Its buffer, created with `Win.Create`, will be read or written to\n",
        "\n",
        "\n",
        "2. **Epochs**\n",
        "* Remote Memory Access (RMA) needs well-defined periods when a window is open for remote operations called epochs\n",
        "* **Access epoch**: from the origin process's perspective (issuing Put, Get, etc.).\n",
        "* **Exposure epoch**: from the target process's perspective (memory is exposed to remote access).\n",
        "* An epoch is always initiated (opened) and terminated (closed) by synchronization calls.\n",
        "\n",
        "3. **Active Target Synchronization**\n",
        "* Both the origin and target participate in starting / ending the epoch\n",
        "* The most typical way of using active target synchronization is with `Fence`\n",
        "* The first call to `win.Fence()` collectively opens an epoch for all ranks\n",
        "* The second call to `win.Fence()` collectively closes it\n",
        "* When fence is used, while the epoch is open, any rank can fetch or read data from windows in any other rank\n",
        "\n",
        "\n",
        "4. **Pasive Target Synchronization**\n",
        "* In passive target synchronization, only the origin participates while the target is unaware\n",
        "* The epoch is initiated with `win.Lock(target, lock_type)` and closed with `win.Unlock(target)`\n",
        "* When the epoch is open you can do `Put`, `Get`, and `Accumulate` on the target rank's exposed memory\n",
        "* There are two types of locks you can use:\n",
        "    * In `MPI.SHARED_LOCK`, multiple origins can hold a shared lock on the same target window\n",
        "    * This is best used in cases where you're doing read-only access (`Get`) or you're doing writes to disjoint regions that don't conflict with each other\n",
        "    * The alternative is `MPI.LOCK_EXCLUSIVE`, in which only one origin can hold an exclusive lock on a target at a time\n",
        "    * MPI ensures no other orgin can open a lock on the target until it's released\n",
        "    * This is best used in cases when you want to update memory where conflicts are possible\n",
        "* Locks aren't mutexes, so they do not prevent the target process from accessing its own window memory\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nk7mVFSXJ7U"
      },
      "source": [
        "### Example of Active Target synchronization\n",
        "\n",
        "* We already saw an example using active target synchronization with `win.Fence()`\n",
        "* Let's look at an example of passive synchronization using `Lock` and `Unlock`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G21lU4HlXJ7U"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "buf = np.array(rank, dtype='i')\n",
        "win = MPI.Win.Create(buf, comm=comm)\n",
        "\n",
        "# Passive target synchronization\n",
        "if rank == 0:\n",
        "    val = np.array(123, dtype='i')\n",
        "    win.Lock(1, MPI.LOCK_SHARED)        # open epoch with rank 1\n",
        "    win.Put([val, MPI.INT], 1)   # origin=0, target=1\n",
        "    win.Flush(1)                        # ensure target sees it\n",
        "    win.Unlock(1)                       # close epoch\n",
        "\n",
        "comm.Barrier()  # just to order printing\n",
        "print(f\"[Lock/Unlock] rank {rank} buf = {buf.item()}\")\n",
        "win.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmQzyIo8XJ7U",
        "outputId": "ce660c01-a7ee-4f1a-bfe0-12dab5b50258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Lock/Unlock] rank 0 buf = 0\n",
            "[Lock/Unlock] rank 1 buf = 123\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 2 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ANL5ULlXJ7U"
      },
      "source": [
        "* In this example, rank 0 is basically using `Lock` to open a channel to communicate with rank 1, then closing the channel with `Unlock`\n",
        "* After the `Flush` operation, we know that the value has been written into rank 1's buffer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2aot4syXJ7U"
      },
      "source": [
        "### Atomic Operations\n",
        "1. In addition to `Get` and `Put` mpi4py has several atomic operations\n",
        "* We have already mentioned one atomic operation for RMA called `Accumulate`\n",
        "* An atomic operation is one that appears to happen all at once without interference from other processes\n",
        "* In MPI RMA, atomic operations are guaranteed to be indivisble\n",
        "* This can be used to avoid race conditions where multiple origins try to update the same target memory at the same time\n",
        "\n",
        "2. What are they used for?\n",
        "* In one-sided communication, multiple ranks may read or write the same window at once.\n",
        "* Without atomics, you'd risk lost updates (e.g., two processes increment a counter and one increment gets overwritten).\n",
        "* Atomics can be used to ensure correctness for:\n",
        "    * Distributed counters\n",
        "    * Locks / flags\n",
        "    * Queues or ticket dispensers\n",
        "    * Reductions (sum, max, etc.)\n",
        "\n",
        "3. Examples:\n",
        "\n",
        "**Accumulate** applies an operation at the target like sum, max, replace, etc.\n",
        "\n",
        "```python\n",
        "# Increment a shared counter at root\n",
        "one = np.array(1, dtype='i')\n",
        "win.Accumulate([one, MPI.INT], ROOT, op=MPI.SUM)\n",
        "```\n",
        "\n",
        " **Get_accumulate** fetches the old value and applies an operation with the origin's value.\n",
        " ```python\n",
        " # Increments a shared counter at root and gets the old counter value to store in old\n",
        "oldval = np.zeros(1, dtype='i')\n",
        "one = np.array(1, dtype='i')\n",
        "win.Get_accumulate([one, MPI.INT], [oldval, MPI.INT],\n",
        "                   ROOT, op=MPI.SUM)\n",
        " ```\n",
        "\n",
        " **Compare_and_swap** is an operation that will replace the value in the target with a new value if it is equal to the current value.\n",
        " ```python\n",
        " # Replaces the value in the root with newval if it is equal to expected value. The value in the root's buffer is returned\n",
        " # regardless of whether the comparison was true or not\n",
        "expected = np.array(5, dtype='i')\n",
        "newval   = np.array(4, dtype='i')\n",
        "oldval   = np.zeros(1, dtype='i')\n",
        "\n",
        "win.Compare_and_swap([newval, MPI.INT],\n",
        "                     [expected, MPI.INT],\n",
        "                     [oldval, MPI.INT],\n",
        "                     ROOT, 0)\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvnIg0tGXJ7U"
      },
      "source": [
        "### Example: Work Stealing Queue\n",
        "\n",
        "* We've seen the use of `ProcessPoolExecutor` in Python where you can create a queue of tasks to be accomplished which will be farmed out to different process\n",
        "* Let's consider how this kind of process pool pattern could be implemented in `mpi4py`\n",
        "* To coordinate differnt processes, the main idea here is that there will be a set of tickets (represented by integers) corresponding to the number of tasks to be accomplished\n",
        "* Processes will independently try to claim a ticket, so they have the privelege of doing a task\n",
        "* In this case, the task is really trivial, the process will just retrieve an integer from the root, square it, and write it back to the root\n",
        "* Squaring some numbers is a silly task to acommplish, but it illustrates a common scheduling pattern in parallel computing\n",
        "* The tasks could easily be replaced with something computationally expensive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBrMZbqNXJ7U"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "\n",
        "ROOT    = 0\n",
        "N_TASKS = 8\n",
        "\n",
        "i32 = np.dtype('i')   # MPI.INT\n",
        "f64 = np.dtype('d')   # MPI.DOUBLE\n",
        "i32B = i32.itemsize\n",
        "f64B = f64.itemsize\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[rank {rank}] {msg}\", flush=True)\n",
        "\n",
        "# -------------------------\n",
        "# Windows & initialization\n",
        "# -------------------------\n",
        "if rank == ROOT:\n",
        "    # pool[0] = counter (remaining tasks)\n",
        "    # pool[1:] = task payloads (just integers here)\n",
        "    pool = np.empty(N_TASKS + 1, dtype=i32)\n",
        "    pool[0] = N_TASKS\n",
        "    pool[1:] = np.arange(1, N_TASKS + 1, dtype=i32)\n",
        "    results = np.full(N_TASKS, np.nan, dtype=f64)\n",
        "else:\n",
        "    pool = None       # workers expose no memory for pool\n",
        "    results = None    # workers expose no memory for results\n",
        "\n",
        "win_pool = MPI.Win.Create(pool,    comm=comm)\n",
        "win_res  = MPI.Win.Create(results, comm=comm)\n",
        "\n",
        "if rank == ROOT:\n",
        "    log(f\"init: counter={pool[0]}, tasks={pool[1:].tolist()}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# Atomic claim (CAS) to avoid overshoot\n",
        "# -----------------------------------------\n",
        "def claim_ticket(win, target, dtype):\n",
        "    \"\"\"\n",
        "    Atomically decrement pool[0] at ROOT only if > 0.\n",
        "    Return ticket index in [0..N_TASKS-1], else -1.\n",
        "    \"\"\"\n",
        "    # Working buffers for the CAS operation\n",
        "    cur    = np.zeros(1, dtype=dtype)   # will hold the current counter value read from ROOT\n",
        "    expect = np.zeros(1, dtype=dtype)   # the value we expect to see (if counter hasn't changed)\n",
        "    new    = np.zeros(1, dtype=dtype)   # the value we want to write if compare succeeds (expect-1)\n",
        "    old    = np.zeros(1, dtype=dtype)   # CAS will return whatever was actually in the target\n",
        "\n",
        "    while True:\n",
        "        # Step 1: read the current counter on ROOT into cur\n",
        "        win.Lock(target, MPI.LOCK_SHARED)\n",
        "        win.Get([cur, MPI.INT], target, 0)   # displacement=0 bytes -> pool[0]\n",
        "        win.Flush(target)\n",
        "        win.Unlock(target)\n",
        "\n",
        "        c = int(cur[0])   # current number of remaining tasks\n",
        "        if c <= 0:\n",
        "            return -1     # no tasks left\n",
        "\n",
        "        # Step 2: prepare expected and new values\n",
        "        expect[0] = c     # we think the counter is still c\n",
        "        new[0]    = c - 1 # we want to write back c-1 (decrement by one)\n",
        "\n",
        "        # Step 3: attempt CAS at ROOT\n",
        "        win.Lock(target, MPI.LOCK_EXCLUSIVE)\n",
        "        win.Compare_and_swap([new, MPI.INT],     # proposed new value\n",
        "                             [expect, MPI.INT],  # expected current value\n",
        "                             [old, MPI.INT],     # gets the actual old value\n",
        "                             target, 0)\n",
        "        win.Flush(target)\n",
        "        win.Unlock(target)\n",
        "\n",
        "        # Step 4: check whether CAS succeeded\n",
        "        if int(old[0]) == c:\n",
        "            # success: we got the ticket corresponding to this claim\n",
        "            return c - 1\n",
        "        # else: someone else beat us to it, retry loop\n",
        "\n",
        "# -----------------------------------------\n",
        "# Workers do the work\n",
        "# -----------------------------------------\n",
        "if rank != ROOT:\n",
        "    while True:\n",
        "        ticket = claim_ticket(win_pool, ROOT, i32)\n",
        "        if ticket < 0:\n",
        "            log(\"no work left; exiting\")\n",
        "            break\n",
        "\n",
        "        # pool[1 + ticket] holds the payload for this task\n",
        "        disp_bytes = (1 + ticket) * i32B\n",
        "        task = np.zeros(1, dtype=i32)\n",
        "\n",
        "        win_pool.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "        win_pool.Get([task, MPI.INT], ROOT, disp_bytes)\n",
        "        win_pool.Flush(ROOT)\n",
        "        win_pool.Unlock(ROOT)\n",
        "\n",
        "        payload = int(task[0])\n",
        "        log(f\"claimed ticket={ticket}, task={payload}\")\n",
        "\n",
        "        # Do some work (here: just square it)\n",
        "        result_val = float(payload * payload)\n",
        "\n",
        "        # Write result back to ROOT's results[ticket]\n",
        "        res_disp = ticket * f64B\n",
        "        res_buf  = np.array([result_val], dtype=f64)\n",
        "\n",
        "        win_res.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "        win_res.Put([res_buf, MPI.DOUBLE], ROOT, res_disp)\n",
        "        win_res.Flush(ROOT)\n",
        "        win_res.Unlock(ROOT)\n",
        "\n",
        "        log(f\"processed task {payload} -> result {result_val:.1f}\")\n",
        "\n",
        "comm.Barrier()\n",
        "\n",
        "if rank == ROOT:\n",
        "    filled = int(np.count_nonzero(~np.isnan(results)))\n",
        "    log(f\"final counter={pool[0]} (should be 0)\")\n",
        "    log(f\"results filled: {filled}/{N_TASKS}\")\n",
        "    log(f\"results: {results.tolist()}\")\n",
        "\n",
        "win_pool.Free()\n",
        "win_res.Free()\n",
        "\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9phIVD8XJ7U",
        "outputId": "c115115a-f85b-473a-910d-302e2db824be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[rank 0] init: counter=8, tasks=[1, 2, 3, 4, 5, 6, 7, 8]\n",
            "[rank 1] claimed ticket=6, task=7\n",
            "[rank 1] processed task 7 -> result 49.0\n",
            "[rank 1] claimed ticket=4, task=5\n",
            "[rank 1] processed task 5 -> result 25.0\n",
            "[rank 2] claimed ticket=5, task=6\n",
            "[rank 2] processed task 6 -> result 36.0\n",
            "[rank 3] claimed ticket=7, task=8\n",
            "[rank 3] processed task 8 -> result 64.0\n",
            "[rank 3] claimed ticket=3, task=4\n",
            "[rank 3] processed task 4 -> result 16.0\n",
            "[rank 2] claimed ticket=2, task=3\n",
            "[rank 1] claimed ticket=1, task=2\n",
            "[rank 2] processed task 3 -> result 9.0\n",
            "[rank 1] processed task 2 -> result 4.0\n",
            "[rank 2] no work left; exiting\n",
            "[rank 3] claimed ticket=0, task=1\n",
            "[rank 1] no work left; exiting\n",
            "[rank 3] processed task 1 -> result 1.0\n",
            "[rank 3] no work left; exiting\n",
            "[rank 0] final counter=0 (should be 0)\n",
            "[rank 0] results filled: 8/8\n",
            "[rank 0] results: [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0]\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT2qXUgJXJ7U"
      },
      "source": [
        "This example is a little bit more complex, so let's walk through it. First, this will initialize some constants.\n",
        "\n",
        "```python\n",
        "ROOT    = 0\n",
        "N_TASKS = 8\n",
        "\n",
        "i32 = np.dtype('i')   # MPI.INT\n",
        "f64 = np.dtype('d')   # MPI.DOUBLE\n",
        "i32B = i32.itemsize\n",
        "f64B = f64.itemsize\n",
        "```\n",
        " Here, `ROOT` is defining the root node, which will essentially serve as the ticket master and data keeper. `N_TASKS` is be the number of tasks to accomplish. Getting the sizes of integer and floating point data types will be used to help MPI figure out where to read and write in memory buffers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCElMBIGXJ7U"
      },
      "source": [
        "On the root, we create an array of integers called pool. The first element of pool is going to contain the number of tasks. Subsequent elements in pool will contain the workload (e.g. the numbers we want to square). We will also create a results buffer on the root where we'll store the results of each task (that is, the results of squaring each number). On all other ranks, the pool is defined `None` which is a useful way to tell MPI that these ranks should expose no memory.\n",
        "```python\n",
        "if rank == ROOT:\n",
        "    # pool[0] = counter (remaining tasks)\n",
        "    # pool[1:] = task payloads (just integers here)\n",
        "    pool = np.empty(N_TASKS + 1, dtype=i32)\n",
        "    pool[0] = N_TASKS\n",
        "    pool[1:] = np.arange(1, N_TASKS + 1, dtype=i32)\n",
        "    results = np.full(N_TASKS, np.nan, dtype=f64)\n",
        "else:\n",
        "    pool = None       # workers expose no memory for pool\n",
        "    results = None    # workers expose no memory for results\n",
        "```\n",
        "\n",
        "```python\n",
        "else:\n",
        "    pool = None       # expose zero bytes on workers\n",
        "    results = None    # expose zero bytes on workers\n",
        "```\n",
        "\n",
        "Note that altough some ranks may not expose any memory, all ranks need to call\n",
        "```python\n",
        "win_pool = MPI.Win.Create(pool,    comm=comm)\n",
        "win_res  = MPI.Win.Create(results, comm=comm)\n",
        "```\n",
        "to facilitate communication, since these are still considered collective communications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG6Y9YI6XJ7V"
      },
      "source": [
        "Next, let's break down the `claim_ticket` function. In this function, each process will rush to claim a ticket that can be redeemed for a unit of work. If the rank is successful, it will return the integer corresponding the claimed ticket (or actually the index of the integer to square, but essentially this corresponds to its ticket number). Otherwise, it will return -1, meaning there is no additional work to complete.\n",
        "\n",
        "The arguments to this function include a window `win`, which will be needed to communicate with the root process. As the target, we will be passing in `ROOT`, since each process will need to get information from the root. The dtype argument will be used to pass in the data type of the `pool` buffer so we can index it properly.  \n",
        "\n",
        "```python\n",
        "def claim_ticket(win, target, dtype):\n",
        "    \"\"\"\n",
        "    Atomically decrement pool[0] at ROOT only if > 0.\n",
        "    Return ticket index in [0..N_TASKS-1], else -1.\n",
        "    \"\"\"\n",
        "    # Working buffers for the CAS operation\n",
        "    cur    = np.zeros(1, dtype=dtype)   # will hold the current counter value read from ROOT\n",
        "    expect = np.zeros(1, dtype=dtype)   # the value we expect to see (if counter hasn't changed)\n",
        "    new    = np.zeros(1, dtype=dtype)   # the value we want to write if compare succeeds (expect-1)\n",
        "    old    = np.zeros(1, dtype=dtype)   # CAS will return whatever was actually in the target\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSKQdOdlXJ7V"
      },
      "source": [
        "The buffers `cur`, `expect`, `new`, and `old` will be used in the ticket claiming logic, which is contained in the while loop below. At every iteration of this loop, the first step is to read the current value of the ticket counter on the root, and store this value in `c`. We then check if this value is $\\leq$ 0, in which case we know that there are no more tasks to complete so we can return.\n",
        "\n",
        "```python\n",
        "# Step 1: read the current counter on ROOT into cur\n",
        "win.Lock(target, MPI.LOCK_SHARED)\n",
        "win.Get([cur, MPI.INT], target, 0)   # displacement=0 bytes -> pool[0]\n",
        "win.Flush(target)\n",
        "win.Unlock(target)\n",
        "\n",
        "c = int(cur[0])   # current number of remaining tasks\n",
        "    if c <= 0:\n",
        "        return -1     # no tasks left\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH2jBERAXJ7V"
      },
      "source": [
        "We then need some logic to deal with the fact that multiple processes could potentially be working to claim the same ticket. For a given rank, once we've retrieved the number of tickets remaining, we can check in with the root and see if that value has changed. If it has, some other process has claimed that ticket. Otherwise, we need to decrement the number of tickets on the root and then actually do the work for that ticket.\n",
        "\n",
        "We'll accomplish this using some code that uses the `Compare_and_swap` operation (CAS).\n",
        "\n",
        "```python\n",
        "# Step 2: prepare expected and new values\n",
        "expect[0] = c     # we think the counter is still c\n",
        "new[0]    = c - 1 # we want to write back c-1 (decrement by one)\n",
        "\n",
        "# Step 3: attempt CAS at ROOT\n",
        "win.Lock(target, MPI.LOCK_EXCLUSIVE)\n",
        "win.Compare_and_swap([new, MPI.INT],     # proposed new value\n",
        "                        [expect, MPI.INT],  # expected current value\n",
        "                        [old, MPI.INT],     # gets the actual old value\n",
        "                        target, 0)\n",
        "win.Flush(target)\n",
        "win.Unlock(target)\n",
        "```\n",
        "\n",
        "Here, for a given rank, we expect that when we check back in with the root after reading the counter, that the value should still be the same. That is, we expect that the counter value in root is equal to the `c`. If so, then great! That means we can claim that ticket, decrement the ticket counter, and move on to doing some work.\n",
        "\n",
        " But what would happen if the counter value in the root changed between we intially read it and we checked in again with the root?  Well, in that case some other process claimed the ticket, and we missed our chance. In this case, we don't want to decrement the ticket counter on the root.\n",
        "\n",
        " This is the logic of using `Compare_and_swap` in the above code. If the value on the root is what we expect, we'll decrement the counter return the ticket number. Otherwise, we'll keep trying to claim a ticket unless there's no more work to be done. After the `Compare_and_swap`, we just need to verify what happened:\n",
        "\n",
        " ```python\n",
        " # Step 4: check whether CAS succeeded\n",
        "if int(old[0]) == c:\n",
        "    # success: we got the ticket corresponding to this claim\n",
        "    return c - 1\n",
        "```\n",
        "If our rank successfully claimed the ticket, then the value returned in the CAS operation will be equal to the old counter value. In this case, we don't return our ticket number directly, but rather the index into the pool array on the root, which we'll use to read the integer that we need to square."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V146WW1XJ7V"
      },
      "source": [
        "Now that we understand `claim_ticket`, let's take a look at the main work loop. If the `claim_ticket` function returns, then there are one of two possibilites. If it returns a value of -1, then there was not work so we can exit the loop.\n",
        "```python\n",
        "while True:\n",
        "    ticket = claim_ticket(win_pool, ROOT, i32)\n",
        "    if ticket < 0:\n",
        "        log(\"no work left; exiting\")\n",
        "        break\n",
        "```\n",
        "Otherwise, we will need to get the work unit associated with the ticket, which, in this case, is just an integer to square. The `Get` function needs the offset in bytes in the pool buffer on root to read the correct integer.\n",
        "\n",
        "```python\n",
        "# pool[1 + ticket] holds the payload for this task\n",
        "disp_bytes = (1 + ticket) * i32B\n",
        "task = np.zeros(1, dtype=i32)\n",
        "\n",
        "win_pool.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "win_pool.Get([task, MPI.INT], ROOT, disp_bytes)\n",
        "win_pool.Flush(ROOT)\n",
        "win_pool.Unlock(ROOT)\n",
        "\n",
        "payload = int(task[0])\n",
        "log(f\"claimed ticket={ticket}, task={payload}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaKJsvjzXJ7V"
      },
      "source": [
        "Once we have the payload, we can do the \"work\":\n",
        "```python\n",
        "# Do some work (here: just square it)\n",
        "result_val = float(payload * payload)\n",
        "```\n",
        "\n",
        "The result can be written into the results buffer on the root. Here we needed to account for the fact that the result buffer was of float type instead of integer type.\n",
        "```python\n",
        "# Write result back to ROOT's results[ticket]\n",
        "res_disp = ticket * f64B\n",
        "res_buf  = np.array([result_val], dtype=f64)\n",
        "\n",
        "win_res.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "win_res.Put([res_buf, MPI.DOUBLE], ROOT, res_disp)\n",
        "win_res.Flush(ROOT)\n",
        "win_res.Unlock(ROOT)\n",
        "\n",
        "log(f\"processed task {payload} -> result {result_val:.1f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWfVyVdMXJ7V"
      },
      "source": [
        "Now the resulting squared integer is stored in its correct slot in the results array!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}