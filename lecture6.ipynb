{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/lecture6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slRqa4xRq8AM"
      },
      "source": [
        "# Advanced mpi4py Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-AXH3DarAOn"
      },
      "source": [
        "* In this lecture we'll cover a handful of advanced features in mpi4py\n",
        "* We'll discuss parallel I/O, persistent communication, and one sided communication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PCVrBNdr8LD"
      },
      "source": [
        "### Persistent Communication\n",
        "* In some of examples, particularly the heat equation, we sent many of the same type of message repreatedly in a loop\n",
        "* In such cases, communication can be optimized by using persistent communication, a particular case of nonblocking communication allowing the reduction of the overhead\n",
        "* For point-to-point communication, persistent communication is used by setting up requests with `Send_init` and Recv_init`\n",
        "* In each loop iteration, you would then call `Start` or `Startall` and subsequently `Wait` or `Waitall`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeZI-879thSi"
      },
      "source": [
        "#### 1. Usage Pattern\n",
        "* Create a request one time\n",
        "```python\n",
        "req_s = comm.Send_init(buf, dest=..., tag=...)\n",
        "req_r = comm.Recv_init(buf, source=..., tage=...)\n",
        "```\n",
        "* In a loop you can repeat the message with the outlines form many times\n",
        "```python\n",
        "req_s.Start()\n",
        "req_r.Start()\n",
        "MPI.Request.Waitall([req_s, req_r])\n",
        "```\n",
        "After you're finished sending messages, clean up with\n",
        "```python\n",
        "req_s.Free()\n",
        "req_r.Free()\n",
        "```\n",
        "* It's a little funky to have to free something in a Python program, but a persistent request creates a `request` that holds onto\n",
        "  * A pointer to the buffer\n",
        "  * Datatype description\n",
        "  * The communicator and tag\n",
        "* Free will tell MPI you're done with these resources and is another reminder of how MPI is a lower level library being wrapped in Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKeXxf5HvGKY"
      },
      "source": [
        "#### 2. Example: Sending Data in a Ring!\n",
        "* Below, let's look at an example where we have each rank send some information to its left, wrapping around to the last rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "suj15gkRAZZT"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "size  = comm.Get_size()\n",
        "\n",
        "right = (rank + 1) % size\n",
        "left  = (rank - 1) % size\n",
        "\n",
        "sendbuf = np.array(rank, dtype='i')   # Will send our rank\n",
        "recvbuf = np.array(-1, dtype='i')     # Will receive from 'left'\n",
        "\n",
        "# Build persistent requests once\n",
        "send_req = comm.Send_init(sendbuf, dest=right, tag=0)\n",
        "recv_req = comm.Recv_init(recvbuf,  source=left,  tag=0)\n",
        "\n",
        "n_iters = 5\n",
        "for it in range(n_iters):\n",
        "    # Optionally update what we send each iter\n",
        "    sendbuf[...] = rank + 100*it\n",
        "\n",
        "    # Start both; then wait for both\n",
        "    send_req.Start()\n",
        "    recv_req.Start()\n",
        "    MPI.Request.Waitall([send_req, recv_req])\n",
        "\n",
        "    print(f\"[iter {it}] rank {rank} got {recvbuf} from {left}\")\n",
        "\n",
        "send_req.Free()\n",
        "recv_req.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[iter 0] rank 0 got 3 from 3\n",
            "[iter 1] rank 0 got 103 from 3\n",
            "[iter 2] rank 0 got 203 from 3\n",
            "[iter 3] rank 0 got 303 from 3\n",
            "[iter 4] rank 0 got 403 from 3\n",
            "[iter 0] rank 1 got 0 from 0\n",
            "[iter 1] rank 1 got 100 from 0\n",
            "[iter 2] rank 1 got 200 from 0\n",
            "[iter 3] rank 1 got 300 from 0\n",
            "[iter 4] rank 1 got 400 from 0\n",
            "[iter 0] rank 2 got 1 from 1\n",
            "[iter 1] rank 2 got 101 from 1\n",
            "[iter 2] rank 2 got 201 from 1\n",
            "[iter 3] rank 2 got 301 from 1\n",
            "[iter 4] rank 2 got 401 from 1\n",
            "[iter 0] rank 3 got 2 from 2\n",
            "[iter 1] rank 3 got 102 from 2\n",
            "[iter 2] rank 3 got 202 from 2\n",
            "[iter 3] rank 3 got 302 from 2\n",
            "[iter 4] rank 3 got 402 from 2\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* What fun, we sent some data in a ring!\n",
        "* There is potentially a slight performance benefit from persistent communication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# One-Sided Communication\n",
        "* We have stated that MPI doesn't use a shared memory paradigm, hence the necessity of sending messages\n",
        "* However, this is not entirely true, as MPI supports a on-sided communication model using **Remote Memmory Access** (RMA)\n",
        "    * In RMA one cprocess (the origin) directly reads from or writes into memory exposed by another procces (the target)\n",
        "    * The target process doesn't need to call a send / receive on the other end\n",
        "* In mpi4py, one-sided operations are available using windows via the `Win` \n",
        "* The main operations for windows are:\n",
        "    * `Put`: write data into a target's windows\n",
        "    * `Get` : Read data from a target's window\n",
        "    * `Accumulate` : atomic fetch and combine into target (sum, max, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Synchronization\n",
        "* You'll probably notice that this paradigm is much like a shared memory model for a multithreaded application\n",
        "* This comes with many of the same challenges as threading including race conditions\n",
        "* RMA has a couple primary synchronization mechanisms:\n",
        "    * **Fence**: collective barrier-like synchronization\n",
        "    * **Lock / Unlock**: finer control for accessing one target at a time\n",
        "* RMA is not a true shared memory model, but it behaves much like one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Example\n",
        "\n",
        "* This first example is very basic \n",
        "* Each process exposes a single integer which can be read or written to\n",
        "* Rank 0 writes into rank 1's window\n",
        "* `Fence` is used for synchronization much like `comm.Barrier()` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "buf = np.array(rank, dtype='i')        \n",
        "win = MPI.Win.Create(buf, comm=comm)\n",
        "\n",
        "if rank == 0:\n",
        "    data = np.array(42, dtype='i')     \n",
        "    win.Fence()\n",
        "    win.Put([data, MPI.INT], 1)\n",
        "    win.Fence()\n",
        "elif rank == 1:\n",
        "    win.Fence()\n",
        "    win.Fence()\n",
        "    print(f\"Rank 1 sees {buf.item()}\") \n",
        "    \n",
        "win.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1 sees 42\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 2 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Synchronization in RMA\n",
        "\n",
        "Synchronization in RMA is a bit complex and requires some terminology.\n",
        "\n",
        "1. **Origin and Target**\n",
        "\n",
        "* When we use the term **origin** or origin process, we mean the rank that initiates the RMA operation \n",
        "* This process calls functions like `Put`, `Get`, and `Accumulate`\n",
        "* In contrast, the **target** or target process is the rank that exposes some memory in a window\n",
        "* Its buffer created with `Win.Create` will be read or written to \n",
        "\n",
        "\n",
        "2. **Epochs**\n",
        "* Remote Memory Access (RMA) needs well-defined periods when a window is open for remote operations called epochs\n",
        "* **Access epoch**: from the origin process’s perspective (issuing Put, Get, etc.).\n",
        "* **Exposure epoch**: from the target process’s perspective (memory is exposed to remote access).\n",
        "* An epoch is always initiated (opened) and terminated (closed) by synchronization calls.\n",
        "\n",
        "> Analogy: Epochs are like opening/closing office hours. Students (origins) can ask questions only when the office door is open. When it’s closed, no access.\n",
        "\n",
        "3. **Active Target Synchronization**\n",
        "* Both the origin and target participate in starting / ending the epoch\n",
        "* The most typical way of using active target synchronization is with `Fence`\n",
        "* The first call to `win.Fence()` collectively opens an epoch for all ranks \n",
        "* The second call to `win.Fence()` collectively closes it \n",
        "* When fence is used, while the epoch is open, any rank can fetch or read data from windows in any other rank\n",
        "\n",
        "\n",
        "4. **Pasive Target Synchronization**\n",
        "* In passive target synchronization, only the origin participates while the target is unaware\n",
        "* The epoch is initiated with `win.Lock(target, lock_type)` and closed with `win.Unlock(target)`\n",
        "* When the epoch is open you can do `Put`, `Get`, and `Accumulate` on the target rank's exposed memory\n",
        "* There are two types of locks you can use in `win.Lock`:\n",
        "    * In `MPI.SHARED_LOCK`, multiple origin can hold a shared lock on the same target window\n",
        "    * This is best used in cases where you're doing read-only access (`Get`) or you're doing writes to disjoint regions that don't conflict with each other\n",
        "    * The alternative is `MPI.LOCK_EXCLUSIVE`, in which only one origin can hold an exclusive lock on a target at a time\n",
        "    * MPI ensures no other orgin can open a lock on the target until it's released\n",
        "    * This is best used in cases when you want to update memory where conflicts are possible \n",
        "* Locks aren't mutexes, so they do not prevent the target process from accessing its own window memory\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example of Active Target synchronization\n",
        "\n",
        "* We already saw an example using active synchronization with fence\n",
        "* Let's look at an example of passive synchronization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "buf = np.array(rank, dtype='i')\n",
        "win = MPI.Win.Create(buf, comm=comm)\n",
        "\n",
        "# Passive target sycnrhonization\n",
        "if rank == 0:\n",
        "    val = np.array(123, dtype='i')\n",
        "    win.Lock(1, MPI.LOCK_SHARED)        # open epoch with rank 1\n",
        "    win.Put([val, MPI.INT], 1)   # origin=0, target=1\n",
        "    win.Flush(1)                        # ensure target sees it\n",
        "    win.Unlock(1)                       # close epoch\n",
        "\n",
        "comm.Barrier()  # just to order printing\n",
        "print(f\"[Lock/Unlock] rank {rank} buf = {buf.item()}\")\n",
        "win.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Lock/Unlock] rank 0 buf = 0\n",
            "[Lock/Unlock] rank 1 buf = 123\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 2 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In this example, rank 0 is basically using `Lock` to open a channel to communicate with rank 1, then closing the channel with `Unlock`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Atomic Operations\n",
        "1. What are they?\n",
        "* We have already mentioned one atomic operation for RMA called `Accumulate`\n",
        "* An atomic operation is one that appears to happen all at once without interference from other processes\n",
        "* In MPI RMA, atomic operations are guaranteed to be indivisble\n",
        "* This can be used to avoid race conditions where multiple origins try to update the same target memory at the same time\n",
        "\n",
        "2. What are they used for?\n",
        "* In one-sided communication, multiple ranks may read or write the same window at once.\n",
        "* Without atomics, you’d risk lost updates (e.g., two processes increment a counter and one increment gets overwritten).\n",
        "* Atomics ensure correctness for:\n",
        "    * Distributed counters\n",
        "    * Locks / flags\n",
        "    * Queues or ticket dispensers\n",
        "    * Reductions (sum, max, etc.)\n",
        "\n",
        "3. Examples:\n",
        "\n",
        "**Accumulate** applies an operation at the target like sum, max, replace, etc. \n",
        "\n",
        "```python\n",
        "# Increment a shared counter at root\n",
        "one = np.array(1, dtype='i')\n",
        "win.Accumulate([one, MPI.INT], ROOT, op=MPI.SUM)\n",
        "```\n",
        "\n",
        " **Get_accumulate** fetches the old value and applies an operation with the origin's value. \n",
        " ```python\n",
        " # Increments a shared counter at root and gets the old counter value to store in old\n",
        "oldval = np.zeros(1, dtype='i')\n",
        "one = np.array(1, dtype='i')\n",
        "win.Get_accumulate([one, MPI.INT], [oldval, MPI.INT],\n",
        "                   ROOT, op=MPI.SUM)\n",
        " ```\n",
        "\n",
        " **Compare_and_swap** is an operation that will replace the value in the target with a new value if it is equal to the current value. \n",
        " ```python\n",
        " # Replaces the value in the root with newval if it is equal to expected value. The value in the root's buffer is returned\n",
        " # regardless of whether the comparison was true or not\n",
        "expected = np.array(5, dtype='i')\n",
        "newval   = np.array(4, dtype='i')\n",
        "oldval   = np.zeros(1, dtype='i')\n",
        "\n",
        "win.Compare_and_swap([newval, MPI.INT],\n",
        "                     [expected, MPI.INT],\n",
        "                     [oldval, MPI.INT],\n",
        "                     ROOT, 0)\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Work Stealing\n",
        "\n",
        "* We've seen the use of `ProcessPoolExecutor` in Python\n",
        "* Let's consider an example of how this kind of pattern could be implemented in `mpi4py`\n",
        "* The idea here is that there will be a set of tickets represented by integers, corresponding to the number of tasks\n",
        "* Processes will try to claim a ticket, so they have the privelage of squaring a number (what an honor!)\n",
        "* Results (the squares) will be written to a buffer on the root\n",
        "* Squaring some numbers is a silly task to acommplish, but it illustrates a common scheduling pattern in parallel computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "\n",
        "ROOT    = 0\n",
        "N_TASKS = 8\n",
        "\n",
        "i32 = np.dtype('i')   # MPI.INT\n",
        "f64 = np.dtype('d')   # MPI.DOUBLE\n",
        "i32B = i32.itemsize\n",
        "f64B = f64.itemsize\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[rank {rank}] {msg}\", flush=True)\n",
        "\n",
        "# -------------------------\n",
        "# Windows & initialization\n",
        "# -------------------------\n",
        "if rank == ROOT:\n",
        "    # pool[0] = counter (remaining tasks)\n",
        "    # pool[1:] = task payloads (just integers here)\n",
        "    pool = np.empty(N_TASKS + 1, dtype=i32)\n",
        "    pool[0] = N_TASKS\n",
        "    pool[1:] = np.arange(1, N_TASKS + 1, dtype=i32)\n",
        "    results = np.full(N_TASKS, np.nan, dtype=f64)\n",
        "else:\n",
        "    pool = None       # workers expose no memory for pool\n",
        "    results = None    # workers expose no memory for results\n",
        "\n",
        "win_pool = MPI.Win.Create(pool,    comm=comm)\n",
        "win_res  = MPI.Win.Create(results, comm=comm)\n",
        "\n",
        "if rank == ROOT:\n",
        "    log(f\"init: counter={pool[0]}, tasks={pool[1:].tolist()}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# Atomic claim (CAS) to avoid overshoot\n",
        "# -----------------------------------------\n",
        "def claim_ticket(win, target, dtype):\n",
        "    \"\"\"\n",
        "    Atomically decrement pool[0] at ROOT only if > 0.\n",
        "    Return ticket index in [0..N_TASKS-1], else -1.\n",
        "    \"\"\"\n",
        "    # Working buffers for the CAS operation\n",
        "    cur    = np.zeros(1, dtype=dtype)   # will hold the current counter value read from ROOT\n",
        "    expect = np.zeros(1, dtype=dtype)   # the value we expect to see (if counter hasn't changed)\n",
        "    new    = np.zeros(1, dtype=dtype)   # the value we want to write if compare succeeds (expect-1)\n",
        "    old    = np.zeros(1, dtype=dtype)   # CAS will return whatever was actually in the target\n",
        "\n",
        "    while True:\n",
        "        # Step 1: read the current counter on ROOT into cur\n",
        "        win.Lock(target, MPI.LOCK_SHARED)\n",
        "        win.Get([cur, MPI.INT], target, 0)   # displacement=0 bytes -> pool[0]\n",
        "        win.Flush(target)\n",
        "        win.Unlock(target)\n",
        "\n",
        "        c = int(cur[0])   # current number of remaining tasks\n",
        "        if c <= 0:\n",
        "            return -1     # no tasks left\n",
        "\n",
        "        # Step 2: prepare expected and new values\n",
        "        expect[0] = c     # we think the counter is still c\n",
        "        new[0]    = c - 1 # we want to write back c-1 (decrement by one)\n",
        "\n",
        "        # Step 3: attempt CAS at ROOT\n",
        "        win.Lock(target, MPI.LOCK_EXCLUSIVE)\n",
        "        win.Compare_and_swap([new, MPI.INT],     # proposed new value\n",
        "                             [expect, MPI.INT],  # expected current value\n",
        "                             [old, MPI.INT],     # gets the actual old value\n",
        "                             target, 0)\n",
        "        win.Flush(target)\n",
        "        win.Unlock(target)\n",
        "\n",
        "        # Step 4: check whether CAS succeeded\n",
        "        if int(old[0]) == c:\n",
        "            # success: we got the ticket corresponding to this claim\n",
        "            return c - 1\n",
        "        # else: someone else beat us to it, retry loop\n",
        "\n",
        "# -----------------------------------------\n",
        "# Workers do the work\n",
        "# -----------------------------------------\n",
        "if rank != ROOT:\n",
        "    while True:\n",
        "        ticket = claim_ticket(win_pool, ROOT, i32)\n",
        "        if ticket < 0:\n",
        "            log(\"no work left; exiting\")\n",
        "            break\n",
        "\n",
        "        # pool[1 + ticket] holds the payload for this task\n",
        "        disp_bytes = (1 + ticket) * i32B\n",
        "        task = np.zeros(1, dtype=i32)\n",
        "\n",
        "        win_pool.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "        win_pool.Get([task, MPI.INT], ROOT, disp_bytes)\n",
        "        win_pool.Flush(ROOT)\n",
        "        win_pool.Unlock(ROOT)\n",
        "\n",
        "        payload = int(task[0])\n",
        "        log(f\"claimed ticket={ticket}, task={payload}\")\n",
        "\n",
        "        # Do some work (here: just square it)\n",
        "        result_val = float(payload * payload)\n",
        "\n",
        "        # Write result back to ROOT's results[ticket]\n",
        "        res_disp = ticket * f64B\n",
        "        res_buf  = np.array([result_val], dtype=f64)\n",
        "\n",
        "        win_res.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "        win_res.Put([res_buf, MPI.DOUBLE], ROOT, res_disp)\n",
        "        win_res.Flush(ROOT)\n",
        "        win_res.Unlock(ROOT)\n",
        "\n",
        "        log(f\"processed task {payload} -> result {result_val:.1f}\")\n",
        "\n",
        "comm.Barrier()\n",
        "\n",
        "if rank == ROOT:\n",
        "    filled = int(np.count_nonzero(~np.isnan(results)))\n",
        "    log(f\"final counter={pool[0]} (should be 0)\")\n",
        "    log(f\"results filled: {filled}/{N_TASKS}\")\n",
        "    log(f\"results: {results.tolist()}\")\n",
        "\n",
        "win_pool.Free()\n",
        "win_res.Free()\n",
        "\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[rank 0] init: counter=8, tasks=[1, 2, 3, 4, 5, 6, 7, 8]\n",
            "[rank 1] claimed ticket=6, task=7\n",
            "[rank 1] processed task 7 -> result 49.0\n",
            "[rank 1] claimed ticket=4, task=5\n",
            "[rank 1] processed task 5 -> result 25.0\n",
            "[rank 2] claimed ticket=5, task=6\n",
            "[rank 2] processed task 6 -> result 36.0\n",
            "[rank 3] claimed ticket=7, task=8\n",
            "[rank 3] processed task 8 -> result 64.0\n",
            "[rank 3] claimed ticket=3, task=4\n",
            "[rank 3] processed task 4 -> result 16.0\n",
            "[rank 2] claimed ticket=2, task=3\n",
            "[rank 1] claimed ticket=1, task=2\n",
            "[rank 2] processed task 3 -> result 9.0\n",
            "[rank 1] processed task 2 -> result 4.0\n",
            "[rank 2] no work left; exiting\n",
            "[rank 3] claimed ticket=0, task=1\n",
            "[rank 1] no work left; exiting\n",
            "[rank 3] processed task 1 -> result 1.0\n",
            "[rank 3] no work left; exiting\n",
            "[rank 0] final counter=0 (should be 0)\n",
            "[rank 0] results filled: 8/8\n",
            "[rank 0] results: [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0]\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example is a little bit more complex, so let's walk through it. First\n",
        "\n",
        "```python\n",
        "ROOT    = 0\n",
        "N_TASKS = 8\n",
        "\n",
        "i32 = np.dtype('i')   # MPI.INT\n",
        "f64 = np.dtype('d')   # MPI.DOUBLE\n",
        "i32B = i32.itemsize\n",
        "f64B = f64.itemsize\n",
        "``` \n",
        "will initialize some constants. `ROOT` is defining the root node, which will essentially the the task coordinator, `N_TASKS` will be the number of tasks to acomplish, and data types will be used to help MPI figure out where to read and write in memory buffers. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the root, we create an array of integers called pool. The first element of pool is going to contain the number of tasks. Subsequent elements in pool will contain the workload (e.g. the numbers we want to square). We will also create a results buffer on the root where we'll store the results of each task (that is, the results of squaring each number). On all other ranks, the pool is defined `None` which is a useful way to tell MPI that these ranks should expose no memory. \n",
        "```python\n",
        "if rank == ROOT:\n",
        "    # pool[0] = counter (remaining tasks)\n",
        "    # pool[1:] = task payloads (just integers here)\n",
        "    pool = np.empty(N_TASKS + 1, dtype=i32)\n",
        "    pool[0] = N_TASKS\n",
        "    pool[1:] = np.arange(1, N_TASKS + 1, dtype=i32)\n",
        "    results = np.full(N_TASKS, np.nan, dtype=f64)\n",
        "else:\n",
        "    pool = None       # workers expose no memory for pool\n",
        "    results = None    # workers expose no memory for results\n",
        "```\n",
        " \n",
        "```python\n",
        "else:\n",
        "    pool = None       # expose zero bytes on workers\n",
        "    results = None    # expose zero bytes on workers\n",
        "```\n",
        "\n",
        "Note that altough some ranks may not expose any memory, all ranks need to call\n",
        "```python\n",
        "win_pool = MPI.Win.Create(pool,    comm=comm)\n",
        "win_res  = MPI.Win.Create(results, comm=comm)\n",
        "```\n",
        "to facilitate communication, since these are still considered collective communications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's break down the `claim_ticket` function. In this function, each process will rush to claim a ticket that can be redeemed for a unit of work. If the rank is successful, it will return the integer corresponding the claimed ticket.Otherwise, it will return -1, meaning there is no additional work to complete. \n",
        "\n",
        "The arguments to this function include a window `win`, which will be needed to do RMA operations on the root process. As the target, we will be passing in `ROOT`, since the number of tickets left is stored in root's window.  The dtype argument will be used to pass in the data type of the `pool` buffer.  \n",
        "\n",
        "```python\n",
        "def claim_ticket(win, target, dtype):\n",
        "    \"\"\"\n",
        "    Atomically decrement pool[0] at ROOT only if > 0.\n",
        "    Return ticket index in [0..N_TASKS-1], else -1.\n",
        "    \"\"\"\n",
        "    # Working buffers for the CAS operation\n",
        "    cur    = np.zeros(1, dtype=dtype)   # will hold the current counter value read from ROOT\n",
        "    expect = np.zeros(1, dtype=dtype)   # the value we expect to see (if counter hasn't changed)\n",
        "    new    = np.zeros(1, dtype=dtype)   # the value we want to write if compare succeeds (expect-1)\n",
        "    old    = np.zeros(1, dtype=dtype)   # CAS will return whatever was actually in the target\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The buffers `cur`, `expect`, `new`, and `old` will be used in the ticket claiming logic, which is contained in the `while loop`. At every iteration of this loop, the first step is to read the current counter on the root, and store the number of tickets remaining in `c`. We then check if this value is $\\leq$ 0, in which case we know that there are no more tasks to complete so we can return. \n",
        "\n",
        "```python\n",
        "# Step 1: read the current counter on ROOT into cur\n",
        "win.Lock(target, MPI.LOCK_SHARED)\n",
        "win.Get([cur, MPI.INT], target, 0)   # displacement=0 bytes -> pool[0]\n",
        "win.Flush(target)\n",
        "win.Unlock(target)\n",
        "\n",
        "c = int(cur[0])   # current number of remaining tasks\n",
        "    if c <= 0:\n",
        "        return -1     # no tasks left\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then need some logic to deal with the fact that multiple processes could potentially be working to claim the same ticket. For a given rank, once we've retrieved the number of tickets remaining, we can check in with the root and see if that value has changed. If it has, some other process has claimed that ticket. Otherwise, we need to decrement the number of tickets on the root and then actually do the work for that ticket. \n",
        "\n",
        "We'll accomplish this using some code that uses the `Compare_and_swap` operation (CAS). \n",
        "\n",
        "```python\n",
        "# Step 2: prepare expected and new values\n",
        "expect[0] = c     # we think the counter is still c\n",
        "new[0]    = c - 1 # we want to write back c-1 (decrement by one)\n",
        "\n",
        "# Step 3: attempt CAS at ROOT\n",
        "win.Lock(target, MPI.LOCK_EXCLUSIVE)\n",
        "win.Compare_and_swap([new, MPI.INT],     # proposed new value\n",
        "                        [expect, MPI.INT],  # expected current value\n",
        "                        [old, MPI.INT],     # gets the actual old value\n",
        "                        target, 0)\n",
        "win.Flush(target)\n",
        "win.Unlock(target)\n",
        "```\n",
        "\n",
        "Here, for a given rank, we expect that when we check back in with the root after reading the counter, that the value should still be the same. That is, we expect that the counter value in root is equal to the `c`. If so, then great! That means we can claim that ticket, decrement the ticket counter, and move on to doing some work. \n",
        "\n",
        " But what would happen if the counter value in the root changed between we intially read it and we checked in again with the root?  Well, in that case some other process claimed the ticket, and we missed our chance. In this case, we don't want to decrement the ticket counter on the root.\n",
        " \n",
        " This is the logic of using `Compare_and_swap` in the above code. If the value on the root is what we expect, we'll decrement the counter return the ticket number. Otherwise, we'll keep trying to claim a ticket unless there's no more work to be done. After the `Compare_and_swap`, we just need to verify what happened:\n",
        "\n",
        " ```python\n",
        " # Step 4: check whether CAS succeeded\n",
        "if int(old[0]) == c:\n",
        "    # success: we got the ticket corresponding to this claim\n",
        "    return c - 1\n",
        "```\n",
        "If our rank successfully claimed the ticket, then the value returned in the CAS operation will be equal to the old counter value. In this case, we don't return our ticket number directly, but rather the index into the pool array on the root, which we'll use to read the integer that we need to square."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we understand `claim_ticket`, let's take a look at the main work loop. If the `claim_ticket` function returns, then there are one of two possibilites. If it returns a value of -1, then there was not work so we can exit the loop.\n",
        "```python\n",
        "while True:\n",
        "    ticket = claim_ticket(win_pool, ROOT, i32)\n",
        "    if ticket < 0:\n",
        "        log(\"no work left; exiting\")\n",
        "        break\n",
        "```\n",
        "Otherwise, we will need to get the work unit associated with the ticket, which, in this case, is just an integer to square. The `Get` function needs the offset in bytes in the pool buffer on root to read the correct integer. \n",
        "\n",
        "```python\n",
        "# pool[1 + ticket] holds the payload for this task\n",
        "disp_bytes = (1 + ticket) * i32B\n",
        "task = np.zeros(1, dtype=i32)\n",
        "\n",
        "win_pool.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "win_pool.Get([task, MPI.INT], ROOT, disp_bytes)\n",
        "win_pool.Flush(ROOT)\n",
        "win_pool.Unlock(ROOT)\n",
        "\n",
        "payload = int(task[0])\n",
        "log(f\"claimed ticket={ticket}, task={payload}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have the payload, we can do the \"work\":\n",
        "```python\n",
        "# Do some work (here: just square it)\n",
        "result_val = float(payload * payload)\n",
        "```\n",
        "\n",
        "The result can be written into the results buffer on the root. Here we needed to account for the fact that the result buffer was of float type instead of integer type.\n",
        "```python\n",
        "# Write result back to ROOT's results[ticket]\n",
        "res_disp = ticket * f64B\n",
        "res_buf  = np.array([result_val], dtype=f64)\n",
        "\n",
        "win_res.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "win_res.Put([res_buf, MPI.DOUBLE], ROOT, res_disp)\n",
        "win_res.Flush(ROOT)\n",
        "win_res.Unlock(ROOT)\n",
        "\n",
        "log(f\"processed task {payload} -> result {result_val:.1f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the resulting squared integer is stored in its correct slot in the results array!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM6/s6728rBWcJXlWE3MlFg",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
