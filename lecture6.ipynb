{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/lecture6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slRqa4xRq8AM"
      },
      "source": [
        "# Advanced mpi4py Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-AXH3DarAOn"
      },
      "source": [
        "* In this lecture we'll cover a handful of advanced features in mpi4py\n",
        "* We'll discuss, persistent communication, one sided communication, and parallel I/O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PCVrBNdr8LD"
      },
      "source": [
        "### Persistent Communication\n",
        "* In some of our previous examples such as the parallel heat equation, we sent many of the same type of message repreatedly in a loop\n",
        "* In such cases, communication can be optimized by using persistent communication, a particular case of nonblocking communication that reduces the overhead of repreatedly setting up the same communication\n",
        "* For point-to-point communication, persistent communication is used by setting up requests with `Send_init` and `Recv_init`\n",
        "* In each loop iteration, you would then call `Start` or `Startall` and subsequently `Wait` or `Waitall`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeZI-879thSi"
      },
      "source": [
        "#### 1. Usage Pattern\n",
        "* Create a request one time\n",
        "```python\n",
        "req_s = comm.Send_init(buf, dest=..., tag=...)\n",
        "req_r = comm.Recv_init(buf, source=..., tag=...)\n",
        "```\n",
        "* In a loop you can repeat the message with the outlined form many times\n",
        "```python\n",
        "req_s.Start()\n",
        "req_r.Start()\n",
        "MPI.Request.Waitall([req_s, req_r])\n",
        "```\n",
        "After you're finished sending messages, clean up with\n",
        "```python\n",
        "req_s.Free()\n",
        "req_r.Free()\n",
        "```\n",
        "* It's a little funky to have to free something in a Python program, but a persistent communication creates a `request` object that holds onto\n",
        "  * A pointer to the buffer\n",
        "  * Datatype description\n",
        "  * The communicator and tag\n",
        "* Free will tell MPI you're done with these resources\n",
        "* This is another reminder of how MPI is a lower level library being wrapped in Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKeXxf5HvGKY"
      },
      "source": [
        "#### 2. Example: Sending Data in a Ring!\n",
        "* Below, let's look at an example where we have each rank send some information to its left, wrapping around to the last rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suj15gkRAZZT"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "size  = comm.Get_size()\n",
        "\n",
        "right = (rank + 1) % size\n",
        "left  = (rank - 1) % size\n",
        "\n",
        "sendbuf = np.array(rank, dtype='i')   # Will send our rank\n",
        "recvbuf = np.array(-1, dtype='i')     # Will receive from 'left'\n",
        "\n",
        "# Build persistent requests once\n",
        "send_req = comm.Send_init(sendbuf, dest=right, tag=0)\n",
        "recv_req = comm.Recv_init(recvbuf,  source=left,  tag=0)\n",
        "\n",
        "n_iters = 5\n",
        "for it in range(n_iters):\n",
        "    # Optionally update what we send each iter\n",
        "    sendbuf[...] = rank + 100*it\n",
        "\n",
        "    # Start both; then wait for both\n",
        "    send_req.Start()\n",
        "    recv_req.Start()\n",
        "    MPI.Request.Waitall([send_req, recv_req])\n",
        "\n",
        "    print(f\"[iter {it}] rank {rank} got {recvbuf} from {left}\")\n",
        "\n",
        "send_req.Free()\n",
        "recv_req.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fus_5a44XJ7S",
        "outputId": "6b228adc-0999-43ab-a231-0fc8cedcfea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[iter 0] rank 0 got 3 from 3\n",
            "[iter 1] rank 0 got 103 from 3\n",
            "[iter 2] rank 0 got 203 from 3\n",
            "[iter 3] rank 0 got 303 from 3\n",
            "[iter 4] rank 0 got 403 from 3\n",
            "[iter 0] rank 1 got 0 from 0\n",
            "[iter 1] rank 1 got 100 from 0\n",
            "[iter 2] rank 1 got 200 from 0\n",
            "[iter 3] rank 1 got 300 from 0\n",
            "[iter 4] rank 1 got 400 from 0\n",
            "[iter 0] rank 2 got 1 from 1\n",
            "[iter 1] rank 2 got 101 from 1\n",
            "[iter 2] rank 2 got 201 from 1\n",
            "[iter 3] rank 2 got 301 from 1\n",
            "[iter 4] rank 2 got 401 from 1\n",
            "[iter 0] rank 3 got 2 from 2\n",
            "[iter 1] rank 3 got 102 from 2\n",
            "[iter 2] rank 3 got 202 from 2\n",
            "[iter 3] rank 3 got 302 from 2\n",
            "[iter 4] rank 3 got 402 from 2\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U0xCNSSXJ7S"
      },
      "source": [
        "* What fun, we sent some data in a ring!\n",
        "* There is potentially a slight performance benefit from persistent communication and it can help clean up code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwGstNTCXJ7T"
      },
      "source": [
        "# One-Sided Communication\n",
        "* We have stated that MPI doesn't use a shared memory paradigm, hence the necessity of sending messages\n",
        "* However, MPI supports a on-sided communication model using **Remote Memmory Access** (RMA), which behaves similarly to a shared memory model\n",
        "    * In RMA one process (the origin) directly reads from or writes into memory exposed by another procces (the target)\n",
        "    * The target process doesn't need to call a send / receive on the other end\n",
        "* In mpi4py, one-sided operations are available using windows via the `Win`\n",
        "* The main operations for windows are:\n",
        "    * `Put`: write data into a target's windows\n",
        "    * `Get` : Read data from a target's window\n",
        "    * `Accumulate` : atomic fetch and combine into target (sum, max, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOkUwbScZCgn"
      },
      "source": [
        "### Why is it Useful\n",
        "* Why would we use one-sided communication as opposed to normal two way communication?\n",
        "\n",
        "* Decoupling synchronization\n",
        "\n",
        "  * In two-way communication, both sender and receiver must call matching operations (e.g., `Send`, `Recv`).\n",
        "  * With one-sided, only the origin process issues the operation; the target doesn't need to explicitly participate\n",
        "  * This leads to simpler control flow for some applications\n",
        "  * Great when receivers don't know in advance who will send them data.\n",
        "* Potential for lower latency and higher throughput\n",
        "  * RMA can take advantage of specialized network hardware for direct memory access (RDMA) when communicating across nodes\n",
        "  * Avoids extra protocol overhead from matching send/recv calls.\n",
        "\n",
        "* With this said two-sided communication is often simpler and preferred over one-sided communication as it works well for many problems and is easier to debug\n",
        "* Moreover two-sided communication supports messages with Python objects, whereas one-sided communication is limited to NumPy arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnatWEN7XJ7T"
      },
      "source": [
        "## 1. Synchronization\n",
        "* You'll probably notice that this paradigm is much like a shared memory model for a multithreaded application\n",
        "* This comes with many of the same challenges as threading including race conditions. To contend with this we need some way of handling synchronization\n",
        "* RMA has a couple primary synchronization mechanisms:\n",
        "    * **Fence**: collective barrier-like synchronization\n",
        "    * **Lock / Unlock**: finer control for accessing one target at a time\n",
        "* RMA is not a true shared memory model, but it behaves much like one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6mD9f3SXJ7T"
      },
      "source": [
        "## 2. Example\n",
        "\n",
        "* This first example is very basic\n",
        "* Each process exposes a single integer which can be read or written to\n",
        "* Rank 0 writes into rank 1's window\n",
        "* `Fence` is used for synchronization much like `comm.Barrier()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LliJi7OFXJ7T"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "buf = np.array(rank, dtype='i')\n",
        "win = MPI.Win.Create(buf, comm=comm)\n",
        "\n",
        "if rank == 0:\n",
        "    data = np.array(42, dtype='i')\n",
        "    win.Fence()\n",
        "    win.Put([data, MPI.INT], 1)\n",
        "    win.Fence()\n",
        "elif rank == 1:\n",
        "    win.Fence()\n",
        "    win.Fence()\n",
        "    print(f\"Rank 1 sees {buf.item()}\")\n",
        "\n",
        "win.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O-ubtAqXJ7T",
        "outputId": "3903ee52-3b94-4411-9170-989091259228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1 sees 42\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 2 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl-xT2LnXJ7T"
      },
      "source": [
        "## 3. Synchronization in RMA\n",
        "\n",
        "Synchronization in RMA is a bit complex and requires knowing some terminology.\n",
        "\n",
        "1. **Origin and Target**\n",
        "\n",
        "* When we use the term **origin** or origin process, we mean the rank that initiates the RMA operation\n",
        "* This process calls functions like `Put`, `Get`, and `Accumulate`\n",
        "* In contrast, the **target** or target process is the rank that exposes some memory in a window\n",
        "* Its buffer, created with `Win.Create`, will be read or written to\n",
        "\n",
        "\n",
        "2. **Epochs**\n",
        "* Remote Memory Access (RMA) needs well-defined periods when a window is open for remote operations called epochs\n",
        "* **Access epoch**: from the origin process's perspective (issuing Put, Get, etc.).\n",
        "* **Exposure epoch**: from the target process's perspective (memory is exposed to remote access).\n",
        "* An epoch is always initiated (opened) and terminated (closed) by synchronization calls.\n",
        "\n",
        "3. **Active Target Synchronization**\n",
        "* Both the origin and target participate in starting / ending the epoch\n",
        "* The most typical way of using active target synchronization is with `Fence`\n",
        "* The first call to `win.Fence()` collectively opens an epoch for all ranks\n",
        "* The second call to `win.Fence()` collectively closes it\n",
        "* When fence is used, while the epoch is open, any rank can fetch or read data from windows in any other rank\n",
        "\n",
        "\n",
        "4. **Pasive Target Synchronization**\n",
        "* In passive target synchronization, only the origin participates while the target is unaware\n",
        "* The epoch is initiated with `win.Lock(target, lock_type)` and closed with `win.Unlock(target)`\n",
        "* When the epoch is open you can do `Put`, `Get`, and `Accumulate` on the target rank's exposed memory\n",
        "* There are two types of locks you can use:\n",
        "    * In `MPI.SHARED_LOCK`, multiple origins can hold a shared lock on the same target window\n",
        "    * This is best used in cases where you're doing read-only access (`Get`) or you're doing writes to disjoint regions that don't conflict with each other\n",
        "    * The alternative is `MPI.LOCK_EXCLUSIVE`, in which only one origin can hold an exclusive lock on a target at a time\n",
        "    * MPI ensures no other orgin can open a lock on the target until it's released\n",
        "    * This is best used in cases when you want to update memory where conflicts are possible\n",
        "* Locks aren't mutexes, so they do not prevent the target process from accessing its own window memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nk7mVFSXJ7U"
      },
      "source": [
        "### Example of Active Target synchronization\n",
        "\n",
        "* We already saw an example using active target synchronization with `win.Fence()`\n",
        "* Let's look at an example of passive synchronization using `Lock` and `Unlock`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G21lU4HlXJ7U"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "buf = np.array(rank, dtype='i')\n",
        "win = MPI.Win.Create(buf, comm=comm)\n",
        "\n",
        "# Passive target synchronization\n",
        "if rank == 0:\n",
        "    val = np.array(123, dtype='i')\n",
        "    win.Lock(1, MPI.LOCK_SHARED)        # open epoch with rank 1\n",
        "    win.Put([val, MPI.INT], 1)   # origin=0, target=1\n",
        "    win.Flush(1)                        # ensure target sees it\n",
        "    win.Unlock(1)                       # close epoch\n",
        "\n",
        "comm.Barrier()  # just to order printing\n",
        "print(f\"[Lock/Unlock] rank {rank} buf = {buf.item()}\")\n",
        "win.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmQzyIo8XJ7U",
        "outputId": "ce660c01-a7ee-4f1a-bfe0-12dab5b50258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Lock/Unlock] rank 0 buf = 0\n",
            "[Lock/Unlock] rank 1 buf = 123\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 2 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ANL5ULlXJ7U"
      },
      "source": [
        "* In this example, rank 0 is basically using `Lock` to open a channel to communicate with rank 1, then closing the channel with `Unlock`\n",
        "* After the `Flush` operation, we know that the value has been written into rank 1's buffer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2aot4syXJ7U"
      },
      "source": [
        "### Atomic Operations\n",
        "1. In addition to `Get` and `Put` mpi4py has several atomic operations\n",
        "* We have already mentioned one atomic operation for RMA called `Accumulate`\n",
        "* An atomic operation is one that appears to happen all at once without interference from other processes\n",
        "* In MPI RMA, atomic operations are guaranteed to be indivisble\n",
        "* This can be used to avoid race conditions where multiple origins try to update the same target memory at the same time\n",
        "\n",
        "2. What are they used for?\n",
        "* In one-sided communication, multiple ranks may read or write the same window at once.\n",
        "* Without atomics, you'd risk lost updates (e.g., two processes increment a counter and one increment gets overwritten).\n",
        "* Atomics can be used to ensure correctness for:\n",
        "    * Distributed counters\n",
        "    * Locks / flags\n",
        "    * Queues or ticket dispensers\n",
        "    * Reductions (sum, max, etc.)\n",
        "\n",
        "3. Examples:\n",
        "\n",
        "**Accumulate** applies an operation at the target like sum, max, replace, etc.\n",
        "\n",
        "```python\n",
        "# Increment a shared counter at root\n",
        "one = np.array(1, dtype='i')\n",
        "win.Accumulate([one, MPI.INT], ROOT, op=MPI.SUM)\n",
        "```\n",
        "\n",
        " **Get_accumulate** fetches the old value and applies an operation with the origin's value.\n",
        " ```python\n",
        " # Increments a shared counter at root and gets the old counter value to store in old\n",
        "oldval = np.zeros(1, dtype='i')\n",
        "one = np.array(1, dtype='i')\n",
        "win.Get_accumulate([one, MPI.INT], [oldval, MPI.INT],\n",
        "                   ROOT, op=MPI.SUM)\n",
        " ```\n",
        "\n",
        " **Compare_and_swap** is an operation that will replace the value in the target with a new value if it is equal to the current value.\n",
        " ```python\n",
        " # Replaces the value in the root with newval if it is equal to expected value. The value in the root's buffer is returned\n",
        " # regardless of whether the comparison was true or not\n",
        "expected = np.array(5, dtype='i')\n",
        "newval   = np.array(4, dtype='i')\n",
        "oldval   = np.zeros(1, dtype='i')\n",
        "\n",
        "win.Compare_and_swap([newval, MPI.INT],\n",
        "                     [expected, MPI.INT],\n",
        "                     [oldval, MPI.INT],\n",
        "                     ROOT, 0)\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvnIg0tGXJ7U"
      },
      "source": [
        "### Example: Work Stealing Queue\n",
        "\n",
        "* We've seen the use of `ProcessPoolExecutor` in Python where you can create a queue of tasks to be accomplished which will be farmed out to different process\n",
        "* Let's consider how this kind of process pool pattern could be implemented in `mpi4py`\n",
        "* To coordinate differnt processes, the main idea here is that there will be a set of tickets (represented by integers) corresponding to the number of tasks to be accomplished\n",
        "* Processes will independently try to claim a ticket, so they have the privelege of doing a task\n",
        "* In this case, the task is really trivial, the process will just retrieve an integer from the root, square it, and write it back to the root\n",
        "* Squaring some numbers is a silly task to acommplish, but it illustrates a common scheduling pattern in parallel computing\n",
        "* The tasks could easily be replaced with something computationally expensive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBrMZbqNXJ7U"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "\n",
        "ROOT    = 0\n",
        "N_TASKS = 8\n",
        "\n",
        "i32 = np.dtype('i')   # MPI.INT\n",
        "f64 = np.dtype('d')   # MPI.DOUBLE\n",
        "i32B = i32.itemsize\n",
        "f64B = f64.itemsize\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[rank {rank}] {msg}\", flush=True)\n",
        "\n",
        "# -------------------------\n",
        "# Windows & initialization\n",
        "# -------------------------\n",
        "if rank == ROOT:\n",
        "    # pool[0] = counter (remaining tasks)\n",
        "    # pool[1:] = task payloads (just integers here)\n",
        "    pool = np.empty(N_TASKS + 1, dtype=i32)\n",
        "    pool[0] = N_TASKS\n",
        "    pool[1:] = np.arange(1, N_TASKS + 1, dtype=i32)\n",
        "    results = np.full(N_TASKS, np.nan, dtype=f64)\n",
        "else:\n",
        "    pool = None       # workers expose no memory for pool\n",
        "    results = None    # workers expose no memory for results\n",
        "\n",
        "win_pool = MPI.Win.Create(pool,    comm=comm)\n",
        "win_res  = MPI.Win.Create(results, comm=comm)\n",
        "\n",
        "if rank == ROOT:\n",
        "    log(f\"init: counter={pool[0]}, tasks={pool[1:].tolist()}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# Atomic claim (CAS) to avoid overshoot\n",
        "# -----------------------------------------\n",
        "def claim_ticket(win, target, dtype):\n",
        "    \"\"\"\n",
        "    Atomically decrement pool[0] at ROOT only if > 0.\n",
        "    Return ticket index in [0..N_TASKS-1], else -1.\n",
        "    \"\"\"\n",
        "    # Working buffers for the CAS operation\n",
        "    cur    = np.zeros(1, dtype=dtype)   # will hold the current counter value read from ROOT\n",
        "    expect = np.zeros(1, dtype=dtype)   # the value we expect to see (if counter hasn't changed)\n",
        "    new    = np.zeros(1, dtype=dtype)   # the value we want to write if compare succeeds (expect-1)\n",
        "    old    = np.zeros(1, dtype=dtype)   # CAS will return whatever was actually in the target\n",
        "\n",
        "    while True:\n",
        "        # Step 1: read the current counter on ROOT into cur\n",
        "        win.Lock(target, MPI.LOCK_SHARED)\n",
        "        win.Get([cur, MPI.INT], target, 0)   # displacement=0 bytes -> pool[0]\n",
        "        win.Flush(target)\n",
        "        win.Unlock(target)\n",
        "\n",
        "        c = int(cur[0])   # current number of remaining tasks\n",
        "        if c <= 0:\n",
        "            return -1     # no tasks left\n",
        "\n",
        "        # Step 2: prepare expected and new values\n",
        "        expect[0] = c     # we think the counter is still c\n",
        "        new[0]    = c - 1 # we want to write back c-1 (decrement by one)\n",
        "\n",
        "        # Step 3: attempt CAS at ROOT\n",
        "        win.Lock(target, MPI.LOCK_EXCLUSIVE)\n",
        "        win.Compare_and_swap([new, MPI.INT],     # proposed new value\n",
        "                             [expect, MPI.INT],  # expected current value\n",
        "                             [old, MPI.INT],     # gets the actual old value\n",
        "                             target, 0)\n",
        "        win.Flush(target)\n",
        "        win.Unlock(target)\n",
        "\n",
        "        # Step 4: check whether CAS succeeded\n",
        "        if int(old[0]) == c:\n",
        "            # success: we got the ticket corresponding to this claim\n",
        "            return c - 1\n",
        "        # else: someone else beat us to it, retry loop\n",
        "\n",
        "# -----------------------------------------\n",
        "# Workers do the work\n",
        "# -----------------------------------------\n",
        "if rank != ROOT:\n",
        "    while True:\n",
        "        ticket = claim_ticket(win_pool, ROOT, i32)\n",
        "        if ticket < 0:\n",
        "            log(\"no work left; exiting\")\n",
        "            break\n",
        "\n",
        "        # pool[1 + ticket] holds the payload for this task\n",
        "        disp_bytes = (1 + ticket) * i32B\n",
        "        task = np.zeros(1, dtype=i32)\n",
        "\n",
        "        win_pool.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "        win_pool.Get([task, MPI.INT], ROOT, disp_bytes)\n",
        "        win_pool.Flush(ROOT)\n",
        "        win_pool.Unlock(ROOT)\n",
        "\n",
        "        payload = int(task[0])\n",
        "        log(f\"claimed ticket={ticket}, task={payload}\")\n",
        "\n",
        "        # Do some work (here: just square it)\n",
        "        result_val = float(payload * payload)\n",
        "\n",
        "        # Write result back to ROOT's results[ticket]\n",
        "        res_disp = ticket * f64B\n",
        "        res_buf  = np.array([result_val], dtype=f64)\n",
        "\n",
        "        win_res.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "        win_res.Put([res_buf, MPI.DOUBLE], ROOT, res_disp)\n",
        "        win_res.Flush(ROOT)\n",
        "        win_res.Unlock(ROOT)\n",
        "\n",
        "        log(f\"processed task {payload} -> result {result_val:.1f}\")\n",
        "\n",
        "comm.Barrier()\n",
        "\n",
        "if rank == ROOT:\n",
        "    filled = int(np.count_nonzero(~np.isnan(results)))\n",
        "    log(f\"final counter={pool[0]} (should be 0)\")\n",
        "    log(f\"results filled: {filled}/{N_TASKS}\")\n",
        "    log(f\"results: {results.tolist()}\")\n",
        "\n",
        "win_pool.Free()\n",
        "win_res.Free()\n",
        "\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9phIVD8XJ7U",
        "outputId": "c115115a-f85b-473a-910d-302e2db824be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[rank 0] init: counter=8, tasks=[1, 2, 3, 4, 5, 6, 7, 8]\n",
            "[rank 1] claimed ticket=6, task=7\n",
            "[rank 1] processed task 7 -> result 49.0\n",
            "[rank 1] claimed ticket=4, task=5\n",
            "[rank 1] processed task 5 -> result 25.0\n",
            "[rank 2] claimed ticket=5, task=6\n",
            "[rank 2] processed task 6 -> result 36.0\n",
            "[rank 3] claimed ticket=7, task=8\n",
            "[rank 3] processed task 8 -> result 64.0\n",
            "[rank 3] claimed ticket=3, task=4\n",
            "[rank 3] processed task 4 -> result 16.0\n",
            "[rank 2] claimed ticket=2, task=3\n",
            "[rank 1] claimed ticket=1, task=2\n",
            "[rank 2] processed task 3 -> result 9.0\n",
            "[rank 1] processed task 2 -> result 4.0\n",
            "[rank 2] no work left; exiting\n",
            "[rank 3] claimed ticket=0, task=1\n",
            "[rank 1] no work left; exiting\n",
            "[rank 3] processed task 1 -> result 1.0\n",
            "[rank 3] no work left; exiting\n",
            "[rank 0] final counter=0 (should be 0)\n",
            "[rank 0] results filled: 8/8\n",
            "[rank 0] results: [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0]\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT2qXUgJXJ7U"
      },
      "source": [
        "This example is a little bit more complex, so let's walk through it. First, this will initialize some constants.\n",
        "\n",
        "```python\n",
        "ROOT    = 0\n",
        "N_TASKS = 8\n",
        "\n",
        "i32 = np.dtype('i')   # MPI.INT\n",
        "f64 = np.dtype('d')   # MPI.DOUBLE\n",
        "i32B = i32.itemsize\n",
        "f64B = f64.itemsize\n",
        "```\n",
        " Here, `ROOT` is defining the root node, which will essentially serve as the ticket master and data keeper. `N_TASKS` is be the number of tasks to accomplish. Getting the sizes of integer and floating point data types will be used to help MPI figure out where to read and write in memory buffers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCElMBIGXJ7U"
      },
      "source": [
        "On the root, we create an array of integers called pool. The first element of pool is going to contain the number of tasks. Subsequent elements in pool will contain the workload (e.g. the numbers we want to square). We will also create a results buffer on the root where we'll store the results of each task (that is, the results of squaring each number). On all other ranks, the pool is defined `None` which is a useful way to tell MPI that these ranks should expose no memory.\n",
        "```python\n",
        "if rank == ROOT:\n",
        "    # pool[0] = counter (remaining tasks)\n",
        "    # pool[1:] = task payloads (just integers here)\n",
        "    pool = np.empty(N_TASKS + 1, dtype=i32)\n",
        "    pool[0] = N_TASKS\n",
        "    pool[1:] = np.arange(1, N_TASKS + 1, dtype=i32)\n",
        "    results = np.full(N_TASKS, np.nan, dtype=f64)\n",
        "else:\n",
        "    pool = None       # workers expose no memory for pool\n",
        "    results = None    # workers expose no memory for results\n",
        "```\n",
        "\n",
        "```python\n",
        "else:\n",
        "    pool = None       # expose zero bytes on workers\n",
        "    results = None    # expose zero bytes on workers\n",
        "```\n",
        "\n",
        "Note that altough some ranks may not expose any memory, all ranks need to call\n",
        "```python\n",
        "win_pool = MPI.Win.Create(pool,    comm=comm)\n",
        "win_res  = MPI.Win.Create(results, comm=comm)\n",
        "```\n",
        "to facilitate communication, since these are still considered collective communications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG6Y9YI6XJ7V"
      },
      "source": [
        "Next, let's break down the `claim_ticket` function. In this function, each process will rush to claim a ticket that can be redeemed for a unit of work. If the rank is successful, it will return the integer corresponding the claimed ticket (or actually the index of the integer to square, but essentially this corresponds to its ticket number). Otherwise, it will return -1, meaning there is no additional work to complete.\n",
        "\n",
        "The arguments to this function include a window `win`, which will be needed to communicate with the root process. As the target, we will be passing in `ROOT`, since each process will need to get information from the root. The dtype argument will be used to pass in the data type of the `pool` buffer so we can index it properly.  \n",
        "\n",
        "```python\n",
        "def claim_ticket(win, target, dtype):\n",
        "    \"\"\"\n",
        "    Atomically decrement pool[0] at ROOT only if > 0.\n",
        "    Return ticket index in [0..N_TASKS-1], else -1.\n",
        "    \"\"\"\n",
        "    # Working buffers for the CAS operation\n",
        "    cur    = np.zeros(1, dtype=dtype)   # will hold the current counter value read from ROOT\n",
        "    expect = np.zeros(1, dtype=dtype)   # the value we expect to see (if counter hasn't changed)\n",
        "    new    = np.zeros(1, dtype=dtype)   # the value we want to write if compare succeeds (expect-1)\n",
        "    old    = np.zeros(1, dtype=dtype)   # CAS will return whatever was actually in the target\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSKQdOdlXJ7V"
      },
      "source": [
        "The buffers `cur`, `expect`, `new`, and `old` will be used in the ticket claiming logic, which is contained in the while loop below. At every iteration of this loop, the first step is to read the current value of the ticket counter on the root, and store this value in `c`. We then check if this value is $\\leq$ 0, in which case we know that there are no more tasks to complete so we can return.\n",
        "\n",
        "```python\n",
        "# Step 1: read the current counter on ROOT into cur\n",
        "win.Lock(target, MPI.LOCK_SHARED)\n",
        "win.Get([cur, MPI.INT], target, 0)   # displacement=0 bytes -> pool[0]\n",
        "win.Flush(target)\n",
        "win.Unlock(target)\n",
        "\n",
        "c = int(cur[0])   # current number of remaining tasks\n",
        "    if c <= 0:\n",
        "        return -1     # no tasks left\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH2jBERAXJ7V"
      },
      "source": [
        "We then need some logic to deal with the fact that multiple processes could potentially be vying to claim the same ticket. For a given rank, once we've retrieved the number of tickets remaining from root, we can check in with the root again to see if the ticket counter value has changed. If it has, some other process has claimed that ticket before us. Otherwise, we need to decrement the number of tickets on the root, which will serve to claim the ticket.\n",
        "\n",
        "We'll accomplish this using some code that uses the `Compare_and_swap` operation (CAS).\n",
        "\n",
        "```python\n",
        "# Step 2: prepare expected and new values\n",
        "expect[0] = c     # we think the counter is still c\n",
        "new[0]    = c - 1 # we want to write back c-1 (decrement by one)\n",
        "\n",
        "# Step 3: attempt CAS at ROOT\n",
        "win.Lock(target, MPI.LOCK_EXCLUSIVE)\n",
        "win.Compare_and_swap([new, MPI.INT],     # proposed new value\n",
        "                        [expect, MPI.INT],  # expected current value\n",
        "                        [old, MPI.INT],     # gets the actual old value\n",
        "                        target, 0)\n",
        "win.Flush(target)\n",
        "win.Unlock(target)\n",
        "```\n",
        "\n",
        "For a given rank, we expect that when we check back in with the root after reading the counter, that the value should still be the same. That is, we expect that the counter value in root is equal to `c`. If so, then great! That means we can claim that ticket decrement the ticket counter, and move on to doing some work. Checking if the value is the same and decrementing the counter can be done in  a single atomic operation.\n",
        "\n",
        " But what would happen if the counter value in the root changed between when we intially read it and we checked in again with the root?  Well, in that case some other process claimed the ticket, and we missed our chance to claim it. In this case, we don't want to decrement the ticket counter on the root.\n",
        "\n",
        " This is the logic of using `Compare_and_swap` in the above code. If the value on the root is what we expect, we'll decrement the counter return the ticket number. Otherwise, we'll see that the value changed and keep trying to claim a ticket unless there's no more work to be done. After the `Compare_and_swap`, we just need to verify if the value on the root changed and act accordingly.\n",
        "\n",
        " ```python\n",
        " # Step 4: check whether CAS succeeded\n",
        "if int(old[0]) == c:\n",
        "    # success: we got the ticket corresponding to this claim\n",
        "    return c - 1\n",
        "```\n",
        "If our rank successfully claimed the ticket, then the value returned in the CAS operation will be equal to the old counter value. In this case, we don't return our ticket number directly, but rather the index into the pool array on the root, which we'll use to read the integer that we need to square."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V146WW1XJ7V"
      },
      "source": [
        "Now that we understand `claim_ticket`, let's take a look at the main work loop. If the `claim_ticket` function returns, then there are one of two possibilites. If it returns a value of -1, then there was not work so we can exit the loop.\n",
        "```python\n",
        "while True:\n",
        "    ticket = claim_ticket(win_pool, ROOT, i32)\n",
        "    if ticket < 0:\n",
        "        log(\"no work left; exiting\")\n",
        "        break\n",
        "```\n",
        "Otherwise, we will need to get the work unit associated with the ticket, which, in this case, is just an integer to square. The `Get` function needs the offset in bytes in the pool buffer on root to read the correct integer.\n",
        "\n",
        "```python\n",
        "# pool[1 + ticket] holds the payload for this task\n",
        "disp_bytes = (1 + ticket) * i32B\n",
        "task = np.zeros(1, dtype=i32)\n",
        "\n",
        "win_pool.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "win_pool.Get([task, MPI.INT], ROOT, disp_bytes)\n",
        "win_pool.Flush(ROOT)\n",
        "win_pool.Unlock(ROOT)\n",
        "\n",
        "payload = int(task[0])\n",
        "log(f\"claimed ticket={ticket}, task={payload}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaKJsvjzXJ7V"
      },
      "source": [
        "Once we have the payload, we can do the \"work\":\n",
        "```python\n",
        "# Do some work (here: just square it)\n",
        "result_val = float(payload * payload)\n",
        "```\n",
        "\n",
        "The result can be written into the results buffer on the root. Here we needed to account for the fact that the result buffer was of float type instead of integer type.\n",
        "```python\n",
        "# Write result back to ROOT's results[ticket]\n",
        "res_disp = ticket * f64B\n",
        "res_buf  = np.array([result_val], dtype=f64)\n",
        "\n",
        "win_res.Lock(ROOT, MPI.LOCK_SHARED)\n",
        "win_res.Put([res_buf, MPI.DOUBLE], ROOT, res_disp)\n",
        "win_res.Flush(ROOT)\n",
        "win_res.Unlock(ROOT)\n",
        "\n",
        "log(f\"processed task {payload} -> result {result_val:.1f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWfVyVdMXJ7V"
      },
      "source": [
        "Now the resulting squared integer is stored in its correct slot in the results array!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yqsTZ8P0-WV"
      },
      "source": [
        "# Parallel I/O\n",
        "* To wrap up our discussion about mpi4py, we'll briefly discuss parallel I/O\n",
        "* mpi4py provides an interface for parallel access to files\n",
        "* Instead of each rank opening its own file, or opening a single file and scattering the data across ranks, all ranks can cooperatively open a single file handle\n",
        "* Benefits:\n",
        "    * Results can be written in parallel without corrupting data\n",
        "    * Big datasets can be read in parallel without creating a bottelneck on one process\n",
        "* mpi4py supports raw binary files for parallel I/O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Basic Functionality\n",
        "* The `MPI.File` object represents a file opened in parallel by miltiple MPI ranks\n",
        "* To open a file collectively, you can use syntax\n",
        "`MPI.File.Open(comm, \"filename.bin\", accessmode)`\n",
        "* The `MPI.FILE` object provideds methods for collective and individual I/O operations \n",
        "* Collective operations include `File.Read_all`, `File.Write_all`, `File.Read_at_all`, and `File.Write_at_all`\n",
        "* These methods can significantly improve performance by allowing the MPI library to optimize data transfer and access patterns.\n",
        "* Alternatively, one can use individual I/O operations using `File.Read`, `File.Write`, `File.Read_at`, `File.Write_at` \n",
        "* As an analogy, collective I/O operations are like collective communication\n",
        "    * Every rank calls the function and participates in the I/O operation\n",
        "    * This is often preferred as it leads to better performance\n",
        "* In individual I/O operations, only one rank participates, and other ranks are oblivious \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example\n",
        "* Let's see a basic example in action\n",
        "* Here every rank is going to have its own buffer\n",
        "* It will write the buffer to the parallel file beginning at a given byte offset\n",
        "* This avoids conflicts among the different ranks\n",
        "* To read data in parallel, each rank will start reading at a particlar byte offset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Each rank prepares some data\n",
        "data = np.arange(5, dtype='i') + 100*rank\n",
        "\n",
        "# Open file for parallel write\n",
        "fh = MPI.File.Open(comm, \"datafile.bin\",\n",
        "                   MPI.MODE_CREATE | MPI.MODE_WRONLY)\n",
        "\n",
        "# Each rank writes at its own offset (rank * bytes_per_rank)\n",
        "offset = rank * data.nbytes\n",
        "fh.Write_at(offset, data)\n",
        "fh.Close()\n",
        "\n",
        "# Reopen and read back\n",
        "buf = np.zeros_like(data)\n",
        "fh = MPI.File.Open(comm, \"datafile.bin\", MPI.MODE_RDONLY)\n",
        "fh.Read_at(offset, buf)\n",
        "fh.Close()\n",
        "\n",
        "print(f\"Rank {rank} read {buf}\")\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 0 read [0 1 2 3 4]\n",
            "Rank 1 read [100 101 102 103 104]\n",
            "Rank 2 read [200 201 202 203 204]\n",
            "Rank 3 read [300 301 302 303 304]\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Saving a 2D Array + Header\n",
        "* Okay, to see where file view come into play let's look at a more complex example\n",
        "* In this example, each rank is going to have its own 2d array, which represent some rows in a global 2d array \n",
        "* In this case the rank's array is just going to contain a bunch of floats set to its rank valeu\n",
        "* To represent this in a binary file, we are going to create a parallel file handle\n",
        "* We'll first write a global header to the file that contains the global array size `(nx_global,ny_global)`. \n",
        "* After writing the global header, each rank will write its own chunk of the 2d array to the file using file views "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "\n",
        "# mpiexec -n 4 python mpiio_rows_fixed_write.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "size  = comm.Get_size()\n",
        "\n",
        "# Each rank is going to have some given number of rows in the final 2d array\n",
        "num_rows  = 3                 # rows per rank \n",
        "nx_global = 10                # columns\n",
        "ny_global = size * num_rows   # total rows (exact multiple)\n",
        "\n",
        "dtype = np.float64\n",
        "etype = MPI.DOUBLE\n",
        "HEADER_BYTES = 16             # two int64: ny_global, nx_global\n",
        "FNAME = \"rows_fixed.bin\"\n",
        "\n",
        "# Local block coordinates in the global array\n",
        "i0 = rank * num_rows          # starting row for this rank\n",
        "ny_local = num_rows           # Number of rows owned by this rank\n",
        "nx_local = nx_global          # Nubmer of elements per column\n",
        "\n",
        "# Local data: every element equals 'rank' as a float\n",
        "arr_local = np.full((ny_local, nx_local), float(rank), dtype=dtype)\n",
        "\n",
        "# Helper: make a 2D subarray type describing this rank’s block in the global array\n",
        "# This will be used for the file view!\n",
        "def make_filetype_2d(ny, nx, i0, nloc, j0, mloc, etype):\n",
        "    sizes    = (ny, nx)          # full array shape\n",
        "    subsizes = (nloc, mloc)      # this rank's block shape\n",
        "    starts   = (i0, j0)          # top-left of this block in the global array\n",
        "    ft = etype.Create_subarray(sizes, subsizes, starts, order=MPI.ORDER_C)\n",
        "    ft.Commit()\n",
        "    return ft\n",
        "\n",
        "# Write the header\n",
        "fh = MPI.File.Open(comm, FNAME, MPI.MODE_CREATE | MPI.MODE_WRONLY)\n",
        "\n",
        "if rank == 0:\n",
        "    hdr = np.array([ny_global, nx_global], dtype=np.int64)\n",
        "    fh.Write_at(0, hdr)          # rank-0-only write\n",
        "\n",
        "# After the 16-byte header, the file is a ny_global×nx_global double array\n",
        "disp = HEADER_BYTES\n",
        "filetype = make_filetype_2d(ny_global, nx_global, i0, ny_local, 0, nx_local, etype)\n",
        "fh.Set_view(disp, etype, filetype, datarep=\"native\")\n",
        "\n",
        "# Collective write of our contiguous local block\n",
        "fh.Write_all(arr_local)\n",
        "\n",
        "fh.Close()\n",
        "filetype.Free()\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"[write] Wrote {ny_global}x{nx_global} (rows_per_rank={num_rows}) to {FNAME}\")\n",
        "    \n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[write] Wrote 12x10 (rows_per_rank=3) to rows_fixed.bin\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Now we've written the file in parallel, so let's look at the code to read it back in in parallel \n",
        "* This is going to look very similar using file views "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "size  = comm.Get_size()\n",
        "\n",
        "dtype = np.float64\n",
        "etype = MPI.DOUBLE\n",
        "HEADER_BYTES = 16\n",
        "FNAME = \"rows_fixed.bin\"\n",
        "\n",
        "# Same filetype helper function as before\n",
        "def make_filetype_2d(ny, nx, i0, nloc, j0, mloc, etype):\n",
        "    sizes    = (ny, nx)\n",
        "    subsizes = (nloc, mloc)\n",
        "    starts   = (i0, j0)\n",
        "    ft = etype.Create_subarray(sizes, subsizes, starts, order=MPI.ORDER_C)\n",
        "    ft.Commit()\n",
        "    return ft\n",
        "\n",
        "# Open the file and read the file in rank 0\n",
        "fh = MPI.File.Open(comm, FNAME, MPI.MODE_RDONLY)\n",
        "\n",
        "# Broadcast the header to all ranks\n",
        "hdr = np.zeros(2, dtype=np.int64)\n",
        "if rank == 0:\n",
        "    fh.Read_at(0, hdr)\n",
        "comm.Bcast(hdr, root=0)\n",
        "\n",
        "ny_global, nx_global = map(int, hdr)\n",
        "\n",
        "# In this simplified layout: each rank has ny_global/size rows\n",
        "assert ny_global % size == 0, \"File assumes equal rows per rank.\"\n",
        "num_rows = ny_global // size\n",
        "\n",
        "# Local block coordinates and shape\n",
        "i0 = rank * num_rows\n",
        "ny_local = num_rows\n",
        "nx_local = nx_global\n",
        "\n",
        "# Set the same file view and read our block collectively\n",
        "disp = HEADER_BYTES\n",
        "filetype = make_filetype_2d(ny_global, nx_global, i0, ny_local, 0, nx_local, etype)\n",
        "fh.Set_view(disp, etype, filetype, datarep=\"native\")\n",
        "\n",
        "buf = np.empty((ny_local, nx_local), dtype=dtype)\n",
        "fh.Read_all(buf)\n",
        "\n",
        "fh.Close()\n",
        "filetype.Free()\n",
        "\n",
        "# Quick sanity check: data should equal 'rank' everywhere in this block\n",
        "ok = np.allclose(buf, float(rank))\n",
        "print(f\"[read] rank {rank}: block {buf.shape}, i0={i0}, ok={ok}, first row={buf[0]}\")\n",
        "\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[read] rank 0: block (3, 10), i0=0, ok=True, first row=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[read] rank 3: block (3, 10), i0=9, ok=True, first row=[3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n",
            "[read] rank 1: block (3, 10), i0=3, ok=True, first row=[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[read] rank 2: block (3, 10), i0=6, ok=True, first row=[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Support for HDF5 and Other File Types\n",
        "* Numerous libraries wrap the basic file I/O functionality in MPI so you can use it for other file types\n",
        "* For example, **hdf5** files and **NetCDF** files can be written and loaded in parallel in Python \n",
        "* Using hdf5 files in particular can make a task like the above a little easier\n",
        "* For example, we could write a similar 2d array to an h5 file in parallel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "\n",
        "from mpi4py import MPI\n",
        "import h5py, numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "ny, nx = 12, 8\n",
        "\n",
        "with h5py.File(\"state.h5\", \"w\", driver=\"mpio\", comm=comm) as f:\n",
        "    dset = f.create_dataset(\"u\", (ny, nx), dtype=\"f32\") \n",
        "\n",
        "    base, rem = divmod(ny, size)\n",
        "    ny_loc = base + (1 if rank < rem else 0)\n",
        "    i0 = rank * base + min(rank, rem)\n",
        "\n",
        "    local = np.full((ny_loc, nx), float(rank), dtype=\"f32\")\n",
        "\n",
        "    # with dset.collective:\n",
        "    dset[i0:i0+ny_loc, :] = local            # parallel slice write\n",
        "\n",
        "comm.Barrier()\n",
        "if rank == 0:\n",
        "    print(\"Wrote state.h5:/u\")\n",
        "    \n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/workspaces/CSCI-491-591/hello_mpi.py\", line 3, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspaces/CSCI-491-591/hello_mpi.py\", line 3, in <module>\n",
            "    import h5py, numpy as np\n",
            "    import h5py, numpy as np\n",
            "ModuleNotFoundErrorModuleNotFoundError: No module named 'h5py'\n",
            ": No module named 'h5py'\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspaces/CSCI-491-591/hello_mpi.py\", line 3, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspaces/CSCI-491-591/hello_mpi.py\", line 3, in <module>\n",
            "    import h5py, numpy as np\n",
            "    import h5py, numpy as np\n",
            "ModuleNotFoundError: No module named 'h5py'ModuleNotFoundError: No module named 'h5py'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
