{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/lecture6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slRqa4xRq8AM"
      },
      "source": [
        "# Advanced mpi4py Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-AXH3DarAOn"
      },
      "source": [
        "* In this lecture we'll cover a handful of advanced features in mpi4py\n",
        "* We'll discuss parallel I/O, persistent communication, and one sided communication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PCVrBNdr8LD"
      },
      "source": [
        "### Persistent Communication\n",
        "* In some of examples, particularly the heat equation, we sent many of the same type of message repreatedly in a loop\n",
        "* In such cases, communication can be optimized by using persistent communication, a particular case of nonblocking communication allowing the reduction of the overhead\n",
        "* For point-to-point communication, persistent communication is used by setting up requests with `Send_init` and Recv_init`\n",
        "* In each loop iteration, you would then call `Start` or `Startall` and subsequently `Wait` or `Waitall`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeZI-879thSi"
      },
      "source": [
        "#### 1. Usage Pattern\n",
        "* Create a request one time\n",
        "```python\n",
        "req_s = comm.Send_init(buf, dest=..., tag=...)\n",
        "req_r = comm.Recv_init(buf, source=..., tage=...)\n",
        "```\n",
        "* In a loop you can repeat the message with the outlines form many times\n",
        "```python\n",
        "req_s.Start()\n",
        "req_r.Start()\n",
        "MPI.Request.Waitall([req_s, req_r])\n",
        "```\n",
        "After you're finished sending messages, clean up with\n",
        "```python\n",
        "req_s.Free()\n",
        "req_r.Free()\n",
        "```\n",
        "* It's a little funky to have to free something in a Python program, but a persistent request creates a `request` that holds onto\n",
        "  * A pointer to the buffer\n",
        "  * Datatype description\n",
        "  * The communicator and tag\n",
        "* Free will tell MPI you're done with these resources and is another reminder of how MPI is a lower level library being wrapped in Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKeXxf5HvGKY"
      },
      "source": [
        "#### 2. Example: Sending Data in a Ring!\n",
        "* Below, let's look at an example where we have each rank send some information to its left, wrapping around to the last rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "suj15gkRAZZT"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm  = MPI.COMM_WORLD\n",
        "rank  = comm.Get_rank()\n",
        "size  = comm.Get_size()\n",
        "\n",
        "right = (rank + 1) % size\n",
        "left  = (rank - 1) % size\n",
        "\n",
        "sendbuf = np.array(rank, dtype='i')   # Will send our rank\n",
        "recvbuf = np.array(-1, dtype='i')     # Will receive from 'left'\n",
        "\n",
        "# Build persistent requests once\n",
        "send_req = comm.Send_init(sendbuf, dest=right, tag=0)\n",
        "recv_req = comm.Recv_init(recvbuf,  source=left,  tag=0)\n",
        "\n",
        "n_iters = 5\n",
        "for it in range(n_iters):\n",
        "    # Optionally update what we send each iter\n",
        "    sendbuf[...] = rank + 100*it\n",
        "\n",
        "    # Start both; then wait for both\n",
        "    send_req.Start()\n",
        "    recv_req.Start()\n",
        "    MPI.Request.Waitall([send_req, recv_req])\n",
        "\n",
        "    print(f\"[iter {it}] rank {rank} got {recvbuf} from {left}\")\n",
        "\n",
        "send_req.Free()\n",
        "recv_req.Free()\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[iter 0] rank 0 got 3 from 3\n",
            "[iter 1] rank 0 got 103 from 3\n",
            "[iter 2] rank 0 got 203 from 3\n",
            "[iter 3] rank 0 got 303 from 3\n",
            "[iter 4] rank 0 got 403 from 3\n",
            "[iter 0] rank 1 got 0 from 0\n",
            "[iter 1] rank 1 got 100 from 0\n",
            "[iter 2] rank 1 got 200 from 0\n",
            "[iter 3] rank 1 got 300 from 0\n",
            "[iter 4] rank 1 got 400 from 0\n",
            "[iter 0] rank 2 got 1 from 1\n",
            "[iter 1] rank 2 got 101 from 1\n",
            "[iter 2] rank 2 got 201 from 1\n",
            "[iter 3] rank 2 got 301 from 1\n",
            "[iter 4] rank 2 got 401 from 1\n",
            "[iter 0] rank 3 got 2 from 2\n",
            "[iter 1] rank 3 got 102 from 2\n",
            "[iter 2] rank 3 got 202 from 2\n",
            "[iter 3] rank 3 got 302 from 2\n",
            "[iter 4] rank 3 got 402 from 2\n"
          ]
        }
      ],
      "source": [
        "!mpiexec -n 4 python hello_mpi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* What fun, we sent some data in a ring!\n",
        "* There is potentially a slight performance benefit from persistent communication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One-Sided Communication\n",
        "* We have stated that MPI doesn't use a shared memory paradigm, hence the necessity of sending messages\n",
        "* However, this is not entirely true, as MPI supports a on-sided communication model using **Remote Memmory Access** (RMA)\n",
        "    * In RMA one cprocess (the origin) directly reads from or writes into memory exposed by another procces (the target)\n",
        "    * The target process doesn't need to call a send / receive on the other end\n",
        "* In mpi4py, one-sided operations are available using windows via the `Win` \n",
        "* The main operations for windows are:\n",
        "    * `Put`: write data into a target's windows\n",
        "    * `Get` : Read data from a target's window\n",
        "    * `Accumulate` : atomic fetch and combine into target (sum, max, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Synchronization\n",
        "* You'll probably notice that this paradigm is much like a shared memory model for a multithreaded application\n",
        "* This comes with many of the same challenges as threading including race conditions\n",
        "* RMA has a couple primary synchronization mechanisms:\n",
        "    * **Fence**: collective barrier-like synchronization\n",
        "    * **Lock / Unlock**: finer control for accessing one target at a time\n",
        "* RMA is not a true shared memory model, but it behaves much like one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Examples\n",
        "\n",
        "* This first example is very basic \n",
        "* Each process exposes a single integer which can be read or written to\n",
        "* Rank 0 writes into rank 1's window\n",
        "* `Fence` is used for synchronization much like `comm.Barrier()` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > hello_mpi.py <<'PY'\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "# Each process exposes one integer\n",
        "buf = np.array(rank, dtype='i')\n",
        "win = MPI.Win.Create(buf, comm=comm)\n",
        "\n",
        "if rank == 0:\n",
        "    data = np.array(42, dtype='i')\n",
        "    win.Fence()\n",
        "    win.Put([data, MPI.INT], target=1)   # rank 0 writes into rank 1\n",
        "    win.Fence()\n",
        "elif rank == 1:\n",
        "    win.Fence()\n",
        "    win.Fence()  # wait for data arrival\n",
        "    print(f\"Rank 1 sees {buf[0]}\")\n",
        "\n",
        "win.Free()\n",
        "PY"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM6/s6728rBWcJXlWE3MlFg",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
