{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/linear_algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFOzjMNMgxek"
      },
      "source": [
        "# Linear Algebra on GPUs\n",
        "\n",
        "* Wow, this sounds super boring, why would anyone care?\n",
        "* The solution of linear algebra problems arises frequently in scientific computing, data science, machine learning, and many other fields\n",
        "* Linear algebra is a cornerstone of modern computing, so its an important topic!\n",
        "* Problems that emerge frequently include\n",
        "    * Factorizing matrices\n",
        "    * Matrix vector multiplication\n",
        "    * Matrix times matrix multiplication\n",
        "    * Computing eigenvalues and eigenvectors\n",
        "    * Solving linear systems\n",
        "    * Sparse linear algebra\n",
        "    * Among others\n",
        "* A great deal of effort has gone into implementing efficient solutions to linear algebra problems over the years\n",
        "* But not all algorithms that were developed for CPUs can be ported directly to GPUs while maintaining performance\n",
        "\n",
        "## Example\n",
        "* Let's say we want to solve $Ax = b$ for some big matrix $A$\n",
        "* One pretty common way of doing this is by using some efficient algorithm to compute the LU decomposition (or a similar decomposition like a Cholesky decomposition)\n",
        "$$\n",
        "A = LU\n",
        "$$\n",
        "where $L$ is a lower tirangular matrix and $U$ is an upper triangular matrix.\n",
        "* This approach is especially useful when the factorization only needs to be computed once and can be reused for multiple tasks\n",
        "* The problem is now\n",
        "$$\n",
        "A = LUx = b\n",
        "$$\n",
        "which we can solve in two passes\n",
        "* First, we solve\n",
        "$$\n",
        "Ly = b\n",
        "$$\n",
        "for $y$ and subsequently\n",
        "$$\n",
        "Ux = y\n",
        "$$\n",
        "for $x$, which by simple substitution satisfies the original problem $Ax=b$\n",
        "\n",
        "* Conveniently, $Ly=b$ is easy to solve because $L$ is lower triangular.\n",
        "* It can be solved using **forward substitution**\n",
        "* Similarly, $Ux=y$ is easy to solve because $U$ is upper triangular\n",
        "* It can be solved with **backward substitution**\n",
        "\n",
        "## The Problem\n",
        "* Let's assume for a minute that a CPU and GPU are equally good at computing the LU decomposition\n",
        "> Indeed, there are some efficient algorithms for computing matrix factorizations on GPUs\n",
        "* With that assumption, how good is a GPU at solving $Ly=b$ using forward substitution or $Ux=y$ with backward substitution?\n",
        "* It's far from an ideal task for a GPU because its entirely **sequential**\n",
        "* Solving for the next element depends on the solution for the previous element\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyQ-TCBigxem"
      },
      "source": [
        "$$\n",
        "\\textbf{Forward substitution:} \\quad\n",
        "L y = b, \\quad\n",
        "L =\n",
        "\\begin{bmatrix}\n",
        "\\ell_{11} & 0         & 0         & \\cdots & 0 \\\\\n",
        "\\ell_{21} & \\ell_{22} & 0         & \\cdots & 0 \\\\\n",
        "\\ell_{31} & \\ell_{32} & \\ell_{33} & \\cdots & 0 \\\\\n",
        "\\vdots    & \\vdots    & \\vdots    & \\ddots & \\vdots \\\\\n",
        "\\ell_{n1} & \\ell_{n2} & \\ell_{n3} & \\cdots & \\ell_{nn}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Rightarrow \\quad\n",
        "\\begin{aligned}\n",
        "y_1 &= \\frac{b_1}{\\ell_{11}}, \\\\\n",
        "y_2 &= \\frac{b_2 - \\ell_{21} y_1}{\\ell_{22}}, \\\\\n",
        "y_3 &= \\frac{b_3 - \\ell_{31} y_1 - \\ell_{32} y_2}{\\ell_{33}}, \\\\\n",
        "&\\ \\vdots \\\\\n",
        "y_n &= \\frac{b_n - \\sum_{j=1}^{n-1} \\ell_{nj} y_j}{\\ell_{nn}}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\textbf{Backward substitution:} \\quad\n",
        "U x = y, \\quad\n",
        "U =\n",
        "\\begin{bmatrix}\n",
        "u_{11} & u_{12} & u_{13} & \\cdots & u_{1n} \\\\\n",
        "0      & u_{22} & u_{23} & \\cdots & u_{2n} \\\\\n",
        "0      & 0      & u_{33} & \\cdots & u_{3n} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0      & 0      & 0      & \\cdots & u_{nn}\n",
        "\\end{bmatrix}\n",
        "$$\\\n",
        "\n",
        "$$\n",
        "\\Rightarrow \\quad\n",
        "\\begin{aligned}\n",
        "x_n &= \\frac{y_n}{u_{nn}}, \\\\\n",
        "x_{n-1} &= \\frac{y_{n-1} - u_{n-1,n} x_n}{u_{n-1,n-1}}, \\\\\n",
        "x_{n-2} &= \\frac{y_{n-2} - u_{n-2,n-1} x_{n-1} - u_{n-2,n} x_n}{u_{n-2,n-2}}, \\\\\n",
        "&\\ \\vdots \\\\\n",
        "x_1 &= \\frac{y_1 - \\sum_{j=2}^{n} u_{1j} x_j}{u_{11}}.\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCLYqCw-gxem"
      },
      "source": [
        "* This is just one example of an algorithm that can be extremely efficient on the GPU, but wouldn't take advantage of the massive parallelism of the GPU\n",
        "* Because translating algorithms from CPUs directly to GPUs is often inefficient, there has been a lot of emphasis on developing efficient algorithms for linear algebra on GPUs in recent years\n",
        "* There are challenges both in the algorithmic / mathematical approaches for tackling linear algebra problems on the GPU as well as engineering problems\n",
        "* Mathematical:\n",
        "  * What algorithms are best suited for GPU parallelism?\n",
        "* Engineering:\n",
        "  * How do we implement an algorithm to utilize GPU resource efficiently?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EnraMExgxen"
      },
      "source": [
        "## Iterative Methods\n",
        "* While far from a state of the art approach, in the Navier Stokes smoke demo, we saw a different algorithm for solving a linear system that was well adapted to a GPU\n",
        "* This was an iterative method called the **Jacobi method**\n",
        "* In particular, when solving the Navier Stokes equations using the approach outlined [here](https://pages.cs.wisc.edu/~chaol/data/cs777/stam-stable_fluids.pdf), we need to compute a divergence free velocity field\n",
        "* This turns out to involve solving a Poisson equation of the form\n",
        "$$\n",
        "\\nabla \\cdot w = \\nabla^2 q\n",
        "$$\n",
        "* Solving this equation via finite elements or finite differences involves discretizing these equations to form a linear system of equations of the form $Ax=b$ where $A$ is given by:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IR-BJhLgxen"
      },
      "source": [
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "T & I &   &        &        \\\\\n",
        "I & T & I &        &        \\\\\n",
        "  & I & T & \\ddots &        \\\\\n",
        "  &   & \\ddots & \\ddots & I \\\\\n",
        "  &   &        & I & T\n",
        "\\end{bmatrix},\n",
        "\\qquad\n",
        "T =\n",
        "\\begin{bmatrix}\n",
        "-4 & 1  &        &        &        \\\\\n",
        " 1 & -4 & 1      &        &        \\\\\n",
        "   &  1 & -4     & 1      &        \\\\\n",
        "   &    & \\ddots & \\ddots & \\ddots \\\\\n",
        "   &    &        & 1      & -4\n",
        "\\end{bmatrix},\n",
        "\\qquad\n",
        "I =\n",
        "\\begin{bmatrix}\n",
        "1 &        &        &        \\\\\n",
        "  & 1      &        &        \\\\\n",
        "  &        & \\ddots &        \\\\\n",
        "  &        &        & 1\n",
        "\\end{bmatrix}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWOauf9-gxen"
      },
      "source": [
        "* The Jacobi method decomposes a matrix $A$ into a digonal matrix $D$, a lower triangular part $L$, and an upper triangular part $U$\n",
        "$$\n",
        "D = \\begin{bmatrix} a_{11} & 0 & \\cdots & 0 \\\\ 0 & a_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\0 & 0 & \\cdots & a_{nn} \\end{bmatrix} \\text{ and } L+U = \\begin{bmatrix} 0 & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & 0 & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & 0 \\end{bmatrix}$$\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM1fjd1Sgxen"
      },
      "source": [
        "* Using this decomposition, the solution $x$ is computed iteratively via\n",
        "$$\n",
        " \\mathbf{x}^{(k+1)} = D^{-1} (\\mathbf{b} - (L+U) \\mathbf{x}^{(k)})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34_FHqfggxeo"
      },
      "source": [
        "* The basic idea here is that we write\n",
        "$$\n",
        "Ax = (D + L + U)x = b\n",
        "$$\n",
        "then multiply by $D^{-1}$ to get\n",
        "$$\n",
        "(I + D^{-1} L + D^{-1} U)x = D^{-1}b  \n",
        "$$.\n",
        "* Simplifying gives\n",
        "$$\n",
        "x = D^{-1} (b - (L + U)x)\n",
        "$$\n",
        "* Then since we have two $x$'s here, we assume that we can use the previous iteration of an iterative method as a reasonable guess so that we get the iterative update formula shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khcJSSVCgxeo"
      },
      "source": [
        "## Strengths and Weaknesses of the Jacobi Method\n",
        "\n",
        "**Strengths**\n",
        "* One really nice thing about this formula is that every step involves computations that are fairly efficient on a GPU\n",
        "* In particular, both of these can be parallelized efficiently:\n",
        "    * Matrix multiplication\n",
        "    * Vector addition\n",
        "* For instance we have vector addition, and matrix multiplication.\n",
        "* Computing and multiplying by the inverse $D^{-1}$ is also trivial since $D$ is diagonal\n",
        "\n",
        "**Weaknesses**\n",
        "* The Jacobi method is not a great iterative solver\n",
        "* It tends to converge to a solution slowly, requiring many iterations\n",
        "* It does not work for every problem! It is best suited for diagonally dominant problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olxZZIVNgxeo"
      },
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rymv-SkKgxeo"
      },
      "source": [
        "## The Jacobi Method for The Poisson Equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uibDuqkCgxeo"
      },
      "source": [
        "* In the case of the Poisson equation we can prety easily write out the Jacobi iteration step\n",
        "* At each iteration the value of $u$ in the $i,j$ cell is updated as\n",
        "$$\n",
        "u^{(k+1)}_{i,j}\n",
        "= \\frac{1}{4}\\left(\n",
        "u^{(k)}_{i+1,j}\n",
        "+ u^{(k)}_{i-1,j}\n",
        "+ u^{(k)}_{i,j+1}\n",
        "+ u^{(k)}_{i,j-1}\n",
        "- h^2 f_{i,j}\n",
        "\\right)\n",
        "$$\n",
        "* Here $h$ is the cell of each grid\n",
        "* Hence, at each iteration, the value in a cell is dependent on the values of its neighbors\n",
        "* Written as a CUDA kernel in Numba, this became\n",
        "\n",
        "```python\n",
        "def jacobi_pressure(p_new, p, div, inv_dx):\n",
        "    x, y = cuda.grid(2)\n",
        "    if x >= W or y >= H: return\n",
        "    xm = x - 1 if x > 0 else x\n",
        "    xp = x + 1 if x < W-1 else x\n",
        "    ym = y - 1 if y > 0 else y\n",
        "    yp = y + 1 if y < H-1 else y\n",
        "    p_new[y, x] = 0.25 * (p[y, xp] + p[y, xm] + p[yp, x] + p[ym, x] - div[y, x] * (DX*DX))\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}