{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7d77dac9",
      "metadata": {
        "id": "7d77dac9"
      },
      "source": [
        "# Parallelizing the Heat Equation\n",
        "\n",
        "* We previously saw some examples of solving the time-dependent heat equation in NumPy. Here, we'll consider how to make this code faster by parallelizing it using MPI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa5dc269",
      "metadata": {
        "id": "fa5dc269"
      },
      "source": [
        "## Idea\n",
        "\n",
        "* The basic idea here is that we can break down the spatial domain into chunks and have each process responsible for computing the diffusion stencil for its own chunk.\n",
        "* Computing the diffusion stencil at a given cell involves the cells adjacent to it.\n",
        "* For most cells in the domain, this is fine, but what do we do at the edge of chunks?\n",
        "* We can't directly access data from a different, chunk, so we can use MPI to communicate between chunks.\n",
        "* In practice each process is responsible for doing compuatations on a given set of rows. Each row contains a subset of rows in the domain, plus top and bottom \"halo\" rows.\n",
        "* Rows from other ranks can be copied into these halos so that each process has all of the information it needs to compute the diffusion stencil on its chunk of the domain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c7a229",
      "metadata": {
        "id": "59c7a229"
      },
      "source": [
        "## Full Code\n",
        "The full code is presented here, but we'll break down each function in turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0805e6d7",
      "metadata": {
        "id": "0805e6d7"
      },
      "outputs": [],
      "source": [
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def init_field(nx, ny, hot=1.0, block=10):\n",
        "    \"\"\"\n",
        "    Initialize a 2D field with a hot square in the center.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    nx : int\n",
        "        Number of columns.\n",
        "    ny : int\n",
        "        Number of rows.\n",
        "    hot : float, optional\n",
        "        Value to assign inside the hot block (default 1.0).\n",
        "    block : int, optional\n",
        "        Approximate size of the hot block edge (default 10).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    u : ndarray of shape (ny, nx)\n",
        "        Array with the hot block set to `hot`, rest zero.\n",
        "    \"\"\"\n",
        "    u = np.zeros((ny, nx), dtype=np.float64)\n",
        "    cx, cy = nx // 2, ny // 2\n",
        "    b = block // 2\n",
        "    u[cy-b:cy+b, cx-b:cx+b] = hot\n",
        "    return u\n",
        "\n",
        "def halo_exchange(comm, u):\n",
        "    \"\"\"\n",
        "    Exchange top/bottom halo rows with neighboring ranks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    comm : MPI.Comm\n",
        "        MPI communicator.\n",
        "    u : ndarray of shape (my+2, nx)\n",
        "        Local array with one ghost row at top and bottom.\n",
        "    \"\"\"\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    up   = rank - 1 if rank > 0 else MPI.PROC_NULL\n",
        "    down = rank + 1 if rank < size - 1 else MPI.PROC_NULL\n",
        "\n",
        "    comm.Sendrecv(u[1, :],  dest=up,   sendtag=0,\n",
        "                  recvbuf=u[-1, :], source=down, recvtag=0)\n",
        "    comm.Sendrecv(u[-2, :], dest=down, sendtag=1,\n",
        "                  recvbuf=u[0, :],    source=up,   recvtag=1)\n",
        "\n",
        "def run_sim(comm, u0=None, steps=500, alpha=0.24):\n",
        "    \"\"\"\n",
        "    Distribute the initial field, run Jacobi iterations, and gather results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    comm : MPI.Comm\n",
        "        MPI communicator.\n",
        "    u0 : ndarray or None\n",
        "        Full initial condition on root; None on other ranks.\n",
        "    steps : int, optional\n",
        "        Number of Jacobi iterations (default 500).\n",
        "    alpha : float, optional\n",
        "        Diffusion coefficient for the update (default 0.24).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    U_final : ndarray or None\n",
        "        Global field on root, None on other ranks.\n",
        "    \"\"\"\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    if rank == 0:\n",
        "        ny, nx = map(int, u0.shape)\n",
        "    else:\n",
        "        ny, nx = None, None\n",
        "    ny, nx = comm.bcast((ny, nx), root=0)\n",
        "\n",
        "    edges = np.floor(np.linspace(0, ny, size + 1)).astype(np.int64)\n",
        "    starts = edges[:-1]\n",
        "    stops  = edges[1:]\n",
        "    y0, y1 = int(starts[rank]), int(stops[rank])\n",
        "    my = y1 - y0\n",
        "\n",
        "    if rank == 0:\n",
        "        counts = (stops - starts) * nx\n",
        "        displs = starts * nx\n",
        "        sendbuf = [u0, counts, displs, MPI.DOUBLE]\n",
        "    else:\n",
        "        sendbuf = None\n",
        "\n",
        "    local = np.empty((my, nx), dtype=np.float64)\n",
        "    comm.Scatterv(sendbuf, local, root=0)\n",
        "\n",
        "    u = np.zeros((my + 2, nx), dtype=np.float64)\n",
        "    u[1:-1, :] = local\n",
        "\n",
        "    for _ in range(steps):\n",
        "        halo_exchange(comm, u)\n",
        "        un = u.copy()\n",
        "        center = u[1:-1, 1:-1]\n",
        "        left   = u[1:-1, :-2]\n",
        "        right  = u[1:-1, 2:]\n",
        "        up     = u[0:-2, 1:-1]\n",
        "        down   = u[2:, 1:-1]\n",
        "        un[1:-1, 1:-1] = center + alpha*(left + right + up + down - 4.0*center)\n",
        "        u = un\n",
        "\n",
        "    interior = u[1:-1, :]\n",
        "\n",
        "    if rank == 0:\n",
        "        recvbuf = np.empty((ny, nx), dtype=u.dtype)\n",
        "        mpi_dt = MPI._typedict[interior.dtype.char]\n",
        "        comm.Gatherv(interior, [recvbuf, counts, displs, mpi_dt], root=0)\n",
        "        U_final = recvbuf\n",
        "    else:\n",
        "        comm.Gatherv(interior, None, root=0)\n",
        "        U_final = None\n",
        "\n",
        "    return U_final\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "\n",
        "    nx, ny = 1024, 1024\n",
        "    if rank == 0:\n",
        "        u0 = init_field(nx=nx, ny=ny, hot=1.0, block=250)\n",
        "    else:\n",
        "        u0 = None\n",
        "\n",
        "    u1 = run_sim(comm, u0=u0, steps=5000, alpha=0.24)\n",
        "\n",
        "    if rank == 0:\n",
        "        plt.imshow(u1)\n",
        "        plt.colorbar()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ade828c2",
      "metadata": {
        "id": "ade828c2"
      },
      "source": [
        "## 1. Initial Condition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e3fd46f",
      "metadata": {
        "id": "3e3fd46f"
      },
      "source": [
        "Let's look at the entry point code. First, it just gets the communicator and the rank. Then the rank 0 process creates an initial condition.\n",
        "\n",
        "\n",
        "```python\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "nx, ny = 1024, 1024\n",
        "if rank == 0:\n",
        "    u0 = init_field(nx=nx, ny=ny, hot=1.0, block=250)\n",
        "else:\n",
        "    u0 = None\n",
        "```\n",
        "Next, every rank calls this `run_sim` function, which runs the simulation for a given number of time steps with the given initial condition (which is really defined only on rank 0).\n",
        "\n",
        "```python\n",
        "u1 = run_sim(comm, u0=u0, steps=5000, alpha=0.24)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4a664b",
      "metadata": {
        "id": "5c4a664b"
      },
      "source": [
        "## 2. Run Simulation Function\n",
        "Next, let's take a look at the `run_sim` function. The first thing that happens is that the rank 0 process figures out the shape of the initial condition and broadcasts it to the other ranks.\n",
        "\n",
        "```python\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "if rank == 0:\n",
        "    ny, nx = map(int, u0.shape)\n",
        "else:\n",
        "    ny, nx = None, None\n",
        "ny, nx = comm.bcast((ny, nx), root=0)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0800c28",
      "metadata": {
        "id": "f0800c28"
      },
      "source": [
        "Based on the size of the number of grid cells in the domain and number of processes, we compute the start and end indexes of each chunk belonging to all processes. Then we get the start and stop index for the rank's chunk and compute its size `my`.\n",
        "```python\n",
        "edges = np.floor(np.linspace(0, ny, size + 1)).astype(np.int64)\n",
        "starts = edges[:-1]\n",
        "stops  = edges[1:]\n",
        "y0, y1 = int(starts[rank]), int(stops[rank])\n",
        "my = y1 - y0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b01621f",
      "metadata": {
        "id": "5b01621f"
      },
      "source": [
        "Once we've figured out how to break up the domain, we can split up the initial condition by scattering it amongst all ranks. Note that `Scatterv` allows scattering of different sized arrays to each process. This is necessary because the last chunk could have a different size than the rest.\n",
        "```python\n",
        "# 3) Prepare Scatterv\n",
        "if rank == 0:\n",
        "    counts = (stops - starts) * nx              # elements per rank\n",
        "    displs = starts * nx                        # element offsets\n",
        "    sendbuf = [u0, counts, displs, MPI.DOUBLE]  # explicit type for clarity\n",
        "else:\n",
        "    sendbuf = None\n",
        "\n",
        "# 4) Receive my interior rows (no halos yet)\n",
        "local = np.empty((my, nx), dtype=np.float64)\n",
        "comm.Scatterv(sendbuf, local, root=0)\n",
        "```\n",
        "At this point in the code, every rank has receieved its local chunk of the initial condition. We create an array `u` that will store the local solution, including two halo rows on the top and bottom. The initial condition is copied into the interior of tis array.\n",
        "\n",
        "```python\n",
        "# 5) Allocate local with halos and copy interiors into rows 1..my\n",
        "u = np.zeros((my + 2, nx), dtype=np.float64)\n",
        "u[1:-1] = local\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27998e07",
      "metadata": {
        "id": "27998e07"
      },
      "source": [
        "### 2.1 Simulation Loop\n",
        "After distributing the initial condition and defining an array to store the local solution, we launch into the simulation loop.\n",
        "\n",
        "```python\n",
        " for _ in range(steps):\n",
        "    # exchange halos each step\n",
        "    halo_exchange(comm, u)\n",
        "\n",
        "    un = u.copy()\n",
        "\n",
        "    center = u[1:-1, 1:-1]\n",
        "    left   = u[1:-1, :-2]   \n",
        "    right  = u[1:-1, 2:]   \n",
        "    up     = u[0:-2, 1:-1]\n",
        "    down   = u[2:, 1:-1]\n",
        "    \n",
        "    un[1:-1, 1:-1] = center + alpha*(left + right + up + down - 4.0*center)\n",
        "    u = un\n",
        "```\n",
        "This is mostly going to look very familar with the exception of calling `halo_exchange` at the beginning, as this just involves copying the initial condition, then computing the new value of all interior cells using the typical diffusion stencil. So what does `halo_exchange` do?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f128fe11",
      "metadata": {
        "id": "f128fe11"
      },
      "source": [
        "### 2.2 Halo Exchange\n",
        "The `halo_exchange` function is responsible for sending information about the rank's local solution to other ranks responsible for adjacent chunks of the domain. If the chunk belonging to a given rank has a chunk above it, it will send an array containing the first row it owns (that is its first non-halo row in its local solution) to the chunk above it. In turn the chunk above it will receive that information in its bottom-most halo row. The process is similar if the rank has another chunk below it. The chunk will send its bottom-most non-halo row to the rank below it. That rank will receive it in its top halo row.\n",
        "\n",
        "```python\n",
        "\n",
        "def halo_exchange(comm, u):\n",
        "    \"\"\"Blocking 1D (row-wise) halo exchange for u of shape (my+2, nx).\"\"\"\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "    my, nx = u.shape[0] - 2, u.shape[1]\n",
        "\n",
        "    up   = rank - 1 if rank > 0 else MPI.PROC_NULL\n",
        "    down = rank + 1 if rank < size - 1 else MPI.PROC_NULL\n",
        "\n",
        "    # Send first interior row up; recv bottom ghost from down\n",
        "    comm.Sendrecv(u[1, :],  dest=up,   sendtag=0,\n",
        "                  recvbuf=u[-1, :], source=down, recvtag=0)\n",
        "    # Send last interior row down; recv top ghost from up\n",
        "    comm.Sendrecv(u[-2, :], dest=down, sendtag=1,\n",
        "                  recvbuf=u[0, :],    source=up,   recvtag=1)\n",
        "```\n",
        "\n",
        "Note that if a rank doesn't have a chunk above / below it, then the up / down rank is defined as `MPI.PROC_NULL`. This is a convenient way to basically say that if there is no rank to send anything to, so the send won't do anything.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "233f6552",
      "metadata": {
        "id": "233f6552"
      },
      "source": [
        "## 3. Assembling a Global Solution From Local Solutions\n",
        "\n",
        "Now we're nearly done with our simulation code! However, if we'd like to plot our solution we need to stitch together all of the local solutions into a global solution . We can accomplish this usng a `Gatherv` operation, which is essentially the reverse of the `Scatterv` we used to split the initial condition into chunks earlier.\n",
        "\n",
        "```python\n",
        " if rank == 0:\n",
        "        recvbuf = np.empty((ny, nx), dtype=u.dtype)  # global stitched array\n",
        "        # If you want to be explicit about type:\n",
        "        mpi_dt = MPI._typedict[interior.dtype.char]\n",
        "        comm.Gatherv(interior, [recvbuf, counts, displs, mpi_dt], root=0)\n",
        "    else:\n",
        "        # Non-root: counts/displs/recvbuf are None; only send your interior\n",
        "        comm.Gatherv(interior, None, root=0)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcd61e0e",
      "metadata": {
        "id": "bcd61e0e"
      },
      "source": [
        "Lastly, rank 0 will return the stitched together global solution and every other rank will return `None`.\n",
        "\n",
        "```python\n",
        "# Now rank 0 has the full field in `recvbuf`\n",
        "if rank == 0:\n",
        "    U_final = recvbuf\n",
        "else:\n",
        "    U_final = None\n",
        "\n",
        "return None\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking MPI Code"
      ],
      "metadata": {
        "id": "3CI8Zc5SH1G6"
      },
      "id": "3CI8Zc5SH1G6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, how much of a performance advantage do we get from using more processes? To find out, let's do some detailed benchmarking. First, we'll alter the entry point as follows."
      ],
      "metadata": {
        "id": "JhbHL05GH-P2"
      },
      "id": "JhbHL05GH-P2"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--measure\", action=\"store_true\",\n",
        "                        help=\"Run timed trials and print CSV (rank 0 only).\")\n",
        "    parser.add_argument(\"--repeat\", type=int, default=1,\n",
        "                        help=\"Number of timed trials to run.\")\n",
        "    parser.add_argument(\"--nx\", type=int, default=1500)\n",
        "    parser.add_argument(\"--ny\", type=int, default=1500)\n",
        "    parser.add_argument(\"--steps\", type=int, default=2500)\n",
        "    parser.add_argument(\"--alpha\", type=float, default=0.24)\n",
        "    parser.add_argument(\"--csv_header\", action=\"store_true\",\n",
        "                        help=\"Print CSV header (rank 0 only).\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.measure:\n",
        "        # optional header\n",
        "        if rank == 0 and args.csv_header:\n",
        "            print(\"n,nx,ny,steps,alpha,trial,wall_max_s,wall_mean_rank_s,wall_std_rank_s,time_per_core_s\")\n",
        "\n",
        "        for trial in range(args.repeat):\n",
        "            # fresh IC each trial (keeps runs comparable)\n",
        "            u0 = init_field(nx=args.nx, ny=args.ny, hot=1.0, block=min(args.nx, args.ny)//4) if rank == 0 else None\n",
        "\n",
        "            comm.Barrier()\n",
        "            t0 = MPI.Wtime()\n",
        "            _ = run_sim(comm, u0=u0, steps=args.steps, alpha=args.alpha)\n",
        "            comm.Barrier()\n",
        "            t1 = MPI.Wtime()\n",
        "\n",
        "            # rank-local elapsed\n",
        "            elapsed = t1 - t0\n",
        "\n",
        "            # summarize across ranks\n",
        "            wall_max = comm.allreduce(elapsed, op=MPI.MAX)  # job wall time\n",
        "            wall_sum = comm.allreduce(elapsed, op=MPI.SUM)\n",
        "            wall_sumsq = comm.allreduce(elapsed * elapsed, op=MPI.SUM)\n",
        "            wall_mean_rank = wall_sum / size\n",
        "            var = max(0.0, wall_sumsq / size - wall_mean_rank * wall_mean_rank)\n",
        "            wall_std_rank = var ** 0.5\n",
        "\n",
        "            if rank == 0:\n",
        "                time_per_core = wall_max / size\n",
        "                print(f\"{size},{args.nx},{args.ny},{args.steps},{args.alpha},{trial},\"\n",
        "                      f\"{wall_max:.6f},{wall_mean_rank:.6f},{wall_std_rank:.6f},{time_per_core:.6f}\",\n",
        "                      flush=True)\n",
        "    else:\n",
        "        # normal, single run (no timing/CSV)\n",
        "        nx, ny = 1024, 1024\n",
        "        u0 = init_field(nx=nx, ny=ny, hot=1.0, block=250) if rank == 0 else None\n",
        "        u1 = run_sim(comm, u0=u0, steps=5000, alpha=0.24)\n",
        "        if rank == 0:\n",
        "            plt.imshow(u1)\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HnzLrTlEIYyh"
      },
      "id": "HnzLrTlEIYyh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Command Line Arguments\n",
        "\n",
        "Let's break this down into pieces. First, add flags to pass in some model parameters as command line arguments like grid size and the number of time steps.\n",
        "\n",
        "\n",
        "```python\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--measure\", action=\"store_true\",\n",
        "                    help=\"Run timed trials and print CSV (rank 0 only).\")\n",
        "parser.add_argument(\"--repeat\", type=int, default=1,\n",
        "                    help=\"Number of timed trials to run.\")\n",
        "parser.add_argument(\"--nx\", type=int, default=1500)\n",
        "parser.add_argument(\"--ny\", type=int, default=1500)\n",
        "parser.add_argument(\"--steps\", type=int, default=2500)\n",
        "parser.add_argument(\"--alpha\", type=float, default=0.24)\n",
        "parser.add_argument(\"--csv_header\", action=\"store_true\",\n",
        "                    help=\"Print CSV header (rank 0 only).\")\n",
        "args = parser.parse_args()\n",
        "```\n",
        " In addition to adding flags for model parameters, we include a few optional flags that are used for benchmarking. For example adding the `--measure` flag will enable benchmarking mode and `--repeat` can be used to specify the number of times a model run is repeated when benchmarking so we can compute timing statistics.\n"
      ],
      "metadata": {
        "id": "kXCbjDXZJCmu"
      },
      "id": "kXCbjDXZJCmu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeated Trials and Statistics\n",
        "\n",
        "We perform a number of model runs specified by command line argument or 1 run by default. In each trial we reset the model with a new initial condition.\n",
        "```python\n",
        "for trial in range(args.repeat):\n",
        "    # fresh IC each trial (keeps runs comparable)\n",
        "    u0 = init_field(nx=args.nx, ny=args.ny, hot=1.0, block=min(args.nx, args.ny)//4) if rank == 0 else None\n",
        "```\n",
        "An important consideration when timing MPI code is that each process can have a slightly different completion time. Moreover the usual Python `time` module tends to be inaccurate, so we need a different approach for benchmarking. The code below uses `comm.Barrier()` to block until all processes are synchronized (that is they've reached the same point in the code). Then we use `MPI.Wtime()` to get the current wall time.\n",
        "```python\n",
        "comm.Barrier()\n",
        "t0 = MPI.Wtime()\n",
        "_ = run_sim(comm, u0=u0, steps=args.steps, alpha=args.alpha)\n",
        "comm.Barrier()\n",
        "t1 = MPI.Wtime()\n",
        "\n",
        "# rank-local elapsed\n",
        "elapsed = t1 - t0\n",
        "\n",
        "# summarize across ranks\n",
        "wall_max = comm.allreduce(elapsed, op=MPI.MAX)  # job wall time\n",
        "wall_sum = comm.allreduce(elapsed, op=MPI.SUM)\n",
        "wall_sumsq = comm.allreduce(elapsed * elapsed, op=MPI.SUM)\n",
        "wall_mean_rank = wall_sum / size\n",
        "var = max(0.0, wall_sumsq / size - wall_mean_rank * wall_mean_rank)\n",
        "wall_std_rank = var ** 0.5\n",
        "\n",
        "if rank == 0:\n",
        "    time_per_core = wall_max / size\n",
        "    print(f\"{size},{args.nx},{args.ny},{args.steps},{args.alpha},{trial},\"\n",
        "          f\"{wall_max:.6f},{wall_mean_rank:.6f},{wall_std_rank:.6f},{time_per_core:.6f}\",\n",
        "          flush=True)\n",
        "```"
      ],
      "metadata": {
        "id": "u1gfUVK0Md8q"
      },
      "id": "u1gfUVK0Md8q"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}