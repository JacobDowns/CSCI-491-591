{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bcdf33b5",
      "metadata": {
        "id": "bcdf33b5"
      },
      "source": [
        "# CuPy\n",
        "\n",
        "* CuPy is functionally similar to Numba-CUDA but more closely mimics NumPy\n",
        "* Many operations can be written using NumPy-like syntax\n",
        "* CuPy has drop-in equivalents of most NumPy operations and syntax syntax with GPU arrays (`cp.ndarray`), ufuncs, broadcasting, reductions.\n",
        "* Unlike Numba-CUDA, it provides implementations of a number of more complex operations in `cupyx.scipy`\n",
        "    * FFT\n",
        "    * linear algebra\n",
        "    * sparse matrices\n",
        "    * signal processing\n",
        "    * random numbers\n",
        "* These can make cupy easier to use for many applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92eaffd4",
      "metadata": {
        "id": "92eaffd4"
      },
      "source": [
        "\n",
        "## CuPy Array Basics (NumPy, but on GPU)\n",
        "* Basically, you can replace `numpy as np` with `cupy as cp` and most ufuncs and array ops just work\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1a74efec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a74efec",
        "outputId": "ccf8716d-e284-4511-b2c3-c742ccb02b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000000,) float32 <class 'cupy.ndarray'>\n",
            "Host preview: [ 1.          1.3817732   0.49315065 -0.8488725  -1.4104462 ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cupy as cp\n",
        "a = cp.arange(10**6, dtype=cp.float32)\n",
        "b = cp.sin(a) + cp.cos(a)  # ufuncs on GPU\n",
        "print(b.shape, b.dtype, type(b))\n",
        "print(\"Host preview:\", cp.asnumpy(b[:5]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using `cp.arange` will allocate an array on GPU device memory\n",
        "* Ufuncs will perform operations on the GPU\n",
        "* `cp.asnumpy` will copy back to the host\n",
        "To minimize data transfer, use operations like `cp.asarray` and `cp.asnumpy` only when needed"
      ],
      "metadata": {
        "id": "dBlRttD7aWA3"
      },
      "id": "dBlRttD7aWA3"
    },
    {
      "cell_type": "markdown",
      "id": "1e4f2a7e",
      "metadata": {
        "id": "1e4f2a7e"
      },
      "source": [
        "### Broadcasting, reductions, and axis ops\n",
        "* The normal broadcasting, indexing, and reduction operations operate as one would expect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0be2b450",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0be2b450",
        "outputId": "11b47114-3e78-4c9c-9d06-73cde38203e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4096,) float32\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = cp.random.random((4096, 4096), dtype=cp.float32)\n",
        "w = cp.linspace(0, 1, x.shape[1], dtype=cp.float32)\n",
        "y = x * w                       # broadcast\n",
        "col_means = y.mean(axis=0)      # reduction on GPU\n",
        "print(col_means.shape, col_means.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6680005",
      "metadata": {
        "id": "e6680005"
      },
      "source": [
        "\n",
        "## Defining Kernels\n",
        "\n",
        "* There are a handful of options for defining kernels in CuPy. These present many options for balancing complexity and fine-grain control.\n",
        "\n",
        "| Approach | Level of Control | Language you write | How you define it | How you launch it | Can use `threadIdx`/`blockIdx` & shared mem? | Best for | Pros | Cons |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| **Vectorized CuPy ops (ufuncs, broadcasting, reductions)** | Low | Python (NumPy-like) | Just write `cp.sin(x)`, `x*y`, `x.mean(axis=0)`, etc. | Regular function call | No, abstracted away | Elementwise/reduction math, linear algebra via `cp.linalg`, FFT via `cupyx.scipy.fft` | Fast to write; leverages vendor libs (cuBLAS/cuFFT); very readable | Limited low-level control; performance tuning mostly indirect |\n",
        "| **`ElementwiseKernel`** | Medium | CUDA C *expression strings* | `ElementwiseKernel(in_params, out_params, operation, name)` | Call like a function: `k(x, ...)` | no explicit thread/block math | Custom elementwise transforms | Minimal boilerplate; in-place and broadcasting friendly | Logic must fit elementwise form; no shared memory |\n",
        "| **`ReductionKernel`** | Medium | CUDA C *expression strings* | `ReductionKernel(in_params, out_params, map_expr, reduce_expr, post_map_expr, identity, name)` | Call like a function: `k(x, ...)` | no | Custom reductions (sum of f(x), argmax, etc.) | Concise custom reductions; good perf | Reduction pattern only; more complex to reason about |\n",
        "| **`cupyx.jit` – `@jit.kernel()`** | Medium–High | Restricted Python (JIT to CUDA) | Write Python function with array indexing; decorate with `@jit.kernel()` | Call like a function (grid/block inferred or provided) | APIs for `threadIdx`, `blockIdx`, shared mem via `jit.shared_memory`) | Custom elementwise/nd kernels with Python syntax | Pythonic; no C string; easy to iterate | Still a restricted subset; not as feature-complete as raw CUDA C |\n",
        "| **`cupyx.jit` – `@jit.rawkernel()`** | High | Restricted Python with explicit launch math | Define func with `@jit.rawkernel()` and compute global index manually | CUDA-like launch: `f[(blocks,), (threads,)](args...)` |full index control; shared mem via `jit.shared_memory` | Hand-tuned kernels without leaving Python | Good balance of control & ergonomics | Some CUDA features may require workarounds; subset semantics |\n",
        "| **`RawKernel`** | Highest | CUDA C/C++ (as a string) | `cp.RawKernel(src, \"func_name\")` with CUDA code | CUDA-like launch: `k((blocks,), (threads,), (args...), shared_mem=...)` | full control | Performance-critical kernels; shared memory tiling; intrinsics | Full CUDA power; predictable | You write/maintain CUDA C strings; harder to prototype |\n",
        "| **`RawModule`** | Highest | CUDA C/C++ (string or file), PTX | `cp.RawModule(code=..., path=..., options=...)`; then `get_function(\"name\")` | Same as `RawKernel` once you get the function | yes| Packaging multiple kernels, using headers, templates, linking | Organize many kernels; reuse across cells | Build/link errors are more complex; still CUDA C |\n",
        "\n",
        "* The best way to start out with CuPy is to use the vectorized operations, ufuncs, broadcasting / other NumPy-esque features\n",
        "* Branch out and expand whre more fine-grain control / complexity is needed\n",
        "\n",
        "> **Rule of thumb:** Start with **CuPy** vectorization; drop to `cupyx.jit`/RawKernel or **Numba** only for hotspots that need custom kernels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ce331e",
      "metadata": {
        "id": "00ce331e"
      },
      "source": [
        "\n",
        "## Elementwise Kernels\n",
        "* Elemntwise and reduction kernels can be a convenient alternative to writing full CUDA kerneels\n",
        "* You don't have to worry about launch syntax like specifying the number of blocks / threads per block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e0a8409a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0a8409a",
        "outputId": "11595f4d-c9bb-4a66-8e5f-1af0b840ae95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elementwise: [1.        1.2857143 1.5714285 1.8571429 2.142857  2.4285715 2.7142859\n",
            " 3.       ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from cupy import ElementwiseKernel, ReductionKernel\n",
        "import cupy as cp\n",
        "\n",
        "# Elementwise: y = a*x + b\n",
        "axpb = ElementwiseKernel(\n",
        "    in_params='float32 x, float32 a, float32 b',\n",
        "    out_params='float32 y',\n",
        "    operation='y = a * x + b;',\n",
        "    name='axpb'\n",
        ")\n",
        "\n",
        "x = cp.linspace(0, 1, 8, dtype=cp.float32)\n",
        "y = axpb(x, 2.0, 1.0)\n",
        "print(\"Elementwise:\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction Kernels\n",
        "\n",
        "* The `cupy.ReductionKernel` API lets you define your own reductions on the GPU (e.g. sum, mean, norm, min/max, etc.) using a few simple expressions — no need to write explicit CUDA kernels.\n",
        "\n",
        "* It's roughly analogous to a CUDA kernel that:\n",
        "1. Maps each input element through some expression (`map_expr`),\n",
        "2. Reduces all mapped results using an associative binary operation (`reduce_expr`),\n",
        "3. Optionally transforms the final accumulator before writing output (`post_map_expr`),\n",
        "4. Starts with an identity (neutral) value for the accumulator.\n"
      ],
      "metadata": {
        "id": "ZOIc7sJTj9Xm"
      },
      "id": "ZOIc7sJTj9Xm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here's an example of a reduction kernel"
      ],
      "metadata": {
        "id": "NpPPkfpsecAc"
      },
      "id": "NpPPkfpsecAc"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reduction: sum of squares\n",
        "sum_squares = ReductionKernel(\n",
        "    in_params='float32 x',\n",
        "    out_params='float32 y',\n",
        "    map_expr='x * x',\n",
        "    reduce_expr='a + b',\n",
        "    post_map_expr='y = a',\n",
        "    identity='0',\n",
        "    name='sum_squares'\n",
        ")\n",
        "print(\"Reduction:\", sum_squares(cp.arange(10, dtype=cp.float32)))\n"
      ],
      "metadata": {
        "id": "pK9OSu9Qee_J",
        "outputId": "0518ff5c-1472-400d-a8d8-0678be2e7726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pK9OSu9Qee_J",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduction: 285.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133ab219",
      "metadata": {
        "id": "133ab219"
      },
      "source": [
        "## Raw Kernels\n",
        "* A raw kernel is basically justa CUDA C kernel written as a string\n",
        "* `cp.RawKernel` lets you write CUDA C kernels (as strings) and launch from Python\n",
        "* This is useful for relatively simple kernels that might not be easily expressed as NumPy-like vectorized operations\n",
        "* Raw kernels require you to specify the block and thread count when launching as in Numba-CUDA / standard CUDA kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "89f0e45f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89f0e45f",
        "outputId": "7c4fcaa7-c389-4351-c67f-fbd80e992c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RawKernel ok; out[0:5] = [0.29604462 0.523803   0.5467715  0.13893062 0.36670983]\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "\n",
        "# RawKernel (CUDA C)\n",
        "saxpy_src = r'''\n",
        "extern \"C\" __global__\n",
        "void saxpy(const float a, const float* __restrict__ x,\n",
        "           const float* __restrict__ y, float* __restrict__ out, int n) {\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i < n) out[i] = a * x[i] + y[i];\n",
        "}\n",
        "''';\n",
        "saxpy_kernel = cp.RawKernel(saxpy_src, \"saxpy\")\n",
        "\n",
        "# test data\n",
        "n = 1_000_000\n",
        "x = cp.random.random(n, dtype=cp.float32)\n",
        "y = cp.random.random(n, dtype=cp.float32)\n",
        "out = cp.empty_like(x)\n",
        "\n",
        "threads = 256\n",
        "blocks = (n + threads - 1) // threads\n",
        "saxpy_kernel((blocks,), (threads,), (2.0, x, y, out, n))\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "print(\"RawKernel ok; out[0:5] =\", out[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6d90e2",
      "metadata": {
        "id": "1d6d90e2"
      },
      "source": [
        "\n",
        "## `cupyx.jit`: CUDA-like kernels in Python\n",
        "* `cupyx.jit` JIT-compiles a restricted Python subset to CUDA\n",
        "* Its a middle ground between high-level array ops and CUDA C strings\n",
        "* Very similar to defining Numba-CUDA kernels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debe6fc7",
      "metadata": {
        "id": "debe6fc7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from cupyx import jit\n",
        "import cupy as cp\n",
        "\n",
        "@jit.rawkernel()\n",
        "def saxpy_jit(a, x, y, out):\n",
        "    i = jit.blockDim.x * jit.blockIdx.x + jit.threadIdx.x\n",
        "    if i < x.size:\n",
        "        out[i] = a * x[i] + y[i]\n",
        "\n",
        "n = 1_000_000\n",
        "x = cp.random.random(n, dtype=cp.float32)\n",
        "y = cp.random.random(n, dtype=cp.float32)\n",
        "out = cp.empty_like(x)\n",
        "threads = 256\n",
        "blocks = (n + threads - 1) // threads\n",
        "saxpy_jit[(blocks,), (threads,)](2.0, x, y, out)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "print(\"cupyx.jit ok; out[0:5] =\", out[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f0851d",
      "metadata": {
        "id": "88f0851d"
      },
      "source": [
        "\n",
        "## 8) SciPy‑Compatible: FFT, Linear Algebra, Sparse\n",
        "`cupyx.scipy` mirrors SciPy APIs and calls high‑performance GPU libs (cuBLAS, cuFFT, cuSPARSE, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bb4df4",
      "metadata": {
        "id": "c7bb4df4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cupy as cp\n",
        "from cupyx.scipy import fftpack, linalg, sparse\n",
        "\n",
        "# FFT round‑trip\n",
        "x = cp.random.random(1<<20).astype(cp.complex64)\n",
        "X = fftpack.fft(x)\n",
        "x_rec = fftpack.ifft(X)\n",
        "print(\"FFT round‑trip error:\", float(cp.max(cp.abs(x - x_rec))))\n",
        "\n",
        "# Linear solve: Ax=b\n",
        "A = cp.random.random((1024, 1024), dtype=cp.float32)\n",
        "b = cp.random.random(1024, dtype=cp.float32)\n",
        "x = linalg.solve(A, b)\n",
        "r = cp.linalg.norm(A @ x - b) / cp.linalg.norm(b)\n",
        "print(\"Solve relative residual:\", float(r))\n",
        "\n",
        "# Sparse example\n",
        "rows = cp.array([0, 0, 1, 2, 2, 2])\n",
        "cols = cp.array([0, 2, 2, 0, 1, 2])\n",
        "vals = cp.array([1, 2, 3, 4, 5, 6], dtype=cp.float32)\n",
        "S = sparse.coo_matrix((vals, (rows, cols)), shape=(3,3)).tocsr()\n",
        "d = S.dot(cp.array([1,2,3], dtype=cp.float32))\n",
        "print(\"Sparse dot:\", d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88fab1c",
      "metadata": {
        "id": "c88fab1c"
      },
      "source": [
        "\n",
        "## 9) Random Numbers & Streams/Events\n",
        "CuPy RNG mirrors NumPy; versions may support Philox/XORWOW/MRG32k3a backends. Streams/events enable concurrency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb666775",
      "metadata": {
        "id": "bb666775"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cupy as cp\n",
        "rng = cp.random.default_rng(seed=123)\n",
        "u = rng.random(5, dtype=cp.float32)\n",
        "n = rng.normal(0, 1, 5, dtype=cp.float32)\n",
        "print(\"Uniform:\", u)\n",
        "print(\"Normal :\", n)\n",
        "\n",
        "# Streams demo (toy): overlapping ops\n",
        "s1 = cp.cuda.Stream(non_blocking=True)\n",
        "s2 = cp.cuda.Stream(non_blocking=True)\n",
        "a = cp.empty((1<<20,), dtype=cp.float32)\n",
        "b = cp.empty_like(a)\n",
        "with s1: a.fill(1.0)\n",
        "with s2: b.fill(2.0)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "print(\"Streams ok; a[0], b[0] =\", float(a[0]), float(b[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac5e98aa",
      "metadata": {
        "id": "ac5e98aa"
      },
      "source": [
        "\n",
        "## 10) Timing & Profiling\n",
        "Rule: **synchronize** before/after timing (CUDA is async). Use timers or CUDA events.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a79996",
      "metadata": {
        "id": "22a79996"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cupy as cp, time\n",
        "\n",
        "def bench(func, *args, warmup=3, iters=10, synchronize=True, **kwargs):\n",
        "    for _ in range(warmup):\n",
        "        func(*args, **kwargs)\n",
        "    if synchronize: cp.cuda.Stream.null.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(iters):\n",
        "        func(*args, **kwargs)\n",
        "    if synchronize: cp.cuda.Stream.null.synchronize()\n",
        "    t1 = time.perf_counter()\n",
        "    return (t1 - t0) / iters\n",
        "\n",
        "x = cp.random.random((1<<24,), dtype=cp.float32)\n",
        "def op(x): return cp.tanh(cp.sin(x) + 0.1*x)\n",
        "\n",
        "t_ms = bench(op, x) * 1e3\n",
        "print(f\"Avg time/op: {t_ms:.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0ab4bc",
      "metadata": {
        "id": "4e0ab4bc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# CUDA events\n",
        "import cupy as cp\n",
        "start, stop = cp.cuda.Event(), cp.cuda.Event()\n",
        "\n",
        "x = cp.random.random((1<<24,), dtype=cp.float32)\n",
        "start.record()\n",
        "y = cp.tanh(cp.sin(x) + 0.1*x)\n",
        "stop.record(); stop.synchronize()\n",
        "print(\"Elapsed (ms):\", cp.cuda.get_elapsed_time(start, stop))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2d68de",
      "metadata": {
        "id": "aa2d68de"
      },
      "source": [
        "\n",
        "## 11) Pitfalls & Best Practices\n",
        "- **Data transfer dominates**: batch device work; avoid frequent small host↔device copies.\n",
        "- **Asynchrony**: `synchronize()` for timing or host reads.\n",
        "- **Dtypes**: prefer `float32` unless double is necessary.\n",
        "- **Memory**: reuse arrays; leverage the memory pool; avoid per‑iteration allocations.\n",
        "- **Vectorize first**: use CuPy ufuncs/broadcasting before custom kernels.\n",
        "- **Interop**: use `__cuda_array_interface__` / DLPack with Numba, PyTorch, JAX, etc.\n",
        "- **Streams**: overlap transfers/compute to hide latency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d74232",
      "metadata": {
        "id": "43d74232"
      },
      "source": [
        "\n",
        "## 12) Quick Decision Guide\n",
        "- Maps to NumPy ops (elementwise, reductions, BLAS, FFT, sparse)? → **CuPy**.\n",
        "- Needs custom control (warp‑level ops, shared memory tiling)? → **Numba‑CUDA** or CuPy RawKernel/`cupyx.jit`.\n",
        "- Need SciPy‑like GPU ecosystem fast? → **CuPy (`cupyx.scipy`)**.\n",
        "- Prototype high‑level in CuPy; specialize hotspots as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad656fc8",
      "metadata": {
        "id": "ad656fc8"
      },
      "source": [
        "\n",
        "## 13) Hands‑On Exercises\n",
        "1. **Vectorized Warm‑up (CuPy):** Implement `softmax` for a 2D batch using ufuncs/broadcasting and compare timings vs NumPy CPU.\n",
        "2. **ElementwiseKernel:** Implement `leaky_relu(x, alpha=0.1)`.\n",
        "3. **ReductionKernel:** Compute per‑row L2 norms for a large matrix.\n",
        "4. **cupyx.jit or RawKernel:** Implement a tiled matrix multiply (e.g., 16×16). Compare to `cp.matmul`.\n",
        "5. **Interop with Numba:** Pass a CuPy array to a Numba kernel via `__cuda_array_interface__` and modify in place.\n",
        "6. **Profiling:** Use CUDA events to compare CuPy vectorized vs `cupyx.jit` vs RawKernel for the same op.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe69fe4f",
      "metadata": {
        "id": "fe69fe4f"
      },
      "source": [
        "\n",
        "### Appendix: Zero‑copy Numba interop via `__cuda_array_interface__`\n",
        "Numba understands CuPy device arrays directly (no copies).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea49bf82",
      "metadata": {
        "id": "ea49bf82"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cupy as cp\n",
        "from numba import cuda\n",
        "\n",
        "# Create a CuPy array\n",
        "cp_arr = cp.arange(16, dtype=cp.float32).reshape(4,4)\n",
        "\n",
        "@cuda.jit\n",
        "def scale_inplace(a, s):\n",
        "    i, j = cuda.grid(2)\n",
        "    if i < a.shape[0] and j < a.shape[1]:\n",
        "        a[i, j] *= s\n",
        "\n",
        "threads = (16, 16)\n",
        "blocks = ((cp_arr.shape[0] + threads[0] - 1)//threads[0],\n",
        "          (cp_arr.shape[1] + threads[1] - 1)//threads[1])\n",
        "\n",
        "# Pass cp_arr directly (zero‑copy interface)\n",
        "scale_inplace[blocks, threads](cp_arr, 3.0)\n",
        "cuda.synchronize()\n",
        "print(\"Scaled via Numba, back in CuPy:\", cp_arr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b709a7",
      "metadata": {
        "id": "46b709a7"
      },
      "source": [
        "\n",
        "## References\n",
        "- CuPy docs: https://docs.cupy.dev  \n",
        "- Numba CUDA: https://numba.readthedocs.io/en/stable/cuda/index.html  \n",
        "- CUDA Array Interface: https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html  \n",
        "- DLPack spec: https://dmlc.github.io/dlpack/latest/\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}