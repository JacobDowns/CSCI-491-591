{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdf33b5",
   "metadata": {},
   "source": [
    "# CuPy \n",
    "\n",
    "* CuPy is functionally similar to Numba-CUDA but more closely mimics NumPy\n",
    "* Many operations can be written using NumPy-like syntax \n",
    "* CuPy has drop-in equivalents of most NumPy operations and syntax syntax with GPU arrays (`cp.ndarray`), ufuncs, broadcasting, reductions.\n",
    "* Unlike Numba-CUDA, it provides implementations of a number of more complex operations in `cupyx.scipy` \n",
    "    * FFT\n",
    "    * linear algebra\n",
    "    * sparse matrices\n",
    "    * signal processing \n",
    "    * random numbers\n",
    "* These can make cupy easier to use for many applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecee97",
   "metadata": {},
   "source": [
    "## Numba-CUDA v. CuPy\n",
    "* While CuPy emphasizes higher level abstractions, many of the features of Numba-CUDA are also available in CuPy\n",
    "\n",
    "### Custom Kernels\n",
    "* There are a handful of options for defining kernels in CuPy. These present many options for balancing complexity and fine-grain control. \n",
    "\n",
    "| Approach | Level of Control | Language you write | How you define it | How you launch it | Can use `threadIdx`/`blockIdx` & shared mem? | Best for | Pros | Cons |\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| **Vectorized CuPy ops (ufuncs, broadcasting, reductions)** | Low | Python (NumPy-like) | Just write `cp.sin(x)`, `x*y`, `x.mean(axis=0)`, etc. | Regular function call | No, abstracted away | Elementwise/reduction math, linear algebra via `cp.linalg`, FFT via `cupyx.scipy.fft` | Fast to write; leverages vendor libs (cuBLAS/cuFFT); very readable | Limited low-level control; performance tuning mostly indirect |\n",
    "| **`ElementwiseKernel`** | Medium | CUDA C *expression strings* | `ElementwiseKernel(in_params, out_params, operation, name)` | Call like a function: `k(x, ...)` | no explicit thread/block math | Custom elementwise transforms | Minimal boilerplate; in-place and broadcasting friendly | Logic must fit elementwise form; no shared memory |\n",
    "| **`ReductionKernel`** | Medium | CUDA C *expression strings* | `ReductionKernel(in_params, out_params, map_expr, reduce_expr, post_map_expr, identity, name)` | Call like a function: `k(x, ...)` | no | Custom reductions (sum of f(x), argmax, etc.) | Concise custom reductions; good perf | Reduction pattern only; more complex to reason about |\n",
    "| **`cupyx.jit` – `@jit.kernel()`** | Medium–High | Restricted Python (JIT to CUDA) | Write Python function with array indexing; decorate with `@jit.kernel()` | Call like a function (grid/block inferred or provided) | APIs for `threadIdx`, `blockIdx`, shared mem via `jit.shared_memory`) | Custom elementwise/nd kernels with Python syntax | Pythonic; no C string; easy to iterate | Still a restricted subset; not as feature-complete as raw CUDA C |\n",
    "| **`cupyx.jit` – `@jit.rawkernel()`** | High | Restricted Python with explicit launch math | Define func with `@jit.rawkernel()` and compute global index manually | CUDA-like launch: `f[(blocks,), (threads,)](args...)` |full index control; shared mem via `jit.shared_memory` | Hand-tuned kernels without leaving Python | Good balance of control & ergonomics | Some CUDA features may require workarounds; subset semantics |\n",
    "| **`RawKernel`** | Highest | CUDA C/C++ (as a string) | `cp.RawKernel(src, \"func_name\")` with CUDA code | CUDA-like launch: `k((blocks,), (threads,), (args...), shared_mem=...)` | full control | Performance-critical kernels; shared memory tiling; intrinsics | Full CUDA power; predictable | You write/maintain CUDA C strings; harder to prototype |\n",
    "| **`RawModule`** | Highest | CUDA C/C++ (string or file), PTX | `cp.RawModule(code=..., path=..., options=...)`; then `get_function(\"name\")` | Same as `RawKernel` once you get the function | yes| Packaging multiple kernels, using headers, templates, linking | Organize many kernels; reuse across cells | Build/link errors are more complex; still CUDA C |\n",
    "\n",
    "* The best way to start out with CuPy is to use the vectorized operations, ufuncs, broadcasting / other NumPy-esque features\n",
    "* Branch out and expand whre more fine-grain control / complexity is needed \n",
    "\n",
    "> **Rule of thumb:** Start with **CuPy** vectorization; drop to `cupyx.jit`/RawKernel or **Numba** only for hotspots that need custom kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eaffd4",
   "metadata": {},
   "source": [
    "\n",
    "## 2) CuPy Array Basics (NumPy, but on GPU)\n",
    "Replace `numpy as np` with `cupy as cp`. Most ufuncs and array ops just work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cupy as cp\n",
    "a = cp.arange(10**6, dtype=cp.float32)\n",
    "b = cp.sin(a) + cp.cos(a)  # ufuncs on GPU\n",
    "print(b.shape, b.dtype, type(b))\n",
    "print(\"Host preview:\", cp.asnumpy(b[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f2a7e",
   "metadata": {},
   "source": [
    "### Broadcasting, reductions, and axis ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = cp.random.random((4096, 4096), dtype=cp.float32)\n",
    "w = cp.linspace(0, 1, x.shape[1], dtype=cp.float32)\n",
    "y = x * w                       # broadcast\n",
    "col_means = y.mean(axis=0)      # reduction on GPU\n",
    "print(col_means.shape, col_means.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6680005",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Data Movement & Interop\n",
    "Minimize host↔device copies. Use:\n",
    "- `cp.asarray(np_array)` (H2D), `cp.asnumpy(cp_array)` (D2H) only when needed.\n",
    "- Zero‑copy interop via `__cuda_array_interface__` and **DLPack**.\n",
    "- Numba ↔ CuPy: both support `__cuda_array_interface__` for zero‑copy exchange.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, cupy as cp\n",
    "\n",
    "np_a = np.arange(8, dtype=np.float32)\n",
    "cp_a = cp.asarray(np_a)              # H2D copy\n",
    "np_b = cp.asnumpy(cp_a * 2)          # D2H copy\n",
    "print(\"Round-trip:\", np_b)\n",
    "\n",
    "# DLPack: share device memory (zero-copy) between frameworks\n",
    "dl = cp_a.toDlpack()\n",
    "cp_b = cp.fromDlpack(dl)             # zero-copy view on same device buffer\n",
    "cp_b *= 3\n",
    "print(\"In-place via DLPack, cp_a[0:4] =>\", cp_a[:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e72c0",
   "metadata": {},
   "source": [
    "\n",
    "## 4) CuPy Memory Pools\n",
    "CuPy uses a **default memory pool** to cache freed GPU memory and reduce `cudaMalloc/cudaFree`. Introspect/adjust it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cupy as cp\n",
    "mpool = cp.cuda.MemoryPool()\n",
    "cp.cuda.set_allocator(mpool.malloc)\n",
    "\n",
    "x = cp.random.random((8192, 8192))\n",
    "del x\n",
    "print(\"Pool total bytes:\", mpool.total_bytes())\n",
    "print(\"Pool used bytes:\",  mpool.used_bytes())\n",
    "\n",
    "mpool.free_all_blocks()\n",
    "print(\"After free:\", mpool.total_bytes(), mpool.used_bytes())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce331e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Elementwise & Reduction Kernels (no explicit launch syntax)\n",
    "Convenient alternatives to writing full CUDA kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cupy import ElementwiseKernel, ReductionKernel\n",
    "import cupy as cp\n",
    "\n",
    "# Elementwise: y = a*x + b\n",
    "axpb = ElementwiseKernel(\n",
    "    in_params='float32 x, float32 a, float32 b',\n",
    "    out_params='float32 y',\n",
    "    operation='y = a * x + b;',\n",
    "    name='axpb'\n",
    ")\n",
    "\n",
    "x = cp.linspace(0, 1, 8, dtype=cp.float32)\n",
    "y = axpb(x, 2.0, 1.0)\n",
    "print(\"Elementwise:\", y)\n",
    "\n",
    "# Reduction: sum of squares\n",
    "sum_squares = ReductionKernel(\n",
    "    in_params='float32 x',\n",
    "    out_params='float32 y',\n",
    "    map_expr='x * x',\n",
    "    reduce_expr='a + b',\n",
    "    post_map_expr='y = a',\n",
    "    identity='0',\n",
    "    name='sum_squares'\n",
    ")\n",
    "print(\"Reduction:\", sum_squares(cp.arange(10, dtype=cp.float32)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ab219",
   "metadata": {},
   "source": [
    "\n",
    "## 6) RawKernel vs Numba‑CUDA: A Side‑by‑Side\n",
    "`cp.RawKernel` lets you write CUDA C kernels (as strings) and launch from Python. Below: SAXPY in RawKernel vs Numba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cupy as cp\n",
    "\n",
    "# --- RawKernel (CUDA C) ---\n",
    "saxpy_src = r'''\n",
    "extern \"C\" __global__\n",
    "void saxpy(const float a, const float* __restrict__ x,\n",
    "           const float* __restrict__ y, float* __restrict__ out, int n) {\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < n) out[i] = a * x[i] + y[i];\n",
    "}\n",
    "''';\n",
    "saxpy_kernel = cp.RawKernel(saxpy_src, \"saxpy\")\n",
    "\n",
    "# test data\n",
    "n = 1_000_000\n",
    "x = cp.random.random(n, dtype=cp.float32)\n",
    "y = cp.random.random(n, dtype=cp.float32)\n",
    "out = cp.empty_like(x)\n",
    "\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "saxpy_kernel((blocks,), (threads,), (2.0, x, y, out, n))\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "print(\"RawKernel ok; out[0:5] =\", out[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Numba‑CUDA version (for comparison) ---\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def saxpy_numba(a, x, y, out):\n",
    "    i = cuda.grid(1)\n",
    "    if i < x.size:\n",
    "        out[i] = a * x[i] + y[i]\n",
    "\n",
    "n = 1_000_000\n",
    "import cupy as cp\n",
    "x_h = cp.asnumpy(cp.random.random(n, dtype=cp.float32))\n",
    "y_h = cp.asnumpy(cp.random.random(n, dtype=cp.float32))\n",
    "x_d = cuda.to_device(x_h)\n",
    "y_d = cuda.to_device(y_h)\n",
    "out_d = cuda.device_array_like(x_d)\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "saxpy_numba[blocks, threads](2.0, x_d, y_d, out_d)\n",
    "cuda.synchronize()\n",
    "print(\"Numba kernel ok; first 5 =\", out_d.copy_to_host()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d90e2",
   "metadata": {},
   "source": [
    "\n",
    "## 7) `cupyx.jit`: CUDA‑like kernels in Python\n",
    "`cupyx.jit` JIT‑compiles a restricted Python subset to CUDA—middle ground between high‑level array ops and CUDA C strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cupyx import jit\n",
    "import cupy as cp\n",
    "\n",
    "@jit.rawkernel()\n",
    "def saxpy_jit(a, x, y, out):\n",
    "    i = jit.blockDim.x * jit.blockIdx.x + jit.threadIdx.x\n",
    "    if i < x.size:\n",
    "        out[i] = a * x[i] + y[i]\n",
    "\n",
    "n = 1_000_000\n",
    "x = cp.random.random(n, dtype=cp.float32)\n",
    "y = cp.random.random(n, dtype=cp.float32)\n",
    "out = cp.empty_like(x)\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "saxpy_jit[(blocks,), (threads,)](2.0, x, y, out)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "print(\"cupyx.jit ok; out[0:5] =\", out[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0851d",
   "metadata": {},
   "source": [
    "\n",
    "## 8) SciPy‑Compatible: FFT, Linear Algebra, Sparse\n",
    "`cupyx.scipy` mirrors SciPy APIs and calls high‑performance GPU libs (cuBLAS, cuFFT, cuSPARSE, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb4df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cupy as cp\n",
    "from cupyx.scipy import fftpack, linalg, sparse\n",
    "\n",
    "# FFT round‑trip\n",
    "x = cp.random.random(1<<20).astype(cp.complex64)\n",
    "X = fftpack.fft(x)\n",
    "x_rec = fftpack.ifft(X)\n",
    "print(\"FFT round‑trip error:\", float(cp.max(cp.abs(x - x_rec))))\n",
    "\n",
    "# Linear solve: Ax=b\n",
    "A = cp.random.random((1024, 1024), dtype=cp.float32)\n",
    "b = cp.random.random(1024, dtype=cp.float32)\n",
    "x = linalg.solve(A, b)\n",
    "r = cp.linalg.norm(A @ x - b) / cp.linalg.norm(b)\n",
    "print(\"Solve relative residual:\", float(r))\n",
    "\n",
    "# Sparse example\n",
    "rows = cp.array([0, 0, 1, 2, 2, 2])\n",
    "cols = cp.array([0, 2, 2, 0, 1, 2])\n",
    "vals = cp.array([1, 2, 3, 4, 5, 6], dtype=cp.float32)\n",
    "S = sparse.coo_matrix((vals, (rows, cols)), shape=(3,3)).tocsr()\n",
    "d = S.dot(cp.array([1,2,3], dtype=cp.float32))\n",
    "print(\"Sparse dot:\", d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fab1c",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Random Numbers & Streams/Events\n",
    "CuPy RNG mirrors NumPy; versions may support Philox/XORWOW/MRG32k3a backends. Streams/events enable concurrency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb666775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cupy as cp\n",
    "rng = cp.random.default_rng(seed=123)\n",
    "u = rng.random(5, dtype=cp.float32)\n",
    "n = rng.normal(0, 1, 5, dtype=cp.float32)\n",
    "print(\"Uniform:\", u)\n",
    "print(\"Normal :\", n)\n",
    "\n",
    "# Streams demo (toy): overlapping ops\n",
    "s1 = cp.cuda.Stream(non_blocking=True)\n",
    "s2 = cp.cuda.Stream(non_blocking=True)\n",
    "a = cp.empty((1<<20,), dtype=cp.float32)\n",
    "b = cp.empty_like(a)\n",
    "with s1: a.fill(1.0)\n",
    "with s2: b.fill(2.0)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "print(\"Streams ok; a[0], b[0] =\", float(a[0]), float(b[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e98aa",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Timing & Profiling\n",
    "Rule: **synchronize** before/after timing (CUDA is async). Use timers or CUDA events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a79996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cupy as cp, time\n",
    "\n",
    "def bench(func, *args, warmup=3, iters=10, synchronize=True, **kwargs):\n",
    "    for _ in range(warmup):\n",
    "        func(*args, **kwargs)\n",
    "    if synchronize: cp.cuda.Stream.null.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        func(*args, **kwargs)\n",
    "    if synchronize: cp.cuda.Stream.null.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters\n",
    "\n",
    "x = cp.random.random((1<<24,), dtype=cp.float32)\n",
    "def op(x): return cp.tanh(cp.sin(x) + 0.1*x)\n",
    "\n",
    "t_ms = bench(op, x) * 1e3\n",
    "print(f\"Avg time/op: {t_ms:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CUDA events\n",
    "import cupy as cp\n",
    "start, stop = cp.cuda.Event(), cp.cuda.Event()\n",
    "\n",
    "x = cp.random.random((1<<24,), dtype=cp.float32)\n",
    "start.record()\n",
    "y = cp.tanh(cp.sin(x) + 0.1*x)\n",
    "stop.record(); stop.synchronize()\n",
    "print(\"Elapsed (ms):\", cp.cuda.get_elapsed_time(start, stop))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d68de",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Pitfalls & Best Practices\n",
    "- **Data transfer dominates**: batch device work; avoid frequent small host↔device copies.\n",
    "- **Asynchrony**: `synchronize()` for timing or host reads.\n",
    "- **Dtypes**: prefer `float32` unless double is necessary.\n",
    "- **Memory**: reuse arrays; leverage the memory pool; avoid per‑iteration allocations.\n",
    "- **Vectorize first**: use CuPy ufuncs/broadcasting before custom kernels.\n",
    "- **Interop**: use `__cuda_array_interface__` / DLPack with Numba, PyTorch, JAX, etc.\n",
    "- **Streams**: overlap transfers/compute to hide latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d74232",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Quick Decision Guide\n",
    "- Maps to NumPy ops (elementwise, reductions, BLAS, FFT, sparse)? → **CuPy**.\n",
    "- Needs custom control (warp‑level ops, shared memory tiling)? → **Numba‑CUDA** or CuPy RawKernel/`cupyx.jit`.\n",
    "- Need SciPy‑like GPU ecosystem fast? → **CuPy (`cupyx.scipy`)**.\n",
    "- Prototype high‑level in CuPy; specialize hotspots as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad656fc8",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Hands‑On Exercises\n",
    "1. **Vectorized Warm‑up (CuPy):** Implement `softmax` for a 2D batch using ufuncs/broadcasting and compare timings vs NumPy CPU.\n",
    "2. **ElementwiseKernel:** Implement `leaky_relu(x, alpha=0.1)`.\n",
    "3. **ReductionKernel:** Compute per‑row L2 norms for a large matrix.\n",
    "4. **cupyx.jit or RawKernel:** Implement a tiled matrix multiply (e.g., 16×16). Compare to `cp.matmul`.\n",
    "5. **Interop with Numba:** Pass a CuPy array to a Numba kernel via `__cuda_array_interface__` and modify in place.\n",
    "6. **Profiling:** Use CUDA events to compare CuPy vectorized vs `cupyx.jit` vs RawKernel for the same op.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69fe4f",
   "metadata": {},
   "source": [
    "\n",
    "### Appendix: Zero‑copy Numba interop via `__cuda_array_interface__`\n",
    "Numba understands CuPy device arrays directly (no copies).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "\n",
    "# Create a CuPy array\n",
    "cp_arr = cp.arange(16, dtype=cp.float32).reshape(4,4)\n",
    "\n",
    "@cuda.jit\n",
    "def scale_inplace(a, s):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < a.shape[0] and j < a.shape[1]:\n",
    "        a[i, j] *= s\n",
    "\n",
    "threads = (16, 16)\n",
    "blocks = ((cp_arr.shape[0] + threads[0] - 1)//threads[0],\n",
    "          (cp_arr.shape[1] + threads[1] - 1)//threads[1])\n",
    "\n",
    "# Pass cp_arr directly (zero‑copy interface)\n",
    "scale_inplace[blocks, threads](cp_arr, 3.0)\n",
    "cuda.synchronize()\n",
    "print(\"Scaled via Numba, back in CuPy:\", cp_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b709a7",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "- CuPy docs: https://docs.cupy.dev  \n",
    "- Numba CUDA: https://numba.readthedocs.io/en/stable/cuda/index.html  \n",
    "- CUDA Array Interface: https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html  \n",
    "- DLPack spec: https://dmlc.github.io/dlpack/latest/\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
