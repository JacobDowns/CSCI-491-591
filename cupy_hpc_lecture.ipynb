{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bcdf33b5",
      "metadata": {
        "id": "bcdf33b5"
      },
      "source": [
        "# CuPy\n",
        "\n",
        "* CuPy is functionally similar to Numba-CUDA but more closely mimics NumPy\n",
        "* Many operations can be written using NumPy-like syntax\n",
        "* CuPy has drop-in equivalents of most NumPy operations and syntax syntax with GPU arrays (`cp.ndarray`), ufuncs, broadcasting, reductions.\n",
        "* Unlike Numba-CUDA, it provides implementations of a number of more complex operations in `cupyx.scipy`\n",
        "    * FFT\n",
        "    * linear algebra\n",
        "    * sparse matrices\n",
        "    * signal processing\n",
        "    * random numbers\n",
        "* These can make cupy easier to use for many applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92eaffd4",
      "metadata": {
        "id": "92eaffd4"
      },
      "source": [
        "\n",
        "## CuPy Array Basics (NumPy, but on GPU)\n",
        "* Basically, you can replace `numpy as np` with `cupy as cp` and most ufuncs and array ops just work\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a74efec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a74efec",
        "outputId": "ccf8716d-e284-4511-b2c3-c742ccb02b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000000,) float32 <class 'cupy.ndarray'>\n",
            "Host preview: [ 1.          1.3817732   0.49315065 -0.8488725  -1.4104462 ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cupy as cp\n",
        "a = cp.arange(10**6, dtype=cp.float32)\n",
        "b = cp.sin(a) + cp.cos(a)  # ufuncs on GPU\n",
        "print(b.shape, b.dtype, type(b))\n",
        "print(\"Host preview:\", cp.asnumpy(b[:5]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using `cp.arange` will allocate an array on GPU device memory\n",
        "* Ufuncs will perform operations on the GPU\n",
        "* `cp.asnumpy` will copy back to the host\n",
        "* To minimize data transfer, use operations like `cp.asarray` and `cp.asnumpy` only when needed"
      ],
      "metadata": {
        "id": "dBlRttD7aWA3"
      },
      "id": "dBlRttD7aWA3"
    },
    {
      "cell_type": "markdown",
      "id": "1e4f2a7e",
      "metadata": {
        "id": "1e4f2a7e"
      },
      "source": [
        "### Broadcasting, reductions, and axis ops\n",
        "* The normal broadcasting, indexing, and reduction operations operate as one would expect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be2b450",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0be2b450",
        "outputId": "11b47114-3e78-4c9c-9d06-73cde38203e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4096,) float32\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = cp.random.random((4096, 4096), dtype=cp.float32)\n",
        "w = cp.linspace(0, 1, x.shape[1], dtype=cp.float32)\n",
        "y = x * w                       # broadcast\n",
        "col_means = y.mean(axis=0)      # reduction on GPU\n",
        "print(col_means.shape, col_means.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6680005",
      "metadata": {
        "id": "e6680005"
      },
      "source": [
        "\n",
        "## Defining Kernels\n",
        "\n",
        "* There are a handful of options for defining kernels in CuPy. These present many options for balancing complexity and fine-grain control.\n",
        "\n",
        "| Approach | Level of Control | Language you write | How you define it | How you launch it | Can use `threadIdx`/`blockIdx` & shared mem? | Best for | Pros | Cons |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| **Vectorized CuPy ops (ufuncs, broadcasting, reductions)** | Low | Python (NumPy-like) | Just write `cp.sin(x)`, `x*y`, `x.mean(axis=0)`, etc. | Regular function call | No, abstracted away | Elementwise/reduction math, linear algebra via `cp.linalg`, FFT via `cupyx.scipy.fft` | Fast to write; leverages vendor libs (cuBLAS/cuFFT); very readable | Limited low-level control; performance tuning mostly indirect |\n",
        "| **`ElementwiseKernel`** | Medium | CUDA C *expression strings* | `ElementwiseKernel(in_params, out_params, operation, name)` | Call like a function: `k(x, ...)` | no explicit thread/block math | Custom elementwise transforms | Minimal boilerplate; in-place and broadcasting friendly | Logic must fit elementwise form; no shared memory |\n",
        "| **`ReductionKernel`** | Medium | CUDA C *expression strings* | `ReductionKernel(in_params, out_params, map_expr, reduce_expr, post_map_expr, identity, name)` | Call like a function: `k(x, ...)` | no | Custom reductions (sum of f(x), argmax, etc.) | Concise custom reductions; good perf | Reduction pattern only; more complex to reason about |\n",
        "| **`cupyx.jit` – `@jit.kernel()`** | Medium–High | Restricted Python (JIT to CUDA) | Write Python function with array indexing; decorate with `@jit.kernel()` | Call like a function (grid/block inferred or provided) | APIs for `threadIdx`, `blockIdx`, shared mem via `jit.shared_memory`) | Custom elementwise/nd kernels with Python syntax | Pythonic; no C string; easy to iterate | Still a restricted subset; not as feature-complete as raw CUDA C |\n",
        "| **`cupyx.jit` – `@jit.rawkernel()`** | High | Restricted Python with explicit launch math | Define func with `@jit.rawkernel()` and compute global index manually | CUDA-like launch: `f[(blocks,), (threads,)](args...)` |full index control; shared mem via `jit.shared_memory` | Hand-tuned kernels without leaving Python | Good balance of control & ergonomics | Some CUDA features may require workarounds; subset semantics |\n",
        "| **`RawKernel`** | Highest | CUDA C/C++ (as a string) | `cp.RawKernel(src, \"func_name\")` with CUDA code | CUDA-like launch: `k((blocks,), (threads,), (args...), shared_mem=...)` | full control | Performance-critical kernels; shared memory tiling; intrinsics | Full CUDA power; predictable | You write/maintain CUDA C strings; harder to prototype |\n",
        "| **`RawModule`** | Highest | CUDA C/C++ (string or file), PTX | `cp.RawModule(code=..., path=..., options=...)`; then `get_function(\"name\")` | Same as `RawKernel` once you get the function | yes| Packaging multiple kernels, using headers, templates, linking | Organize many kernels; reuse across cells | Build/link errors are more complex; still CUDA C |\n",
        "\n",
        "* The best way to start out with CuPy is to use the vectorized operations, ufuncs, broadcasting / other NumPy-esque features\n",
        "* Branch out and expand whre more fine-grain control / complexity is needed\n",
        "\n",
        "> **Rule of thumb:** Start with **CuPy** vectorization; drop to `cupyx.jit`/RawKernel or **Numba** only for hotspots that need custom kernels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ce331e",
      "metadata": {
        "id": "00ce331e"
      },
      "source": [
        "\n",
        "## Elementwise Kernels\n",
        "* Elemntwise and reduction kernels can be a convenient alternative to writing full CUDA kerneels\n",
        "* You don't have to worry about launch syntax like specifying the number of blocks / threads per block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a8409a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0a8409a",
        "outputId": "11595f4d-c9bb-4a66-8e5f-1af0b840ae95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elementwise: [1.        1.2857143 1.5714285 1.8571429 2.142857  2.4285715 2.7142859\n",
            " 3.       ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from cupy import ElementwiseKernel, ReductionKernel\n",
        "import cupy as cp\n",
        "\n",
        "# Elementwise: y = a*x + b\n",
        "axpb = ElementwiseKernel(\n",
        "    in_params='float32 x, float32 a, float32 b',\n",
        "    out_params='float32 y',\n",
        "    operation='y = a * x + b;',\n",
        "    name='axpb'\n",
        ")\n",
        "\n",
        "x = cp.linspace(0, 1, 8, dtype=cp.float32)\n",
        "y = axpb(x, 2.0, 1.0)\n",
        "print(\"Elementwise:\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction Kernels\n",
        "\n",
        "* The `cupy.ReductionKernel` API lets you define your own reductions on the GPU (e.g. sum, mean, norm, min/max, etc.) using a few simple expressions — no need to write explicit CUDA kernels.\n",
        "\n",
        "* It's roughly analogous to a CUDA kernel that:\n",
        "1. Maps each input element through some expression (`map_expr`),\n",
        "2. Reduces all mapped results using an associative binary operation (`reduce_expr`),\n",
        "3. Optionally transforms the final accumulator before writing output (`post_map_expr`),\n",
        "4. Starts with an identity (neutral) value for the accumulator.\n"
      ],
      "metadata": {
        "id": "ZOIc7sJTj9Xm"
      },
      "id": "ZOIc7sJTj9Xm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here's an example of a reduction kernel"
      ],
      "metadata": {
        "id": "NpPPkfpsecAc"
      },
      "id": "NpPPkfpsecAc"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reduction: sum of squares\n",
        "sum_squares = ReductionKernel(\n",
        "    in_params='float32 x',\n",
        "    out_params='float32 y',\n",
        "    map_expr='x * x',\n",
        "    reduce_expr='a + b',\n",
        "    post_map_expr='y = a',\n",
        "    identity='0',\n",
        "    name='sum_squares'\n",
        ")\n",
        "print(\"Reduction:\", sum_squares(cp.arange(10, dtype=cp.float32)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK9OSu9Qee_J",
        "outputId": "0518ff5c-1472-400d-a8d8-0678be2e7726"
      },
      "id": "pK9OSu9Qee_J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduction: 285.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133ab219",
      "metadata": {
        "id": "133ab219"
      },
      "source": [
        "## Raw Kernels\n",
        "* A raw kernel is basically justa CUDA C kernel written as a string\n",
        "* `cp.RawKernel` lets you write CUDA C kernels (as strings) and launch from Python\n",
        "* This is useful for relatively simple kernels that might not be easily expressed as NumPy-like vectorized operations\n",
        "* Raw kernels require you to specify the block and thread count when launching as in Numba-CUDA / standard CUDA kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f0e45f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89f0e45f",
        "outputId": "7c4fcaa7-c389-4351-c67f-fbd80e992c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RawKernel ok; out[0:5] = [0.29604462 0.523803   0.5467715  0.13893062 0.36670983]\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "\n",
        "# RawKernel (CUDA C)\n",
        "saxpy_src = r'''\n",
        "extern \"C\" __global__\n",
        "void saxpy(const float a, const float* __restrict__ x,\n",
        "           const float* __restrict__ y, float* __restrict__ out, int n) {\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i < n) out[i] = a * x[i] + y[i];\n",
        "}\n",
        "''';\n",
        "saxpy_kernel = cp.RawKernel(saxpy_src, \"saxpy\")\n",
        "\n",
        "# test data\n",
        "n = 1_000_000\n",
        "x = cp.random.random(n, dtype=cp.float32)\n",
        "y = cp.random.random(n, dtype=cp.float32)\n",
        "out = cp.empty_like(x)\n",
        "\n",
        "threads = 256\n",
        "blocks = (n + threads - 1) // threads\n",
        "saxpy_kernel((blocks,), (threads,), (2.0, x, y, out, n))\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "print(\"RawKernel ok; out[0:5] =\", out[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note** For a raw kernel, we want to use the `synchronize` operation like in Numba-CUDA to ensure that all threads have finished before we need to use the output of the kernel. CuPy uses the default stream, called the null stream for computations by default."
      ],
      "metadata": {
        "id": "0Do8rJo9ximB"
      },
      "id": "0Do8rJo9ximB"
    },
    {
      "cell_type": "markdown",
      "id": "1d6d90e2",
      "metadata": {
        "id": "1d6d90e2"
      },
      "source": [
        "\n",
        "## `cupyx.jit`: CUDA-like kernels in Python\n",
        "* `cupyx.jit` JIT-compiles a restricted Python subset to CUDA\n",
        "* Its a middle ground between high-level array ops and CUDA C strings\n",
        "* Very similar to defining Numba-CUDA kernels defined in standard python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debe6fc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "debe6fc7",
        "outputId": "a809a6ea-0ed1-4310-d879-cd841131dacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
            "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cupyx.jit ok; out[0:5] = [1.6782546  1.2052308  1.0658538  0.5613324  0.48698527]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from cupyx import jit\n",
        "import cupy as cp\n",
        "\n",
        "@jit.rawkernel()\n",
        "def saxpy_jit(a, x, y, out):\n",
        "    i = jit.blockDim.x * jit.blockIdx.x + jit.threadIdx.x\n",
        "    if i < x.size:\n",
        "        out[i] = a * x[i] + y[i]\n",
        "\n",
        "n = 1_000_000\n",
        "x = cp.random.random(n, dtype=cp.float32)\n",
        "y = cp.random.random(n, dtype=cp.float32)\n",
        "out = cp.empty_like(x)\n",
        "threads = 256\n",
        "blocks = (n + threads - 1) // threads\n",
        "saxpy_jit[(blocks,), (threads,)](2.0, x, y, out)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "print(\"cupyx.jit ok; out[0:5] =\", out[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f0851d",
      "metadata": {
        "id": "88f0851d"
      },
      "source": [
        "\n",
        "## SciPy‑Compatible: FFT, Linear Algebra, Sparse\n",
        "* CuPy has existing kernels for solving linear algebra problems, performing FFTs and many other common tasks\n",
        "* `cupyx.scipy` mirrors SciPy APIs and calls high-performance GPU libs (cuBLAS, cuFFT, cuSPARSE, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bb4df4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7bb4df4",
        "outputId": "497d14ec-4df4-4262-fa46-6a515c4c2afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FFT round‑trip error: 7.802258892297687e-07\n",
            "Solve relative residual: 0.00032664884929545224\n",
            "Sparse dot: [ 7.  9. 32.]\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "from cupyx.scipy import fftpack, linalg, sparse\n",
        "\n",
        "# FFT round‑trip\n",
        "x = cp.random.random(1<<20).astype(cp.complex64)\n",
        "X = fftpack.fft(x)\n",
        "x_rec = fftpack.ifft(X)\n",
        "print(\"FFT round‑trip error:\", float(cp.max(cp.abs(x - x_rec))))\n",
        "\n",
        "# Linear solve: Ax=b\n",
        "A = cp.random.random((1024, 1024), dtype=cp.float32)\n",
        "b = cp.random.random(1024, dtype=cp.float32)\n",
        "x = cp.linalg.solve(A, b)\n",
        "r = cp.linalg.norm(A @ x - b) / cp.linalg.norm(b)\n",
        "print(\"Solve relative residual:\", float(r))\n",
        "\n",
        "# Sparse example\n",
        "rows = cp.array([0, 0, 1, 2, 2, 2])\n",
        "cols = cp.array([0, 2, 2, 0, 1, 2])\n",
        "vals = cp.array([1, 2, 3, 4, 5, 6], dtype=cp.float32)\n",
        "S = sparse.coo_matrix((vals, (rows, cols)), shape=(3,3)).tocsr()\n",
        "d = S.dot(cp.array([1,2,3], dtype=cp.float32))\n",
        "print(\"Sparse dot:\", d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88fab1c",
      "metadata": {
        "id": "c88fab1c"
      },
      "source": [
        "\n",
        "## Random Numbers\n",
        "* Random numbers are a bit nicer to handle in CuPy than Numba-CUDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb666775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb666775",
        "outputId": "fa536f9c-1b01-4657-9c4b-4d716bf8a7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform: [0.9656452  0.64809096 0.61324495 0.18948276 0.69727665]\n",
            "Normal : [-0.6466216   0.54218817 -1.4606272  -0.6613516  -0.35003424]\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "rng = cp.random.default_rng(seed=123)\n",
        "u = rng.random(5, dtype=cp.float32)\n",
        "n = rng.standard_normal(5, dtype=cp.float32)\n",
        "print(\"Uniform:\", u)\n",
        "print(\"Normal :\", n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0ab4bc",
      "metadata": {
        "id": "4e0ab4bc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# CUDA events\n",
        "import cupy as cp\n",
        "start, stop = cp.cuda.Event(), cp.cuda.Event()\n",
        "\n",
        "x = cp.random.random((1<<24,), dtype=cp.float32)\n",
        "start.record()\n",
        "y = cp.tanh(cp.sin(x) + 0.1*x)\n",
        "stop.record(); stop.synchronize()\n",
        "print(\"Elapsed (ms):\", cp.cuda.get_elapsed_time(start, stop))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Streams in CuPy\n",
        "* Recall that streams are independent queues of GPU work.\n",
        "* Operation in the same stream execute in order\n",
        "* Operations in different streams may run concurrently\n",
        "* By default, CuPy uses the **null (default) stream**. You can create additional streams to overlap kernels and transfers.\n",
        "\n",
        "### Key ideas\n",
        "- **Default vs custom streams**  \n",
        "  - Default stream: `cp.cuda.Stream.null` (used unless you set your own).\n",
        "  - Custom stream: `s = cp.cuda.Stream(non_blocking=True)`; use with a context manager.\n",
        "- **Asynchrony**  \n",
        "  - Kernel launches and most copies are asynchronous with respect to the host (CPU).\n",
        "  - Use `stream.synchronize()` or `cp.cuda.Stream.null.synchronize()` when you need results or accurate timings."
      ],
      "metadata": {
        "id": "R_9jboTc1O-Y"
      },
      "id": "R_9jboTc1O-Y"
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import math, time\n",
        "\n",
        "\n",
        "N = 50_000_000\n",
        "x = cp.random.random(N, dtype=cp.float32)\n",
        "y = cp.random.random(N, dtype=cp.float32)\n",
        "\n",
        "def heavy_op(a):\n",
        "    # A chain of elementwise ops to keep the GPU busy\n",
        "    return cp.tanh(cp.sin(a) + 0.1*a) + cp.sqrt(cp.abs(a))\n",
        "\n",
        "### Single stream computation\n",
        "r1 = heavy_op(x)\n",
        "r2 = heavy_op(y)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "\n",
        "### Multiple stream computation\n",
        "s1 = cp.cuda.Stream(non_blocking=True)\n",
        "s2 = cp.cuda.Stream(non_blocking=True)\n",
        "\n",
        "with s1:\n",
        "    r1_ov = heavy_op(x)\n",
        "with s2:\n",
        "    r2_ov = heavy_op(y)\n",
        "\n",
        "s1.synchronize()\n",
        "s2.synchronize()"
      ],
      "metadata": {
        "id": "oFT8p8Sq2Ac8"
      },
      "id": "oFT8p8Sq2Ac8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As in Numba-CUDA, you may be able to speed up your computation by using overlapping transfer and compute with a Ping-Pong kernel or other multi-stream methods"
      ],
      "metadata": {
        "id": "50Q-G4Pj2y-f"
      },
      "id": "50Q-G4Pj2y-f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CuPy's Memory Pool\n",
        "\n",
        "* GPU allocations via the CUDA driver (`cudaMalloc` / `cudaFree`) are expensive operations compared to CPU `malloc/free`.\n",
        "* They cross the user-kernel boundary, update GPU virtual memory structures, and often imply device synchronization.\n",
        "* In tight loops with many small arrays, this overhead can dominate runtime.\n",
        "* CuPy's memory pool acts like a caching allocator:\n",
        "  * New CuPy arrays are served from a pool of already-allocated GPU blocks, avoiding repeated `cudaMalloc` calls.\n",
        "  * When arrays die, memory returns to the pool (not immediately to the driver), so future allocations can reuse it instantly.\n",
        "\n",
        "### Key takeaways\n",
        "- `cudaMalloc`/`cudaFree` are heavyweight; frequent calls serialize work and stall the GPU.\n",
        "- The memory pool removes most allocation overhead by reusing blocks.\n",
        "- You can inspect/reset the pool:"
      ],
      "metadata": {
        "id": "W5xDkDOFjfuy"
      },
      "id": "W5xDkDOFjfuy"
    },
    {
      "cell_type": "code",
      "source": [
        "mpool = cp.get_default_memory_pool()\n",
        "print(\"pool total bytes:\", mpool.total_bytes())\n",
        "print(\"pool used bytes:\", mpool.used_bytes())\n",
        "mpool.free_all_blocks()  # return cached blocks to the driver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3ZsahrOji5r",
        "outputId": "6edee097-a624-4297-d851-49b549652639"
      },
      "id": "N3ZsahrOji5r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pool total bytes: 1279109632\n",
            "pool used bytes: 1248805888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* As an example, let's see what happens when we do raw `cudaAlloc` and `cudaFree` versus relying ont the memory pool"
      ],
      "metadata": {
        "id": "_HbyLeTx4zgC"
      },
      "id": "_HbyLeTx4zgC"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cupy as cp, time\n",
        "\n",
        "def alloc_free_loop(n=1000, shape=(1024, 1024), dtype=cp.float32):\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(n):\n",
        "        x = cp.empty(shape, dtype=dtype)\n",
        "        # Do nothing with x; we are measuring allocation + free path\n",
        "        del x\n",
        "    cp.cuda.Stream.null.synchronize()\n",
        "    return time.perf_counter() - start\n",
        "\n",
        "#  Disable CuPy's memory pool (use raw cudaMalloc/cudaFree)\n",
        "cp.cuda.set_allocator(None)  # turn off pooled allocator\n",
        "t_no_pool = alloc_free_loop()\n",
        "\n",
        "# Re-enable CuPy's default memory pool\n",
        "mpool = cp.cuda.MemoryPool()\n",
        "cp.cuda.set_allocator(mpool.malloc)\n",
        "t_pool = alloc_free_loop()\n",
        "\n",
        "print(f\"Without pool: {t_no_pool:.3f}s\")\n",
        "print(f\"With pool:    {t_pool:.3f}s  (speedup: {t_no_pool/t_pool:.1f}×)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYI9MU7n560w",
        "outputId": "97139ca8-75a5-4607-b433-aa9c5588bea2"
      },
      "id": "mYI9MU7n560w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without pool: 0.143s\n",
            "With pool:    0.005s  (speedup: 28.9×)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinned Memory\n",
        "* As in Numba-CUDA, CuPy allows you to use pinned memory. Recall that pinned memory is page locked memory that the OS will not page out which allows higher host to device bandwidth\n",
        "* Pinned memory allows for true asynchronous copies that overlap with kernels on other streams\n",
        "* In contrast, copies from pageable memory may inovle an internal pin to copy to unpin staging path that blocks on the host, preventing work from being queued on another stream\n",
        "\n",
        "\n",
        "### When pinned memory helps\n",
        "* To reiterate a few points on pinned memory, pinned memory can be useful when:\n",
        "  - You frequently transfer large arrays between host and device.\n",
        "  - You want to overlap copies with compute using multiple CUDA streams.\n",
        "\n",
        ">  Pinned memory is a limited system resource. Over-pinning large regions can hurt overall system performance. Reuse pinned buffers; don't pin “just in case.”\n",
        "\n",
        "\n",
        "### Using pinned memory in CuPy\n",
        "* You can use pinned memory like this:\n"
      ],
      "metadata": {
        "id": "-e5Acr-8QYfQ"
      },
      "id": "-e5Acr-8QYfQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, cupy as cp\n",
        "\n",
        "n = 10_000_000\n",
        "buf = cp.cuda.alloc_pinned_memory(n * np.dtype(np.float32).itemsize)\n",
        "h = np.frombuffer(buf, dtype=np.float32, count=n)  # NumPy view, no copy\n",
        "h.fill(1.0)  # normal NumPy ops\n",
        "# Copy to device memory from pinned memory\n",
        "d = cp.asarray(h)"
      ],
      "metadata": {
        "id": "0C-puFTzSoqC"
      },
      "id": "0C-puFTzSoqC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By default, CuPy uses a pinned memory pool similar to the normal memory pool"
      ],
      "metadata": {
        "id": "cyb7q4ipkEIZ"
      },
      "id": "cyb7q4ipkEIZ"
    },
    {
      "cell_type": "code",
      "source": [
        "mpool = cp.get_default_memory_pool()"
      ],
      "metadata": {
        "id": "0wgl6KuFlfR7"
      },
      "id": "0wgl6KuFlfR7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "\n",
        "pinned_pool = cp.cuda.PinnedMemoryPool()\n",
        "cp.cuda.set_pinned_memory_allocator(pinned_pool.malloc)\n",
        "\n",
        "\n",
        "buf = cp.cuda.alloc_pinned_memory(n * np.dtype(np.float32).itemsize)\n",
        "print(\"free blocks:\", pinned_pool.n_free_blocks())\n",
        "\n",
        "# Now any cp.cuda.alloc_pinned_memory(...) call will use this pool.\n",
        "# Also, if CuPy internally needs pinned buffers, it can reuse from here.\n",
        "\n",
        "\n",
        "# Return cached pinned blocks to the OS if you want to trim:\n",
        "# pinned_pool.free_all_blocks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgZh14YjkMsS",
        "outputId": "43bd41c6-8d27-424e-e99f-946bc518354b"
      },
      "id": "PgZh14YjkMsS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "free blocks: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmark\n",
        "\n",
        "* We can compare how long it takes to copy data from pageable versus pinned memory to device memory to see the performance benefit\n",
        "* The basic idea here is that we allocate an array `d` on the GPU. We'll repeatedly do some copies from pageable versus pinned memory to `d` to see the difference in performance"
      ],
      "metadata": {
        "id": "G3HXUqYylwLn"
      },
      "id": "G3HXUqYylwLn"
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "# Ensure a pinned pool is installed (helps reuse pinned buffers)\n",
        "pinned_pool = cp.cuda.PinnedMemoryPool()\n",
        "cp.cuda.set_pinned_memory_allocator(pinned_pool.malloc)\n",
        "\n",
        "N = 50_000_000\n",
        "bytes_ = N * np.dtype(np.float32).itemsize\n",
        "\n",
        "# Pageable host array (regular NumPy)\n",
        "h_pageable = np.ones(N, dtype=np.float32)\n",
        "\n",
        "# Pinned host array\n",
        "buf = cp.cuda.alloc_pinned_memory(bytes_)\n",
        "h_pinned = np.frombuffer(buf, dtype=np.float32, count=N)\n",
        "h_pinned.fill(1.0)\n",
        "\n",
        "d = cp.empty(N, dtype=cp.float32)\n",
        "\n",
        "def time_h2d(src, repeats=3):\n",
        "    times = []\n",
        "    for _ in range(repeats):\n",
        "        start, stop = cp.cuda.Event(), cp.cuda.Event()\n",
        "        start.record()\n",
        "        d.set(src)\n",
        "        stop.record()\n",
        "        stop.synchronize()\n",
        "\n",
        "        times.append(cp.cuda.get_elapsed_time(start, stop))\n",
        "\n",
        "    return np.array(times).mean()\n",
        "\n",
        "# H2D pageable (default stream)\n",
        "t_pageable = time_h2d(h_pageable)\n",
        "\n",
        "# H2D pinned\n",
        "t_pinned = time_h2d(h_pinned)\n",
        "\n",
        "print(f\"H2D pageable (ms): {t_pageable:.1f}\")\n",
        "print(f\"H2D pinned   (ms): {t_pinned:.1f}\")\n",
        "print(f\"Speedup: {t_pageable / t_pinned:.2f}×\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhCuq02zl1Bo",
        "outputId": "1e66b2a4-a3f3-47db-e7d6-b565a65333f0"
      },
      "id": "xhCuq02zl1Bo",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H2D pageable (ms): 42.4\n",
            "H2D pinned   (ms): 16.2\n",
            "Speedup: 2.62×\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b709a7",
      "metadata": {
        "id": "46b709a7"
      },
      "source": [
        "\n",
        "## References\n",
        "- CuPy docs: https://docs.cupy.dev  \n",
        "- Numba CUDA: https://numba.readthedocs.io/en/stable/cuda/index.html  \n",
        "- CUDA Array Interface: https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html  \n",
        "- DLPack spec: https://dmlc.github.io/dlpack/latest/\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}